From 86ded39f8962db4a08b0efb65bcf8795c3306288 Mon Sep 17 00:00:00 2001
From: Winston-leon <1871056255@qq.com>
Date: Wed, 18 Nov 2020 16:36:16 +0800
Subject: [PATCH 01/19] Lock-sys optimization: sharded lock_sys mutex

---
 share/messages_to_error_log.txt               |    4 +-
 storage/innobase/CMakeLists.txt               |    4 +-
 storage/innobase/btr/btr0btr.cc               |   11 +-
 storage/innobase/buf/buf0buf.cc               |   48 +-
 storage/innobase/buf/buf0dblwr.cc             |    4 +-
 storage/innobase/dict/dict0dict.cc            |    2 +-
 storage/innobase/dict/mem.cc                  |    2 +
 storage/innobase/gis/gis0sea.cc               |    3 +-
 storage/innobase/handler/ha_innodb.cc         |   24 +-
 storage/innobase/handler/p_s.cc               |   37 +-
 storage/innobase/include/buf0buf.h            |   12 +-
 storage/innobase/include/dict0mem.h           |   31 +-
 storage/innobase/include/lock0guards.h        |  147 ++
 storage/innobase/include/lock0latches.h       |  300 +++
 storage/innobase/include/lock0lock.h          |  199 +-
 storage/innobase/include/lock0lock.ic         |    2 +-
 storage/innobase/include/lock0priv.h          |   62 +-
 storage/innobase/include/lock0priv.ic         |   25 +-
 storage/innobase/include/log0types.h          |   46 +-
 storage/innobase/include/os0file.h            |    2 +-
 storage/innobase/include/que0que.h            |   18 +-
 storage/innobase/include/rem0rec.h            |   78 +-
 storage/innobase/include/row0vers.h           |   17 +-
 storage/innobase/include/srv0srv.h            |   20 +-
 storage/innobase/include/sync0sharded_rw.h    |   39 +-
 storage/innobase/include/sync0sync.h          |    6 +-
 storage/innobase/include/sync0types.h         |   22 +-
 storage/innobase/include/trx0i_s.h            |    4 +-
 storage/innobase/include/trx0sys.h            |    9 +-
 storage/innobase/include/trx0sys.ic           |   10 +-
 storage/innobase/include/trx0trx.h            |  205 +-
 .../innobase/include/ut0class_life_cycle.h    |   52 +
 storage/innobase/include/ut0counter.h         |   24 +-
 storage/innobase/include/ut0cpu_cache.h       |   56 +
 storage/innobase/include/ut0link_buf.h        |    4 +-
 storage/innobase/include/ut0mpmcbq.h          |    6 +-
 storage/innobase/include/ut0new.h             |   10 +-
 storage/innobase/lock/lock0guards.cc          |  114 ++
 storage/innobase/lock/lock0iter.cc            |    8 +-
 storage/innobase/lock/lock0latches.cc         |  107 +
 storage/innobase/lock/lock0lock.cc            | 1759 +++++++++--------
 storage/innobase/lock/lock0prdt.cc            |  114 +-
 storage/innobase/lock/lock0wait.cc            |  109 +-
 storage/innobase/que/que0que.cc               |   26 +-
 storage/innobase/row/row0ins.cc               |   13 +-
 storage/innobase/row/row0mysql.cc             |   16 +-
 storage/innobase/row/row0vers.cc              |   42 +-
 storage/innobase/srv/srv0srv.cc               |   87 +-
 storage/innobase/sync/sync0debug.cc           |   51 +-
 storage/innobase/sync/sync0sync.cc            |    6 +-
 storage/innobase/trx/trx0i_s.cc               |   93 +-
 storage/innobase/trx/trx0roll.cc              |    5 +-
 storage/innobase/trx/trx0trx.cc               |  201 +-
 53 files changed, 2727 insertions(+), 1569 deletions(-)
 create mode 100644 storage/innobase/include/lock0guards.h
 create mode 100644 storage/innobase/include/lock0latches.h
 create mode 100644 storage/innobase/include/ut0class_life_cycle.h
 create mode 100644 storage/innobase/include/ut0cpu_cache.h
 create mode 100644 storage/innobase/lock/lock0guards.cc
 create mode 100644 storage/innobase/lock/lock0latches.cc

diff --git a/share/messages_to_error_log.txt b/share/messages_to_error_log.txt
index 82a27b57c17..c643313f044 100644
--- a/share/messages_to_error_log.txt
+++ b/share/messages_to_error_log.txt
@@ -7451,10 +7451,10 @@ ER_IB_MSG_638
 ER_IB_MSG_639
   eng "%s"
 
-ER_IB_MSG_640
+OBSOLETE_ER_IB_MSG_640
   eng "%s"
 
-ER_IB_MSG_641
+OBSOLETE_ER_IB_MSG_641
   eng "%s"
 
 ER_IB_MSG_642
diff --git a/storage/innobase/CMakeLists.txt b/storage/innobase/CMakeLists.txt
index 882c8085bad..80bebd0acfd 100644
--- a/storage/innobase/CMakeLists.txt
+++ b/storage/innobase/CMakeLists.txt
@@ -1,4 +1,4 @@
-# Copyright (c) 2006, 2019, Oracle and/or its affiliates. All rights reserved.
+# Copyright (c) 2006, 2020, Oracle and/or its affiliates. All rights reserved.
 #
 # This program is free software; you can redistribute it and/or modify
 # it under the terms of the GNU General Public License, version 2.0,
@@ -135,8 +135,10 @@ SET(INNOBASE_SOURCES
   lob/zlob0update.cc
   lob/zlob0first.cc
   lob/zlob0read.cc
+  lock/lock0guards.cc
   lock/lock0iter.cc
   lock/lock0prdt.cc
+  lock/lock0latches.cc
   lock/lock0lock.cc
   lock/lock0wait.cc
   log/log0buf.cc
diff --git a/storage/innobase/btr/btr0btr.cc b/storage/innobase/btr/btr0btr.cc
index 040fac51caa..e343371205e 100644
--- a/storage/innobase/btr/btr0btr.cc
+++ b/storage/innobase/btr/btr0btr.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1994, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1994, 2020, Oracle and/or its affiliates. All Rights Reserved.
 Copyright (c) 2012, Facebook Inc.
 
 This program is free software; you can redistribute it and/or modify it under
@@ -2951,9 +2951,8 @@ static buf_block_t *btr_lift_page_up(
   if (!dict_table_is_locking_disabled(index->table)) {
     /* Free predicate page locks on the block */
     if (dict_index_is_spatial(index)) {
-      lock_mutex_enter();
+      locksys::Shard_latch_guard guard{block->get_page_id()};
       lock_prdt_page_free_from_discard(block, lock_sys->prdt_page_hash);
-      lock_mutex_exit();
     }
     lock_update_copy_and_discard(father_block, block);
   }
@@ -3220,10 +3219,9 @@ retry:
       }
 
       /* No GAP lock needs to be worrying about */
-      lock_mutex_enter();
+      locksys::Shard_latch_guard guard{block->get_page_id()};
       lock_prdt_page_free_from_discard(block, lock_sys->prdt_page_hash);
       lock_rec_free_all_from_discard_page(block);
-      lock_mutex_exit();
     } else {
       btr_node_ptr_delete(index, block, mtr);
       if (!dict_table_is_locking_disabled(index->table)) {
@@ -3355,10 +3353,9 @@ retry:
         rtr_merge_and_update_mbr(&cursor2, &father_cursor, offsets2, offsets,
                                  merge_page, merge_block, block, index, mtr);
       }
-      lock_mutex_enter();
+      locksys::Shard_latch_guard guard{block->get_page_id()};
       lock_prdt_page_free_from_discard(block, lock_sys->prdt_page_hash);
       lock_rec_free_all_from_discard_page(block);
-      lock_mutex_exit();
     } else {
       compressed = btr_cur_pessimistic_delete(
           &err, TRUE, &cursor2, BTR_CREATE_FLAG, false, 0, 0, 0, mtr);
diff --git a/storage/innobase/buf/buf0buf.cc b/storage/innobase/buf/buf0buf.cc
index 25aec18f5bd..0edbc98dda1 100644
--- a/storage/innobase/buf/buf0buf.cc
+++ b/storage/innobase/buf/buf0buf.cc
@@ -2140,28 +2140,30 @@ withdraw_retry:
       message_interval *= 2;
     }
 
-    lock_mutex_enter();
-    trx_sys_mutex_enter();
-    bool found = false;
-    for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->mysql_trx_list);
-         trx != nullptr; trx = UT_LIST_GET_NEXT(mysql_trx_list, trx)) {
-      if (trx->state != TRX_STATE_NOT_STARTED && trx->mysql_thd != nullptr &&
-          ut_difftime(withdraw_started, trx->start_time) > 0) {
-        if (!found) {
-          ib::warn(ER_IB_MSG_61) << "The following trx might hold"
-                                    " the blocks in buffer pool to"
-                                    " be withdrawn. Buffer pool"
-                                    " resizing can complete only"
-                                    " after all the transactions"
-                                    " below release the blocks.";
-          found = true;
-        }
+    {
+      /* lock_trx_print_wait_and_mvcc_state() requires exclusive global latch */
+      locksys::Global_exclusive_latch_guard guard{};
+      trx_sys_mutex_enter();
+      bool found = false;
+      for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->mysql_trx_list);
+           trx != nullptr; trx = UT_LIST_GET_NEXT(mysql_trx_list, trx)) {
+        if (trx->state != TRX_STATE_NOT_STARTED && trx->mysql_thd != nullptr &&
+            ut_difftime(withdraw_started, trx->start_time) > 0) {
+          if (!found) {
+            ib::warn(ER_IB_MSG_61) << "The following trx might hold"
+                                      " the blocks in buffer pool to"
+                                      " be withdrawn. Buffer pool"
+                                      " resizing can complete only"
+                                      " after all the transactions"
+                                      " below release the blocks.";
+            found = true;
+          }
 
-        lock_trx_print_wait_and_mvcc_state(stderr, trx);
+          lock_trx_print_wait_and_mvcc_state(stderr, trx);
+        }
       }
+      trx_sys_mutex_exit();
     }
-    trx_sys_mutex_exit();
-    lock_mutex_exit();
 
     withdraw_started = ut_time();
   }
@@ -4404,14 +4406,6 @@ bool buf_page_get_known_nowait(ulint rw_latch, buf_block_t *block,
   return (true);
 }
 
-/** Given a tablespace id and page number tries to get that page. If the
-page is not in the buffer pool it is not loaded and NULL is returned.
-Suitable for using when holding the lock_sys_t::mutex.
-@param[in]	page_id	page id
-@param[in]	file	file name
-@param[in]	line	line where called
-@param[in]	mtr	mini-transaction
-@return pointer to a page or NULL */
 const buf_block_t *buf_page_try_get_func(const page_id_t &page_id,
                                          const char *file, ulint line,
                                          mtr_t *mtr) {
diff --git a/storage/innobase/buf/buf0dblwr.cc b/storage/innobase/buf/buf0dblwr.cc
index 45fe60f126d..c557b9862fe 100644
--- a/storage/innobase/buf/buf0dblwr.cc
+++ b/storage/innobase/buf/buf0dblwr.cc
@@ -745,12 +745,12 @@ class Batch_segment : public Segment {
   /** The instance that is being written to disk. */
   Double_write *m_dblwr{};
 
-  byte m_pad1[INNOBASE_CACHE_LINE_SIZE];
+  byte m_pad1[ut::INNODB_CACHE_LINE_SIZE];
 
   /** Size of the batch. */
   std::atomic_int m_batch_size{};
 
-  byte m_pad2[INNOBASE_CACHE_LINE_SIZE];
+  byte m_pad2[ut::INNODB_CACHE_LINE_SIZE];
 
   /** Number of pages to write. */
   std::atomic_int m_written{};
diff --git a/storage/innobase/dict/dict0dict.cc b/storage/innobase/dict/dict0dict.cc
index 757d8d70b39..f7b15d584c9 100644
--- a/storage/innobase/dict/dict0dict.cc
+++ b/storage/innobase/dict/dict0dict.cc
@@ -1885,7 +1885,7 @@ static void dict_table_remove_from_cache_low(
   ut_ad(table);
   ut_ad(dict_lru_validate());
   ut_a(table->get_ref_count() == 0);
-  ut_a(table->n_rec_locks == 0);
+  ut_a(table->n_rec_locks.load() == 0);
   ut_ad(mutex_own(&dict_sys->mutex));
   ut_ad(table->magic_n == DICT_TABLE_MAGIC_N);
 
diff --git a/storage/innobase/dict/mem.cc b/storage/innobase/dict/mem.cc
index b342d66ba4c..6544ff39221 100644
--- a/storage/innobase/dict/mem.cc
+++ b/storage/innobase/dict/mem.cc
@@ -39,7 +39,9 @@ external tools. */
 
 #include "dict0dict.h"
 #ifndef UNIV_HOTBACKUP
+#ifndef UNIV_LIBRARY
 #include "lock0lock.h"
+#endif /* !UNIV_LIBRARY */
 #endif /* !UNIV_HOTBACKUP */
 
 /** Append 'name' to 'col_names'.  @see dict_table_t::col_names
diff --git a/storage/innobase/gis/gis0sea.cc b/storage/innobase/gis/gis0sea.cc
index 3b55bde6475..0871677693a 100644
--- a/storage/innobase/gis/gis0sea.cc
+++ b/storage/innobase/gis/gis0sea.cc
@@ -1123,10 +1123,9 @@ void rtr_check_discard_page(
 
   mutex_exit(&index->rtr_track->rtr_active_mutex);
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
   lock_prdt_page_free_from_discard(block, lock_sys->prdt_hash);
   lock_prdt_page_free_from_discard(block, lock_sys->prdt_page_hash);
-  lock_mutex_exit();
 }
 
 /** Restore the stored position of a persistent cursor bufferfixing the page */
diff --git a/storage/innobase/handler/ha_innodb.cc b/storage/innobase/handler/ha_innodb.cc
index 146f4401b6f..fc194d2e493 100644
--- a/storage/innobase/handler/ha_innodb.cc
+++ b/storage/innobase/handler/ha_innodb.cc
@@ -663,7 +663,8 @@ static PSI_mutex_info all_innodb_mutexes[] = {
     PSI_MUTEX_KEY(trx_pool_manager_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(temp_pool_manager_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(srv_sys_mutex, 0, 0, PSI_DOCUMENT_ME),
-    PSI_MUTEX_KEY(lock_mutex, 0, 0, PSI_DOCUMENT_ME),
+    PSI_MUTEX_KEY(lock_sys_page_mutex, 0, 0, PSI_DOCUMENT_ME),
+    PSI_MUTEX_KEY(lock_sys_table_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(lock_wait_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(trx_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(srv_threads_mutex, 0, 0, PSI_DOCUMENT_ME),
@@ -699,6 +700,7 @@ static PSI_rwlock_info all_innodb_rwlocks[] = {
     PSI_RWLOCK_KEY(log_sn_lock, 0, PSI_DOCUMENT_ME),
     PSI_RWLOCK_KEY(undo_spaces_lock, 0, PSI_DOCUMENT_ME),
     PSI_RWLOCK_KEY(rsegs_lock, 0, PSI_DOCUMENT_ME),
+    PSI_RWLOCK_KEY(lock_sys_global_rw_lock, 0, PSI_DOCUMENT_ME),
     PSI_RWLOCK_KEY(fts_cache_rw_lock, 0, PSI_DOCUMENT_ME),
     PSI_RWLOCK_KEY(fts_cache_init_rw_lock, 0, PSI_DOCUMENT_ME),
     PSI_RWLOCK_KEY(trx_i_s_cache_lock, 0, PSI_DOCUMENT_ME),
@@ -5515,25 +5517,13 @@ static bool innobase_rollback_to_savepoint_can_release_mdl(
 
   TrxInInnoDB trx_in_innodb(trx);
 
-  /* If transaction has not acquired any locks then it is safe
-  to release MDL after rollback to savepoint.
-  We assume that we are in the thread which is running the transaction, and
-  we check the length of this list without holding trx->mutex nor lock_sys
-  exclusive latch, so at least in theory other threads can concurrently modify
-  this list. However, such modifications are either implicit-to-explicit
-  conversions (which is only possible if trx has any implicit locks, which in
-  turn requires that it has acquired at least one IX table lock, so the list
-  is not empty) or related to B-tree reorganization (which is always performed
-  by first making a copy of a lock and then removing the old lock, so the number
-  of locks can not drop to zero). So, if we are only interested in "emptiness"
-  of the list, we should get accurate result without holding any latch. */
+  trx_mutex_enter(trx);
   ut_ad(thd == current_thd);
   ut_ad(trx->lock.wait_lock == nullptr);
-  if (UT_LIST_GET_LEN(trx->lock.trx_locks) == 0) {
-    return true;
-  }
+  const bool has_no_locks = (UT_LIST_GET_LEN(trx->lock.trx_locks) == 0);
+  trx_mutex_exit(trx);
 
-  return false;
+  return has_no_locks;
 }
 
 /** Release transaction savepoint name.
diff --git a/storage/innobase/handler/p_s.cc b/storage/innobase/handler/p_s.cc
index 84512db936d..4775fb71f54 100644
--- a/storage/innobase/handler/p_s.cc
+++ b/storage/innobase/handler/p_s.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 2016, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 2016, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -382,7 +382,9 @@ static const trx_t *fetch_trx_in_trx_list(uint64_t filter_trx_immutable_id,
                                           trx_ut_list_t *trx_list) {
   const trx_t *trx;
 
-  ut_ad(lock_mutex_own());
+  /* It is not obvious if and why we need lock_sys exclusive access, but we do
+  own exclusive latch here, so treat this assert more as a documentation */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
@@ -583,7 +585,8 @@ bool Innodb_data_lock_iterator::scan(PSI_server_data_lock_container *container,
     return true;
   }
 
-  lock_mutex_enter();
+  /* We want locks reported in a single scan to be a consistent snapshot. */
+  locksys::Global_exclusive_latch_guard guard{};
 
   trx_sys_mutex_enter();
 
@@ -603,8 +606,6 @@ bool Innodb_data_lock_iterator::scan(PSI_server_data_lock_container *container,
 
   trx_sys_mutex_exit();
 
-  lock_mutex_exit();
-
   return false;
 }
 
@@ -629,7 +630,8 @@ bool Innodb_data_lock_iterator::fetch(PSI_server_data_lock_container *container,
     return true;
   }
 
-  lock_mutex_enter();
+  /* scan_trx() requires exclusive global latch to iterate over locks of trx */
+  locksys::Global_exclusive_latch_guard guard{};
 
   trx_sys_mutex_enter();
 
@@ -646,8 +648,6 @@ bool Innodb_data_lock_iterator::fetch(PSI_server_data_lock_container *container,
 
   trx_sys_mutex_exit();
 
-  lock_mutex_exit();
-
   return true;
 }
 
@@ -666,7 +666,9 @@ size_t Innodb_data_lock_iterator::scan_trx_list(
   trx_id_t trx_id;
   size_t found = 0;
 
-  ut_ad(lock_mutex_own());
+  /* We are about to scan over various locks of multiple transactions not
+  limited to any particular shard thus we need an exclusive latch on lock_sys */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
@@ -732,7 +734,7 @@ size_t Innodb_data_lock_iterator::scan_trx(
   ulint heap_no;
   int record_type;
   lock_t *wait_lock;
-
+  ut_ad(locksys::owns_exclusive_global_latch());
   wait_lock = trx->lock.wait_lock;
 
   trx_id = trx_get_id_for_print(trx);
@@ -856,7 +858,8 @@ bool Innodb_data_lock_wait_iterator::scan(
     return true;
   }
 
-  lock_mutex_enter();
+  /* We want locks reported in a single scan to be a consistent snapshot. */
+  locksys::Global_exclusive_latch_guard guard{};
 
   trx_sys_mutex_enter();
 
@@ -874,8 +877,6 @@ bool Innodb_data_lock_wait_iterator::scan(
 
   trx_sys_mutex_exit();
 
-  lock_mutex_exit();
-
   return false;
 }
 
@@ -915,7 +916,8 @@ bool Innodb_data_lock_wait_iterator::fetch(
     return true;
   }
 
-  lock_mutex_enter();
+  /* scan_trx() requires exclusive global latch to iterate over locks of trx */
+  locksys::Global_exclusive_latch_guard guard{};
 
   trx_sys_mutex_enter();
 
@@ -934,8 +936,6 @@ bool Innodb_data_lock_wait_iterator::fetch(
 
   trx_sys_mutex_exit();
 
-  lock_mutex_exit();
-
   return true;
 }
 
@@ -952,7 +952,9 @@ size_t Innodb_data_lock_wait_iterator::scan_trx_list(
   trx_id_t trx_id;
   size_t found = 0;
 
-  ut_ad(lock_mutex_own());
+  /* We are about to scan over various locks of multiple transactions not
+  limited to any particular shard thus we need an exclusive latch on lock_sys */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
@@ -1007,6 +1009,7 @@ size_t Innodb_data_lock_wait_iterator::scan_trx(
   const void *blocking_identity;
   char blocking_engine_lock_id[TRX_I_S_LOCK_ID_MAX_LEN + 1];
   size_t blocking_engine_lock_id_length;
+  ut_ad(locksys::owns_exclusive_global_latch());
   lock_t *wait_lock = trx->lock.wait_lock;
   const lock_t *curr_lock;
   int requesting_record_type;
diff --git a/storage/innobase/include/buf0buf.h b/storage/innobase/include/buf0buf.h
index 13e999a3f54..19f95b1cc0f 100644
--- a/storage/innobase/include/buf0buf.h
+++ b/storage/innobase/include/buf0buf.h
@@ -401,7 +401,7 @@ bool buf_page_get_known_nowait(ulint rw_latch, buf_block_t *block,
 
 /** Given a tablespace id and page number tries to get that page. If the
 page is not in the buffer pool it is not loaded and NULL is returned.
-Suitable for using when holding the lock_sys_t::mutex.
+Suitable for using when holding the lock_sys latches (as it avoids deadlock).
 @param[in]	page_id	page id
 @param[in]	file	file name
 @param[in]	line	line where called
@@ -411,9 +411,9 @@ const buf_block_t *buf_page_try_get_func(const page_id_t &page_id,
                                          const char *file, ulint line,
                                          mtr_t *mtr);
 
-/** Tries to get a page.
-If the page is not in the buffer pool it is not loaded. Suitable for using
-when holding the lock_sys_t::mutex.
+/** Given a tablespace id and page number tries to get that page. If the
+page is not in the buffer pool it is not loaded and NULL is returned.
+Suitable for using when holding the lock_sys latches (as it avoids deadlock).
 @param[in]	page_id	page identifier
 @param[in]	mtr	mini-transaction
 @return the page if in buffer pool, NULL if not */
@@ -1478,6 +1478,10 @@ struct buf_block_t {
   new mutex in InnoDB-5.1 to relieve contention on the buffer pool mutex */
   BPageMutex mutex;
 
+  /** Get the page number and space id of the current buffer block.
+  @return page number of the current buffer block. */
+  const page_id_t &get_page_id() const { return page.id; }
+
   /** Get the page number of the current buffer block.
   @return page number of the current buffer block. */
   page_no_t get_page_no() const { return (page.id.page_no()); }
diff --git a/storage/innobase/include/dict0mem.h b/storage/innobase/include/dict0mem.h
index a9637f79eae..3244c634383 100644
--- a/storage/innobase/include/dict0mem.h
+++ b/storage/innobase/include/dict0mem.h
@@ -1883,8 +1883,8 @@ detect this and will eventually quit sooner. */
   /* The actual collection of tables locked during AUTOINC read/write is
   kept in trx_t. In order to quickly determine whether a transaction has
   locked the AUTOINC lock we keep a pointer to the transaction here in
-  the 'autoinc_trx' member. This is to avoid acquiring the
-  lock_sys_t::mutex and scanning the vector in trx_t.
+  the 'autoinc_trx' member. This is to avoid acquiring lock_sys latches and
+  scanning the vector in trx_t.
   When an AUTOINC lock has to wait, the corresponding lock instance is
   created on the trx lock heap rather than use the pre-allocated instance
   in autoinc_lock below. */
@@ -1933,9 +1933,13 @@ detect this and will eventually quit sooner. */
   be no conflict to access it, so no protection is needed. */
   ulint autoinc_field_no;
 
-  /** The transaction that currently holds the the AUTOINC lock on this
-  table. Protected by lock_sys->mutex. */
-  const trx_t *autoinc_trx;
+  /** The transaction that currently holds the the AUTOINC lock on this table.
+  Protected by lock_sys table shard latch. To "peek" the current value one
+  can read it without any latch, understanding that in general it may change.
+  Such access pattern is correct if trx thread wants to check if it has the lock
+  granted, as the field can only change to other value when lock is released,
+  which can not happen concurrently to thread executing the trx. */
+  std::atomic<const trx_t *> autoinc_trx;
 
   /* @} */
 
@@ -1951,8 +1955,13 @@ detect this and will eventually quit sooner. */
 
   /** Count of the number of record locks on this table. We use this to
   determine whether we can evict the table from the dictionary cache.
-  It is protected by lock_sys->mutex. */
-  ulint n_rec_locks;
+  Writes (atomic increments and decrements) are performed when holding a shared
+  latch on lock_sys. (Note that this the table's shard latch is NOT required,
+  as this is field counts *record* locks, so a page shard is latched instead)
+  Reads should be performed when holding exclusive lock_sys latch, however:
+  - Some places assert this field is zero without holding any latch.
+  - Some places assert this field is positive holding only shared latch. */
+  std::atomic<size_t> n_rec_locks;
 
 #ifndef UNIV_DEBUG
  private:
@@ -1964,7 +1973,7 @@ detect this and will eventually quit sooner. */
 
  public:
 #ifndef UNIV_HOTBACKUP
-  /** List of locks on the table. Protected by lock_sys->mutex. */
+  /** List of locks on the table. Protected by lock_sys shard latch. */
   table_lock_list_t locks;
   /** count_by_mode[M] = number of locks in this->locks with
   lock->type_mode&LOCK_MODE_MASK == M.
@@ -1972,12 +1981,12 @@ detect this and will eventually quit sooner. */
   modes incompatible with LOCK_IS and LOCK_IX, to avoid costly iteration over
   this->locks when adding LOCK_IS or LOCK_IX.
   We use count_by_mode[LOCK_AUTO_INC] to track the number of granted and pending
-  autoinc locks on this table. This value is set after acquiring the
-  lock_sys_t::mutex but we peek the contents to determine whether other
+  autoinc locks on this table. This value is set after acquiring the lock_sys
+  table shard latch, but we peek the contents to determine whether other
   transactions have acquired the AUTOINC lock or not. Of course only one
   transaction can be granted the lock but there can be multiple
   waiters.
-  Protected by lock_sys->mutex. */
+  Protected by lock_sys table shard latch. */
   ulong count_by_mode[LOCK_NUM];
 #endif /* !UNIV_HOTBACKUP */
 
diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
new file mode 100644
index 00000000000..8b50d0efad1
--- /dev/null
+++ b/storage/innobase/include/lock0guards.h
@@ -0,0 +1,147 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+
+#ifndef lock0guards_h
+#define lock0guards_h
+
+#include "lock0lock.h"
+#include "ut0class_life_cycle.h"
+
+namespace locksys {
+/**
+A RAII helper which latches global_latch in exclusive mode during constructor,
+and unlatches it during destruction, preventing any other threads from activity
+within lock_sys for it's entire scope.
+*/
+class Global_exclusive_latch_guard : private ut::Non_copyable {
+ public:
+  Global_exclusive_latch_guard();
+  ~Global_exclusive_latch_guard();
+};
+
+/**
+A RAII helper which tries to exclusively latch the global_lach in constructor
+and unlatches it, if needed, during destruction, preventing any other threads
+from activity within lock_sys for it's entire scope, if owns_lock().
+*/
+class Global_exclusive_try_latch : private ut::Non_copyable {
+ public:
+  Global_exclusive_try_latch();
+  ~Global_exclusive_try_latch();
+  /** Checks if succeeded to latch the global_latch during construction.
+  @return true iff the current thread owns (through this instance) the exclusive
+          global lock_sys latch */
+  bool owns_lock() const noexcept { return m_owns_exclusive_global_latch; }
+
+ private:
+  /** Did the constructor succeed to acquire exclusive global lock_sys latch? */
+  bool m_owns_exclusive_global_latch;
+};
+
+/**
+A RAII helper which latches global_latch in shared mode during constructor,
+and unlatches it during destruction, preventing any other thread from acquiring
+exclusive latch. This should be used in combination Shard_naked_latch_guard,
+preferably by simply using Shard_latch_guard which combines the two for you.
+*/
+class Global_shared_latch_guard : private ut::Non_copyable {
+ public:
+  Global_shared_latch_guard();
+  ~Global_shared_latch_guard();
+};
+
+/**
+A RAII helper which latches the mutex protecting given shard during constructor,
+and unlatches it during destruction.
+You quite probably don't want to use this class, which only takes a shard's
+latch, without acquiring global_latch - which gives no protection from threads
+which latch only the global_latch exclusively to prevent any activity.
+You should use it in combination with Global_shared_latch_guard, so that you
+first obtain an s-latch on the global_latch, or simply use the Shard_latch_guard
+class which already combines the two for you.
+*/
+class Shard_naked_latch_guard : private ut::Non_copyable {
+  explicit Shard_naked_latch_guard(Lock_mutex &shard_mutex);
+
+ public:
+  explicit Shard_naked_latch_guard(const dict_table_t &table);
+
+  explicit Shard_naked_latch_guard(const page_id_t &page_id);
+
+  ~Shard_naked_latch_guard();
+
+ private:
+  /** The mutex protecting the shard requested in constructor */
+  Lock_mutex &m_shard_mutex;
+};
+
+/**
+A RAII wrapper class which combines Global_shared_latch_guard and
+Shard_naked_latch_guard to s-latch the global lock_sys latch and latch the mutex
+protecting the specified shard for the duration of its scope.
+The order of initialization is important: we have to take shared global latch
+BEFORE we attempt to use hash function to compute correct shard and latch it. */
+class Shard_latch_guard {
+  Global_shared_latch_guard m_global_shared_latch_guard;
+  Shard_naked_latch_guard m_shard_naked_latch_guard;
+
+ public:
+  explicit Shard_latch_guard(const dict_table_t &table)
+      : m_global_shared_latch_guard{}, m_shard_naked_latch_guard{table} {}
+
+  explicit Shard_latch_guard(const page_id_t &page_id)
+      : m_global_shared_latch_guard{}, m_shard_naked_latch_guard{page_id} {}
+};
+
+/**
+A RAII wrapper class which s-latches the global lock_sys shard, and mutexes
+protecting specified shards for the duration of its scope.
+It makes sure to take the latches in correct order and handles the case where
+both pages are in the same shard correctly.
+*/
+class Shard_latches_guard {
+  explicit Shard_latches_guard(Lock_mutex &shard_mutex_a,
+                               Lock_mutex &shard_mutex_b);
+
+ public:
+  explicit Shard_latches_guard(const buf_block_t &block_a,
+                               const buf_block_t &block_b);
+
+  ~Shard_latches_guard();
+
+ private:
+  Global_shared_latch_guard m_global_shared_latch_guard;
+  /** The "smallest" of the two shards' mutexes in the latching order */
+  Lock_mutex &m_shard_mutex_1;
+  /** The "largest" of the two shards' mutexes in the latching order */
+  Lock_mutex &m_shard_mutex_2;
+  /** The ordering on shard mutexes used to avoid deadlocks */
+  static constexpr std::less<Lock_mutex *> MUTEX_ORDER{};
+};
+
+}  // namespace locksys
+
+#endif /* lock0guards_h */
diff --git a/storage/innobase/include/lock0latches.h b/storage/innobase/include/lock0latches.h
new file mode 100644
index 00000000000..cc28380f98a
--- /dev/null
+++ b/storage/innobase/include/lock0latches.h
@@ -0,0 +1,300 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+#ifndef lock0latches_h
+#define lock0latches_h
+
+#include "sync0sharded_rw.h"
+#include "ut0cpu_cache.h"
+#include "ut0mutex.h"
+
+/* Forward declarations */
+struct dict_table_t;
+class page_id_t;
+
+namespace locksys {
+/**
+The class which handles the logic of latching of lock_sys queues themselves.
+The lock requests for table locks and record locks are stored in queues, and to
+allow concurrent operations on these queues, we need a mechanism to latch these
+queues in safe and quick fashion.
+In the past we had a single latch which protected access to all of them.
+Now, we use more granular approach.
+In extreme, one could imagine protecting each queue with a separate latch.
+To avoid having too many latch objects, and having to create and remove them on
+demand, we use a more conservative approach.
+The queues are grouped into a fixed number of shards, and each shard is
+protected by its own mutex.
+
+However, there are several rare events in which we need to "stop the world" -
+latch all queues, to prevent any activity inside lock-sys.
+One way to accomplish this would be to simply latch all the shards one by one,
+but it turns out to be way too slow in debug runs, where such "stop the world"
+events are very frequent due to lock_sys validation.
+
+To allow for efficient latching of everything, we've introduced a global_latch,
+which is a read-write latch.
+Most of the time, we operate on one or two shards, in which case it is
+sufficient to s-latch the global_latch and then latch shard's mutex.
+For the "stop the world" operations, we x-latch the global_latch, which prevents
+any other thread from latching any shard.
+
+However, it turned out that on ARM architecture, the default implementation of
+read-write latch (rw_lock_t) is too slow because increments and decrements of
+the number of s-latchers is implemented as read-update-try-to-write loop, which
+means multiple threads try to modify the same cache line disrupting each other.
+Therefore, we use a sharded version of read-write latch (Sharded_rw_lock), which
+internally uses multiple instances of rw_lock_t, spreading the load over several
+cache lines. Note that this sharding is a technical internal detail of the
+global_latch, which for all other purposes can be treated as a single entity.
+
+This his how this conceptually looks like:
+```
+  [                           global latch                                ]
+                                  |
+                                  v
+  [table shard 1] ... [table shard 512] [page shard 1] ... [page shard 512]
+
+```
+
+So, for example access two queues for two records involves following steps:
+1. s-latch the global_latch
+2. identify the 2 pages to which the records belong
+3. identify the lock_sys 2 hash buckets which contain the queues for given pages
+4. identify the 2 shard ids which contain these two buckets
+5. latch mutexes for the two shards in the order of their addresses
+
+All of the steps above (except 2, as we usually know the page already) are
+accomplished with the help of single line:
+
+    locksys::Shard_latches_guard guard{*block_a, *block_b};
+
+And to "stop the world" one can simply x-latch the global latch by using:
+
+    locksys::Global_exclusive_latch_guard guard{};
+
+This class does not expose too many public functions, as the intention is to
+rather use friend guard classes, like the Shard_latches_guard demonstrated.
+*/
+class Latches {
+ private:
+  using Lock_mutex = ib_mutex_t;
+
+  /** A helper wrapper around Shared_rw_lock which simplifies:
+    - lifecycle by providing constructor and destructor, and
+    - s-latching and s-unlatching by keeping track of the shard id used for
+      spreading the contention.
+  There must be at most one instance of this class (the one in the lock_sys), as
+  it uses thread_local-s to remember which shard of sharded rw lock was used by
+  this thread to perform s-latching (so, hypothetical other instances would
+  share this field, overwriting it and leading to errors). */
+  class Unique_sharded_rw_lock {
+    /** The actual rw_lock implementation doing the heavy lifting */
+    Sharded_rw_lock rw_lock;
+
+    /** The value used for m_shard_id to indicate that current thread did not
+    s-latch any of the rw_lock's shards */
+    static constexpr size_t NOT_IN_USE = std::numeric_limits<size_t>::max();
+
+    /** The id of the rw_lock's shard which this thread has s-latched, or
+    NOT_IN_USE if it has not s-latched any*/
+    static thread_local size_t m_shard_id;
+
+   public:
+    Unique_sharded_rw_lock();
+    ~Unique_sharded_rw_lock();
+    bool try_x_lock() { return rw_lock.try_x_lock(); }
+    void x_lock() { rw_lock.x_lock(); }
+    void x_unlock() { rw_lock.x_unlock(); }
+    void s_lock() {
+      ut_ad(m_shard_id == NOT_IN_USE);
+      m_shard_id = rw_lock.s_lock();
+    }
+    void s_unlock() {
+      ut_ad(m_shard_id != NOT_IN_USE);
+      rw_lock.s_unlock(m_shard_id);
+      m_shard_id = NOT_IN_USE;
+    }
+#ifdef UNIV_DEBUG
+    bool x_own() const { return rw_lock.x_own(); }
+    bool s_own() const {
+      return m_shard_id != NOT_IN_USE && rw_lock.s_own(m_shard_id);
+    }
+#endif
+  };
+
+  using Padded_mutex = ut::Cacheline_padded<Lock_mutex>;
+
+  /** Number of page shards, and also number of table shards.
+  Must be a power of two */
+  static constexpr size_t SHARDS_COUNT = 512;
+
+  /*
+  Functions related to sharding by page (containing records to lock).
+
+  This must be done in such a way that two pages which share a single lock
+  queue fall into the same shard. We accomplish this by reusing hash function
+  used to determine lock queue, and then group multiple queues into single
+  shard.
+  */
+  class Page_shards {
+    /** Each shard is protected by a separate mutex. Mutexes are padded to avoid
+    false sharing issues with cache. */
+    Padded_mutex mutexes[SHARDS_COUNT];
+    /**
+    Identifies the page shard which contains record locks for records from the
+    given page.
+    @param[in]    page_id    The space_id and page_no of the page
+    @return Integer in the range [0..lock_sys_t::SHARDS_COUNT)
+    */
+    static size_t get_shard(const page_id_t &page_id);
+
+   public:
+    Page_shards();
+    ~Page_shards();
+
+    /**
+    Returns the mutex which (together with the global latch) protects the page
+    shard which contains record locks for records from the given page.
+    @param[in]    page_id    The space_id and page_no of the page
+    @return The mutex responsible for the shard containing the page
+    */
+    const Lock_mutex &get_mutex(const page_id_t &page_id) const;
+
+    /**
+    Returns the mutex which (together with the global latch) protects the page
+    shard which contains record locks for records from the given page.
+    @param[in]    page_id    The space_id and page_no of the page
+    @return The mutex responsible for the shard containing the page
+    */
+    Lock_mutex &get_mutex(const page_id_t &page_id);
+  };
+
+  /*
+  Functions related to sharding by table
+
+  We identify tables by their id. Each table has its own lock queue, so we
+  simply group several such queues into single shard.
+  */
+  class Table_shards {
+    /** Each shard is protected by a separate mutex. Mutexes are padded to avoid
+    false sharing issues with cache. */
+    Padded_mutex mutexes[SHARDS_COUNT];
+    /**
+    Identifies the table shard which contains locks for the given table.
+    @param[in]    table     The table
+    @return Integer in the range [0..lock_sys_t::SHARDS_COUNT)
+    */
+    static size_t get_shard(const dict_table_t &table);
+
+   public:
+    Table_shards();
+    ~Table_shards();
+
+    /** Returns the mutex which (together with the global latch) protects the
+    table shard which contains table locks for the given table.
+    @param[in]    table     The table
+    @return The mutex responsible for the shard containing the table
+    */
+    Lock_mutex &get_mutex(const dict_table_t &table);
+
+    /** Returns the mutex which (together with the global latch) protects the
+    table shard which contains table locks for the given table.
+    @param[in]    table     The table
+    @return The mutex responsible for the shard containing the table
+    */
+    const Lock_mutex &get_mutex(const dict_table_t &table) const;
+  };
+
+  /** padding to prevent other memory update hotspots from residing on the same
+  memory cache line */
+  char pad1[ut::INNODB_CACHE_LINE_SIZE] = {};
+
+  Unique_sharded_rw_lock global_latch;
+
+  Page_shards page_shards;
+
+  Table_shards table_shards;
+
+ public:
+  /* You should use following RAII guards to modify the state of Latches. */
+  friend class Global_exclusive_latch_guard;
+  friend class Global_exclusive_try_latch;
+  friend class Global_shared_latch_guard;
+  friend class Shard_naked_latch_guard;
+  friend class Shard_latch_guard;
+  friend class Shard_latches_guard;
+
+  /** You should not use this functionality in new code.
+  Instead use Global_exclusive_latch_guard.
+  This is intended only to be use within lock0* module, thus this class is only
+  accessible through lock0priv.h.
+  It is only used by lock_rec_fetch_page() as a workaround. */
+  friend class Unsafe_global_latch_manipulator;
+
+  Latches() = default;
+  ~Latches() = default;
+
+#ifdef UNIV_DEBUG
+  /**
+  Tests if lock_sys latch is exclusively owned by the current thread.
+  @return true iff the current thread owns exclusive global lock_sys latch
+  */
+  bool owns_exclusive_global_latch() const { return global_latch.x_own(); }
+
+  /**
+  Tests if lock_sys latch is owned in shared mode by the current thread.
+  @return true iff the current thread owns shared global lock_sys latch
+  */
+  bool owns_shared_global_latch() const { return global_latch.s_own(); }
+
+  /**
+  Tests if given page shard can be safely accessed by the current thread.
+  @param[in]    page_id    The space_id and page_no of the page
+  @return true iff the current thread owns exclusive global lock_sys latch or
+  both a shared global lock_sys latch and mutex protecting the page shard
+  */
+  bool owns_page_shard(const page_id_t &page_id) const {
+    return owns_exclusive_global_latch() ||
+           (page_shards.get_mutex(page_id).is_owned() &&
+            owns_shared_global_latch());
+  }
+
+  /**
+  Tests if given table shard can be safely accessed by the current thread.
+  @param  table   the table
+  @return true iff the current thread owns exclusive global lock_sys latch or
+  both a shared global lock_sys latch and mutex protecting the table shard
+  */
+  bool owns_table_shard(const dict_table_t &table) const {
+    return owns_exclusive_global_latch() ||
+           (table_shards.get_mutex(table).is_owned() &&
+            owns_shared_global_latch());
+  }
+#endif /* UNIV_DEBUG */
+};
+}  // namespace locksys
+
+#endif /* lock0latches_h */
diff --git a/storage/innobase/include/lock0lock.h b/storage/innobase/include/lock0lock.h
index 08a02d35b20..ba98ea8c0a6 100644
--- a/storage/innobase/include/lock0lock.h
+++ b/storage/innobase/include/lock0lock.h
@@ -47,6 +47,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #ifndef UNIV_HOTBACKUP
 #include "gis0rtree.h"
 #endif /* UNIV_HOTBACKUP */
+#include "lock0latches.h"
 #include "lock0prdt.h"
 
 /**
@@ -389,9 +390,10 @@ void lock_rec_restore_from_page_infimum(
                                 the infimum */
 
 /** Determines if there are explicit record locks on a page.
- @return an explicit record lock on the page, or NULL if there are none */
-lock_t *lock_rec_expl_exist_on_page(space_id_t space,  /*!< in: space id */
-                                    page_no_t page_no) /*!< in: page number */
+@param[in]    space     space id
+@param[in]    page_no   page number
+@return true iff an explicit record lock on the page exists */
+bool lock_rec_expl_exist_on_page(space_id_t space, page_no_t page_no)
     MY_ATTRIBUTE((warn_unused_result));
 /** Checks if locks of other transactions prevent an immediate insert of
  a record. If they do, first tests if the query thread should anyway
@@ -608,7 +610,7 @@ void lock_rec_unlock(
  TRX_STATE_COMMITTED_IN_MEMORY. */
 void lock_trx_release_locks(trx_t *trx); /*!< in/out: transaction */
 
-/** Release read locks of a transacion. It is called during XA
+/** Release read locks of a transaction. It is called during XA
 prepare to release locks early.
 @param[in,out]	trx		transaction
 @param[in]	only_gap	release only GAP locks */
@@ -688,12 +690,8 @@ void lock_report_trx_id_insanity(
     trx_id_t max_trx_id);      /*!< in: trx_sys_get_max_trx_id() */
 
 /** Prints info of locks for all transactions.
-@return false if not able to obtain lock mutex and exits without
-printing info */
-bool lock_print_info_summary(
-    FILE *file,   /*!< in: file where to print */
-    ibool nowait) /*!< in: whether to wait for the lock mutex */
-    MY_ATTRIBUTE((warn_unused_result));
+@param[in]  file   file where to print */
+void lock_print_info_summary(FILE *file);
 
 /** Prints transaction lock wait and MVCC state.
 @param[in,out]	file	file where to print
@@ -701,16 +699,18 @@ bool lock_print_info_summary(
 void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx);
 
 /** Prints info of locks for each transaction. This function assumes that the
- caller holds the lock mutex and more importantly it will release the lock
- mutex on behalf of the caller. (This should be fixed in the future). */
-void lock_print_info_all_transactions(
-    FILE *file); /*!< in: file where to print */
+caller holds the exclusive global latch and more importantly it may release and
+reacquire it on behalf of the caller. (This should be fixed in the future).
+@param[in,out] file  the file where to print */
+void lock_print_info_all_transactions(FILE *file);
+
 /** Return approximate number or record locks (bits set in the bitmap) for
  this transaction. Since delete-marked records may be removed, the
  record count will not be precise.
- The caller must be holding lock_sys->mutex. */
-ulint lock_number_of_rows_locked(
-    const trx_lock_t *trx_lock) /*!< in: transaction locks */
+ The caller must be holding exclusive global lock_sys latch.
+ @param[in] trx_lock  transaction locks
+ */
+ulint lock_number_of_rows_locked(const trx_lock_t *trx_lock)
     MY_ATTRIBUTE((warn_unused_result));
 
 /** Return the number of table locks for a transaction.
@@ -795,12 +795,13 @@ space_id_t lock_rec_get_space_id(const lock_t *lock); /*!< in: lock */
 /** For a record lock, gets the page number on which the lock is.
  @return page number */
 page_no_t lock_rec_get_page_no(const lock_t *lock); /*!< in: lock */
+
 /** Check if there are any locks (table or rec) against table.
- @return true if locks exist */
-bool lock_table_has_locks(
-    const dict_table_t *table); /*!< in: check if there are any locks
-                                held on records in this table or on the
-                                table itself */
+Returned value might be obsolete.
+@param[in]  table   the table
+@return true if there were any locks held on records in this table or on the
+table itself at some point in time during the call */
+bool lock_table_has_locks(const dict_table_t *table);
 
 /** A thread which wakes up threads whose lock wait may have lasted too long. */
 void lock_wait_timeout_thread();
@@ -841,6 +842,7 @@ bool lock_check_trx_id_sanity(
     const dict_index_t *index, /*!< in: index */
     const ulint *offsets)      /*!< in: rec_get_offsets(rec, index) */
     MY_ATTRIBUTE((warn_unused_result));
+
 /** Check if the transaction holds an exclusive lock on a record.
 @param[in]  thr     query thread of the transaction
 @param[in]  table   table to check
@@ -850,6 +852,10 @@ bool lock_check_trx_id_sanity(
 bool lock_trx_has_rec_x_lock(que_thr_t *thr, const dict_table_t *table,
                              const buf_block_t *block, ulint heap_no)
     MY_ATTRIBUTE((warn_unused_result));
+
+/** Validates the lock system.
+ @return true if ok */
+bool lock_validate();
 #endif /* UNIV_DEBUG */
 
 /**
@@ -930,52 +936,51 @@ struct lock_op_t {
   lock_mode mode;      /*!< lock mode */
 };
 
-typedef ib_mutex_t LockMutex;
+typedef ib_mutex_t Lock_mutex;
 
 /** The lock system struct */
 struct lock_sys_t {
-  char pad1[INNOBASE_CACHE_LINE_SIZE];
-  /*!< padding to prevent other
-  memory update hotspots from
-  residing on the same memory
-  cache line */
-  LockMutex mutex;              /*!< Mutex protecting the
-                                locks */
-  hash_table_t *rec_hash;       /*!< hash table of the record
-                                locks */
-  hash_table_t *prdt_hash;      /*!< hash table of the predicate
-                                lock */
-  hash_table_t *prdt_page_hash; /*!< hash table of the page
-                                lock */
-
-  char pad2[INNOBASE_CACHE_LINE_SIZE]; /*!< Padding */
-  LockMutex wait_mutex;                /*!< Mutex protecting the
-                                       next two fields */
-  srv_slot_t *waiting_threads;         /*!< Array  of user threads
-                                       suspended while waiting for
-                                       locks within InnoDB, protected
-                                       by the lock_sys->wait_mutex */
-  srv_slot_t *last_slot;               /*!< highest slot ever used
-                                       in the waiting_threads array,
-                                       protected by
-                                       lock_sys->wait_mutex */
-
-  ibool rollback_complete;
-  /*!< TRUE if rollback of all
-  recovered transactions is
-  complete. Protected by
-  lock_sys->mutex */
-
-  ulint n_lock_max_wait_time; /*!< Max wait time */
-
-  os_event_t timeout_event; /*!< Set to the event that is
-                            created in the lock wait monitor
-                            thread. A value of 0 means the
-                            thread is not active */
+  /** The latches protecting queues of record and table locks */
+  locksys::Latches latches;
+
+  /** The hash table of the record (LOCK_REC) locks, except for predicate
+  (LOCK_PREDICATE) and predicate page (LOCK_PRDT_PAGE) locks */
+  hash_table_t *rec_hash;
+
+  /** The hash table of predicate (LOCK_PREDICATE) locks */
+  hash_table_t *prdt_hash;
+
+  /** The hash table of the predicate page (LOCK_PRD_PAGE) locks */
+  hash_table_t *prdt_page_hash;
+
+  /** Padding to avoid false sharing of wait_mutex field */
+  char pad2[ut::INNODB_CACHE_LINE_SIZE];
+
+  /** The mutex protecting the next two fields */
+  Lock_mutex wait_mutex;
+
+  /** Array of user threads suspended while waiting for locks within InnoDB.
+  Protected by the lock_sys->wait_mutex. */
+  srv_slot_t *waiting_threads;
+
+  /** The highest slot ever used in the waiting_threads array.
+  Protected by lock_sys->wait_mutex. */
+  srv_slot_t *last_slot;
+
+  /** TRUE if rollback of all recovered transactions is complete.
+  Protected by exclusive global lock_sys latch. */
+  bool rollback_complete;
+
+  /** Max lock wait time observed, for innodb_row_lock_time_max reporting. */
+  ulint n_lock_max_wait_time;
+
+  /** Set to the event that is created in the lock wait monitor thread. A value
+  of 0 means the thread is not active */
+  os_event_t timeout_event;
 
 #ifdef UNIV_DEBUG
-  /** Lock timestamp counter */
-  uint64_t m_seq;
+  /** Lock timestamp counter, used to assign lock->m_seq on creation. */
+  std::atomic<uint64_t> m_seq;
 #endif /* UNIV_DEBUG */
 };
 
@@ -1023,27 +1028,12 @@ void lock_rec_trx_wait(lock_t *lock, ulint i, ulint type);
 /** The lock system */
 extern lock_sys_t *lock_sys;
 
-/** Test if lock_sys->mutex can be acquired without waiting. */
-#define lock_mutex_enter_nowait() (lock_sys->mutex.trylock(__FILE__, __LINE__))
-
-/** Test if lock_sys->mutex is owned by the current thread. */
-#define lock_mutex_own() (lock_sys->mutex.is_owned())
-
-/** Acquire the lock_sys->mutex. */
-#define lock_mutex_enter()         \
-  do {                             \
-    mutex_enter(&lock_sys->mutex); \
-  } while (0)
-
-/** Release the lock_sys->mutex. */
-#define lock_mutex_exit()   \
-  do {                      \
-    lock_sys->mutex.exit(); \
-  } while (0)
-
+#ifdef UNIV_DEBUG
 /** Test if lock_sys->wait_mutex is owned. */
 #define lock_wait_mutex_own() (lock_sys->wait_mutex.is_owned())
 
+#endif
+
 /** Acquire the lock_sys->wait_mutex. */
 #define lock_wait_mutex_enter()         \
   do {                                  \
@@ -1058,4 +1048,51 @@ extern lock_sys_t *lock_sys;
 
 #include "lock0lock.ic"
 
+namespace locksys {
+
+/* OWNERSHIP TESTS */
+#ifdef UNIV_DEBUG
+
+/**
+Tests if lock_sys latch is exclusively owned by the current thread.
+@return true iff the current thread owns exclusive global lock_sys latch
+*/
+bool owns_exclusive_global_latch();
+
+/**
+Tests if lock_sys latch is owned in shared mode by the current thread.
+@return true iff the current thread owns shared global lock_sys latch
+*/
+bool owns_shared_global_latch();
+
+/**
+Tests if given page shard can be safely accessed by the current thread.
+@param  page_id    specifies the page
+@return true iff the current thread owns exclusive global lock_sys latch or both
+a shared global lock_sys latch and mutex protecting the page shard
+*/
+bool owns_page_shard(const page_id_t &page_id);
+
+/**
+Test if given table shard can be safely accessed by the current thread.
+@param  table   the table
+@return true iff the current thread owns exclusive global lock_sys latch or both
+        a shared global lock_sys latch and mutex protecting the table shard
+*/
+bool owns_table_shard(const dict_table_t &table);
+
+/** Checks if shard which contains lock is latched (or that an exclusive latch
+on whole lock_sys is held) by current thread
+@param[in]  lock   lock which belongs to a shard we want to check
+@return true iff the current thread owns exclusive global lock_sys latch or both
+        a shared global lock_sys latch and mutex protecting the shard containing
+        the specified lock */
+bool owns_lock_shard(const lock_t *lock);
+
+#endif /* UNIV_DEBUG */
+
+}  // namespace locksys
+
+#include "lock0guards.h"
+
 #endif
diff --git a/storage/innobase/include/lock0lock.ic b/storage/innobase/include/lock0lock.ic
index 5051a49d6dd..7826c662b6f 100644
--- a/storage/innobase/include/lock0lock.ic
+++ b/storage/innobase/include/lock0lock.ic
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1996, 2018, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1996, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
diff --git a/storage/innobase/include/lock0priv.h b/storage/innobase/include/lock0priv.h
index 318b72c7a8e..cb2903b0858 100644
--- a/storage/innobase/include/lock0priv.h
+++ b/storage/innobase/include/lock0priv.h
@@ -86,6 +86,7 @@ struct lock_rec_t {
                      placed immediately after the
                      lock struct */
 
+  page_id_t get_page_id() const { return page_id_t(space, page_no); }
   /** Print the record lock into the given output stream
   @param[in,out]	out	the output stream
   @return the given output stream. */
@@ -128,7 +129,7 @@ bool lock_mode_is_next_key_lock(ulint mode) {
 UNIV_INLINE
 bool lock_rec_get_nth_bit(const lock_t *lock, ulint i);
 
-/** Lock struct; protected by lock_sys->mutex */
+/** Lock struct; protected by lock_sys latches */
 struct lock_t {
   /** transaction owning the lock */
   trx_t *trx;
@@ -612,26 +613,19 @@ struct RecID {
   @param[in]	lock		Record lock
   @param[in]	heap_no		Heap number in the page */
   RecID(const lock_t *lock, ulint heap_no)
-      : m_space_id(lock->rec_lock.space),
-        m_page_no(lock->rec_lock.page_no),
-        m_heap_no(static_cast<uint32_t>(heap_no)),
-        m_fold(lock_rec_fold(m_space_id, m_page_no)) {
-    ut_ad(m_space_id < UINT32_MAX);
-    ut_ad(m_page_no < UINT32_MAX);
-    ut_ad(m_heap_no < UINT32_MAX);
+      : RecID(lock->rec_lock.get_page_id(), heap_no) {
+    ut_ad(lock->is_record_lock());
   }
 
   /** Constructor
-  @param[in]	space_id	Tablespace ID
-  @param[in]	page_no		Page number in space_id
-  @param[in]	heap_no		Heap number in <space_id, page_no> */
-  RecID(space_id_t space_id, page_no_t page_no, ulint heap_no)
-      : m_space_id(space_id),
-        m_page_no(page_no),
-        m_heap_no(static_cast<uint32_t>(heap_no)),
-        m_fold(lock_rec_fold(m_space_id, m_page_no)) {
-    ut_ad(m_space_id < UINT32_MAX);
-    ut_ad(m_page_no < UINT32_MAX);
+  @param[in]	page_id		Tablespace ID and page number within space
+  @param[in]	heap_no		Heap number in the page */
+  RecID(page_id_t page_id, uint32_t heap_no)
+      : m_page_id(page_id),
+        m_heap_no(heap_no),
+        m_fold(lock_rec_fold(page_id.space(), page_id.page_no())) {
+    ut_ad(m_page_id.space() < UINT32_MAX);
+    ut_ad(m_page_id.page_no() < UINT32_MAX);
     ut_ad(m_heap_no < UINT32_MAX);
   }
 
@@ -639,12 +633,7 @@ struct RecID {
   @param[in]	block		Block in a tablespace
   @param[in]	heap_no		Heap number in the block */
   RecID(const buf_block_t *block, ulint heap_no)
-      : m_space_id(block->page.id.space()),
-        m_page_no(block->page.id.page_no()),
-        m_heap_no(static_cast<uint32_t>(heap_no)),
-        m_fold(lock_rec_fold(m_space_id, m_page_no)) {
-    ut_ad(heap_no < UINT32_MAX);
-  }
+      : RecID(block->get_page_id(), heap_no) {}
 
   /**
   @return the "folded" value of {space, page_no} */
@@ -658,13 +647,10 @@ struct RecID {
   @return true if <space, page_no, heap_no> matches the lock. */
   inline bool matches(const lock_t *lock) const;
 
-  /**
-  Tablespace ID */
-  space_id_t m_space_id;
+  const page_id_t &get_page_id() const { return m_page_id; }
 
-  /**
-  Page number within the space ID */
-  page_no_t m_page_no;
+  /** Tablespace ID and page number within space  */
+  page_id_t m_page_id;
 
   /**
   Heap number within the page */
@@ -801,7 +787,7 @@ class RecLock {
   /**
   Setup the context from the requirements */
   void init(const page_t *page) {
-    ut_ad(lock_mutex_own());
+    ut_ad(locksys::owns_page_shard(m_rec_id.get_page_id()));
     ut_ad(!srv_read_only_mode);
     ut_ad(m_index->is_clustered() || !dict_index_is_online_ddl(m_index));
     ut_ad(m_thr == nullptr || m_trx == thr_get_trx(m_thr));
@@ -1070,7 +1056,7 @@ struct Lock_iter {
   @param[in]	lock		The current lock
   @return matching lock or nullptr if end of list */
   static lock_t *advance(const RecID &rec_id, lock_t *lock) {
-    ut_ad(lock_mutex_own());
+    ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
     ut_ad(lock->is_record_lock());
 
     while ((lock = static_cast<lock_t *>(lock->hash)) != nullptr) {
@@ -1090,7 +1076,7 @@ struct Lock_iter {
   @param[in]	rec_id		Record ID
   @return	first lock, nullptr if none exists */
   static lock_t *first(hash_cell_t *list, const RecID &rec_id) {
-    ut_ad(lock_mutex_own());
+    ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
 
     auto lock = static_cast<lock_t *>(list->node);
 
@@ -1111,7 +1097,7 @@ struct Lock_iter {
   template <typename F>
   static const lock_t *for_each(const RecID &rec_id, F &&f,
                                 hash_table_t *hash_table = lock_sys->rec_hash) {
-    ut_ad(lock_mutex_own());
+    ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
 
     auto list = hash_get_nth_cell(hash_table,
                                   hash_calc_hash(rec_id.m_fold, hash_table));
@@ -1129,4 +1115,12 @@ struct Lock_iter {
   }
 };
 
+namespace locksys {
+class Unsafe_global_latch_manipulator {
+ public:
+  static void exclusive_unlatch() { lock_sys->latches.global_latch.x_unlock(); }
+  static void exclusive_latch() { lock_sys->latches.global_latch.x_lock(); }
+};
+}  // namespace locksys
+
 #endif /* lock0priv_h */
diff --git a/storage/innobase/include/lock0priv.ic b/storage/innobase/include/lock0priv.ic
index 5d95c90a5d4..a9eb6270c8f 100644
--- a/storage/innobase/include/lock0priv.ic
+++ b/storage/innobase/include/lock0priv.ic
@@ -109,7 +109,7 @@ lock_t *lock_rec_get_first_on_page_addr(
     space_id_t space,        /*!< in: space */
     page_no_t page_no)       /*!< in: page number */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(page_id_t{space, page_no}));
 
   for (lock_t *lock = static_cast<lock_t *>(
            HASH_GET_FIRST(lock_hash, lock_rec_hash(space, page_no)));
@@ -131,7 +131,7 @@ lock_t *lock_rec_get_first_on_page(
     hash_table_t *lock_hash,  /*!< in: lock hash table */
     const buf_block_t *block) /*!< in: buffer block */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   space_id_t space = block->page.id.space();
   page_no_t page_no = block->page.id.page_no();
@@ -155,7 +155,7 @@ UNIV_INLINE
 lock_t *lock_rec_get_next(ulint heap_no, /*!< in: heap number of the record */
                           lock_t *lock)  /*!< in: lock */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
 
   do {
     ut_ad(lock_get_type_low(lock) == LOCK_REC);
@@ -184,7 +184,7 @@ lock_t *lock_rec_get_first(
     const buf_block_t *block, /*!< in: block containing the record */
     ulint heap_no)            /*!< in: heap number of the record */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   for (lock_t *lock = lock_rec_get_first_on_page(hash, block); lock;
        lock = lock_rec_get_next_on_page(lock)) {
@@ -222,15 +222,13 @@ UNIV_INLINE
 const lock_t *lock_rec_get_next_on_page_const(
     const lock_t *lock) /*!< in: a record lock */
 {
-  ut_ad(lock_mutex_own());
   ut_ad(lock_get_type_low(lock) == LOCK_REC);
-
-  space_id_t space = lock->space_id();
-  page_no_t page_no = lock->page_no();
+  const auto page_id = lock->rec_lock.get_page_id();
+  ut_ad(locksys::owns_page_shard(page_id));
 
   while ((lock = static_cast<const lock_t *>(HASH_GET_NEXT(hash, lock))) !=
          nullptr) {
-    if (lock->space_id() == space && lock->page_no() == page_no) {
+    if (page_id.equals_to(lock->rec_lock.get_page_id())) {
       return (lock);
     }
   }
@@ -283,10 +281,9 @@ ulint lock_get_wait(const lock_t *lock) /*!< in: lock */
 UNIV_INLINE
 void lock_reset_lock_and_trx_wait(lock_t *lock) /*!< in/out: record lock */
 {
-  ut_ad(lock->trx->lock.wait_lock == lock);
+  ut_ad(locksys::owns_lock_shard(lock));
   ut_ad(lock_get_wait(lock));
-  ut_ad(lock_mutex_own());
-
+  ut_ad(lock->trx->lock.wait_lock == lock);
   /* We intentionally don't clear trx->lock.blocking_trx here, as
   lock_reset_lock_and_trx_wait() is called also during movements of locks from
   one page to another, which does not really change the structure of the
@@ -294,6 +291,7 @@ void lock_reset_lock_and_trx_wait(lock_t *lock) /*!< in/out: record lock */
   is responsible for clearing the blocking_trx field once it is sure that
   we really want to remove the edge from the wait-for graph.*/
   lock->trx->lock.wait_lock = nullptr;
+
   /* We intentionally don't clear lock->trx->lock.wait_lock_type here, to make
   it easier to obtain stats about the last wait in lock_wait_suspend_thread().
   @see trx_lock_t::wait_lock_type for more detailed explanation. */
@@ -346,8 +344,7 @@ bool lock_table_has(const trx_t *trx, const dict_table_t *table,
 @param[i]	lock		Lock to compare with
 @return true if <space, page_no, heap_no> matches the lock. */
 bool RecID::matches(const lock_t *lock) const {
-  return (lock->rec_lock.space == m_space_id &&
-          lock->rec_lock.page_no == m_page_no &&
+  return (lock->rec_lock.get_page_id().equals_to(get_page_id()) &&
           lock_rec_get_nth_bit(lock, m_heap_no));
 }
 
diff --git a/storage/innobase/include/log0types.h b/storage/innobase/include/log0types.h
index a9e252c2577..544d15dca28 100644
--- a/storage/innobase/include/log0types.h
+++ b/storage/innobase/include/log0types.h
@@ -133,7 +133,7 @@ struct Log_handle {
 
 /** Redo log - single data structure with state of the redo log system.
 In future, one could consider splitting this to multiple data structures. */
-struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
+struct alignas(ut::INNODB_CACHE_LINE_SIZE) log_t {
   /**************************************************/ /**
 
    @name Users writing to log buffer
@@ -148,7 +148,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   Log archiver (Clone plugin) acquires x-lock. */
   mutable Sharded_rw_lock sn_lock;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Current sn value. Used to reserve space in the redo log,
       and used to acquire an exclusive access to the log buffer.
@@ -160,7 +160,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
 
   /** Padding after the _sn to avoid false sharing issues for
   constants below (due to changes of sn). */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Pointer to the log buffer, aligned up to OS_FILE_LOG_BLOCK_SIZE.
       The alignment is to ensure that buffer parts specified for file IO write
@@ -177,19 +177,19 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   that is including bytes for headers and footers of log blocks. */
   size_t buf_size;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** The recent written buffer.
       Protected by: sn_lock or writer_mutex. */
       Link_buf<lsn_t> recent_written;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** The recent closed buffer.
       Protected by: sn_lock or closer_mutex. */
       Link_buf<lsn_t> recent_closed;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -208,14 +208,14 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
       Protected by: writer_mutex (writes). */
       atomic_sn_t buf_limit_sn;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Up to this lsn, data has been written to disk (fsync not required).
       Protected by: writer_mutex (writes).
       @see @ref subsect_redo_log_write_lsn */
       atomic_lsn_t write_lsn;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Unaligned pointer to array with events, which are used for
       notifications sent from the log write notifier thread to user threads.
@@ -231,17 +231,17 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   size_t write_events_size;
 
   /** Approx. number of requests to write/flush redo since startup. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
       std::atomic<uint64_t> write_to_file_requests_total;
 
   /** How often redo write/flush is requested in average.
   Measures in microseconds. Log threads do not spin when
   the write/flush requests are not frequent. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
       std::atomic<uint64_t> write_to_file_requests_interval;
 
   /** This padding is probably not needed, left for convenience. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -268,13 +268,13 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   size_t flush_events_size;
 
   /** Padding before the frequently updated flushed_to_disk_lsn. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Up to this lsn data has been flushed to disk (fsynced). */
       atomic_lsn_t flushed_to_disk_lsn;
 
   /** Padding after the frequently updated flushed_to_disk_lsn. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -299,13 +299,13 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   /** Mutex which can be used to pause log flusher thread. */
   mutable ib_mutex_t flusher_mutex;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       os_event_t flusher_event;
 
   /** Padding to avoid any dependency between the log flusher
   and the log writer threads. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -387,13 +387,13 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   /** Mutex which can be used to pause log writer thread. */
   mutable ib_mutex_t writer_mutex;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       os_event_t writer_event;
 
   /** Padding after section for the log writer thread, to avoid any
   dependency between the log writer and the log closer threads. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -413,7 +413,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
 
   /** Padding after the log closer thread and before the memory used
   for communication between the log flusher and notifier threads. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -435,7 +435,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   mutable ib_mutex_t flush_notifier_mutex;
 
   /** Padding. */
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -450,14 +450,14 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
       /** Mutex which can be used to pause log write notifier thread. */
       mutable ib_mutex_t write_notifier_mutex;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** Event used by the log writer thread to notify the log write
       notifier thread, that it should proceed with notifying user threads
       waiting for the advanced write_lsn (because it has been advanced). */
       os_event_t write_notifier_event;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -515,7 +515,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
 
 #endif /* UNIV_DEBUG */
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
@@ -601,7 +601,7 @@ struct alignas(INNOBASE_CACHE_LINE_SIZE) log_t {
   Protected by (updates only): limits_mutex. */
   atomic_sn_t dict_persist_margin;
 
-  alignas(INNOBASE_CACHE_LINE_SIZE)
+  alignas(ut::INNODB_CACHE_LINE_SIZE)
 
       /** @} */
 
diff --git a/storage/innobase/include/os0file.h b/storage/innobase/include/os0file.h
index f74f18ba884..946d7803a9d 100644
--- a/storage/innobase/include/os0file.h
+++ b/storage/innobase/include/os0file.h
@@ -91,7 +91,7 @@ struct Block {
 
   byte *m_ptr;
 
-  byte pad[INNOBASE_CACHE_LINE_SIZE - sizeof(ulint)];
+  byte pad[ut::INNODB_CACHE_LINE_SIZE - sizeof(ulint)];
   lock_word_t m_in_use;
 };
 }  // namespace file
diff --git a/storage/innobase/include/que0que.h b/storage/innobase/include/que0que.h
index 7ee7356ad46..81a4f3c0a61 100644
--- a/storage/innobase/include/que0que.h
+++ b/storage/innobase/include/que0que.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1996, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1996, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -86,10 +86,12 @@ void que_graph_free(que_t *graph); /*!< in: query graph; we assume that the
                                    que_graph_free_recursive and free the heap
                                    afterwards! */
 /** Stops a query thread if graph or trx is in a state requiring it. The
- conditions are tested in the order (1) graph, (2) trx. The lock_sys_t::mutex
- has to be reserved.
+ conditions are tested in the order (1) graph, (2) trx.
+ Caller must hold the trx mutex.
+ @param[in,out]   thr   query thread
  @return true if stopped */
-ibool que_thr_stop(que_thr_t *thr); /*!< in: query thread */
+bool que_thr_stop(que_thr_t *thr);
+
 /** Moves a thread from another state to the QUE_THR_RUNNING state. Increments
  the n_active_thrs counters of the query graph and transaction. */
 void que_thr_move_to_run_state_for_mysql(
@@ -272,9 +274,11 @@ struct que_thr_t {
   /** The thread slot in the lock_sys->waiting_threads array protected by
   lock_sys->wait_mutex when writing to it, and also by trx->mutex when changing
   from null to non-null.
-  While reading, one either hold the lock_sys->wait_mutex, or hold the
-  lock_sys->mutex, trx->mutex and a proof that noone else has woken the trx yet,
-  so the slot is either null, or changing to non-null, but definitely not
+  While reading, one can either hold the lock_sys->wait_mutex, or hold the
+  trx->mutex and a proof that no one has woken the trx yet,
+  so the slot is either still null (if trx hadn't yet started the sleep), or
+  already non-null (if it already started sleep), but definitely not
+  changing from null to non-null (as it requires trx->mutex) nor
   changing from non-null to null (as it happens after wake up). */
   struct srv_slot_t *slot;
   /*------------------------------*/
diff --git a/storage/innobase/include/rem0rec.h b/storage/innobase/include/rem0rec.h
index 61ad27f7e92..bccba812ba3 100644
--- a/storage/innobase/include/rem0rec.h
+++ b/storage/innobase/include/rem0rec.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1994, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1994, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -44,6 +44,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "rem/rec.h"
 #include "rem0types.h"
 #include "trx0types.h"
+#include "ut0class_life_cycle.h"
 
 /** The following function is used to get the pointer of the next chained record
  on the same page.
@@ -439,6 +440,81 @@ ulint rec_get_data_size_old(const rec_t *rec) /*!< in: physical record */
     MY_ATTRIBUTE((warn_unused_result));
 #define rec_offs_init(offsets) \
   rec_offs_set_n_alloc(offsets, (sizeof offsets) / sizeof *offsets)
+
+/**
+A helper RAII wrapper for otherwise difficult to use sequence of:
+
+  ulint offsets_[REC_OFFS_NORMAL_SIZE];
+  rec_offs_init(offsets_);
+  mem_heap_t *heap = nullptr;
+
+  const ulint *offsets =
+      rec_get_offsets(rec, index, offsets_, ULINT_UNDEFINED, &heap);
+
+  DO_SOMETHING(offsets);
+
+  if (heap != nullptr) {
+    mem_heap_free(heap);
+  }
+
+With this helper you can simply do:
+
+  DO_SOMETHING(Rec_offsets().compute(rec,index));
+
+And if you need to reuse the memory allocated offsets several times you can:
+  Rec_offsets offsets;
+  for(rec: recs) DO_SOMTHING(offsets.compute(rec,index))
+*/
+class Rec_offsets : private ut::Non_copyable {
+ public:
+  /** Prepares offsets to initially point to the fixed-size buffer, and marks
+  the memory as allocated, but uninitialized. You first need to call compute()
+  to use it */
+  Rec_offsets() { rec_offs_init(m_preallocated_buffer); }
+
+  /** Computes offsets for given record. Returned array is owned by this
+  instance. You can use its value as long as this object does not go out of
+  scope (which can free the buffer), and you don't call compute() again (which
+  can overwrite the offsets).
+  @param[in]  rec   The record for which you want to compute the offsets
+  @param[in]  index The index which contains the record
+  @return A pointer to offsets array owned by this instance. Valid till next
+  call to compute() or end of this instance lifetime.
+  */
+  const ulint *compute(const rec_t *rec, const dict_index_t *index) {
+    m_offsets =
+        rec_get_offsets(rec, index, m_offsets, ULINT_UNDEFINED, &m_heap);
+    return m_offsets;
+  }
+  /** Deallocated dynamically allocated memory, if any. */
+  ~Rec_offsets() {
+    if (m_heap) {
+      mem_heap_free(m_heap);
+      m_heap = nullptr;
+    }
+  }
+
+ private:
+  /** Pointer to heap used by rec_get_offsets(). Initially nullptr. If row is
+  really big, rec_get_offsets() may need to allocate new buffer for offsets.
+  At, first, when heap is null, rec_get_offsets() will create new heap, and pass
+  it back via reference. On subsequent calls, we will pass this heap, so it
+  is reused if needed. Therefore all allocated buffers are in this heap, if it
+  is not nullptr */
+  mem_heap_t *m_heap{nullptr};
+
+  /** Buffer with size large enough to handle common cases without having to use
+  heap. This is the initial value of m_offsets.*/
+  ulint m_preallocated_buffer[REC_OFFS_NORMAL_SIZE];
+
+  /* Initially points to m_preallocated_buffer (which is uninitialized memory).
+  After each call to compute() contains the pointer to the most recently
+  computed offsets.
+  We pass it back to rec_get_offsets() on subsequent calls to compute() to reuse
+  the same memory if possible. */
+  ulint *m_offsets{m_preallocated_buffer};
+};
+
 /** The following function returns the data size of a physical
  record, that is the sum of field lengths. SQL null fields
  are counted as length 0 fields. The value returned by the function
diff --git a/storage/innobase/include/row0vers.h b/storage/innobase/include/row0vers.h
index e8bf13693b6..b6cc006b126 100644
--- a/storage/innobase/include/row0vers.h
+++ b/storage/innobase/include/row0vers.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1997, 2018, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1997, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -48,14 +48,17 @@ class ReadView;
 
 /** Finds out if an active transaction has inserted or modified a secondary
  index record.
+ @param[in]   rec       record in a secondary index
+ @param[in]   index     the secondary index
+ @param[in]   offsets   rec_get_offsets(rec, index)
  @return 0 if committed, else the active transaction id;
  NOTE that this function can return false positives but never false
- negatives. The caller must confirm all positive results by calling
- trx_is_active() while holding lock_sys->mutex. */
-trx_t *row_vers_impl_x_locked(
-    const rec_t *rec,          /*!< in: record in a secondary index */
-    const dict_index_t *index, /*!< in: the secondary index */
-    const ulint *offsets);     /*!< in: rec_get_offsets(rec, index) */
+ negatives. The caller must confirm all positive results by checking if the trx
+ is still active.
+*/
+trx_t *row_vers_impl_x_locked(const rec_t *rec, const dict_index_t *index,
+                              const ulint *offsets);
+
 /** Finds out if we must preserve a delete marked earlier version of a clustered
  index record, because it is >= the purge view.
  @param[in]	trx_id		transaction id in the version
diff --git a/storage/innobase/include/srv0srv.h b/storage/innobase/include/srv0srv.h
index 5966f131b6d..3b2ceacebdb 100644
--- a/storage/innobase/include/srv0srv.h
+++ b/storage/innobase/include/srv0srv.h
@@ -950,16 +950,16 @@ void srv_active_wake_master_thread_low(void);
 void srv_wake_master_thread(void);
 #ifndef UNIV_HOTBACKUP
 /** Outputs to a file the output of the InnoDB Monitor.
- @return false if not all information printed
- due to failure to obtain necessary mutex */
-ibool srv_printf_innodb_monitor(
-    FILE *file,       /*!< in: output stream */
-    ibool nowait,     /*!< in: whether to wait for the
-                      lock_sys_t::mutex */
-    ulint *trx_start, /*!< out: file position of the start of
-                      the list of active transactions */
-    ulint *trx_end);  /*!< out: file position of the end of
-                      the list of active transactions */
+@param[in]    file      output stream
+@param[in]    nowait    whether to wait for the exclusive global lock_sys latch
+@param[out]   trx_start file position of the start of the list of active
+                        transactions
+@param[out]   trx_end   file position of the end of the list of active
+                        transactions
+@return false if not all information printed due to failure to obtain necessary
+        mutex */
+bool srv_printf_innodb_monitor(FILE *file, bool nowait, ulint *trx_start,
+                               ulint *trx_end);
 
 /** Function to pass InnoDB status variables to MySQL */
 void srv_export_innodb_status(void);
diff --git a/storage/innobase/include/sync0sharded_rw.h b/storage/innobase/include/sync0sharded_rw.h
index c53850d1a3b..0431fc7ed74 100644
--- a/storage/innobase/include/sync0sharded_rw.h
+++ b/storage/innobase/include/sync0sharded_rw.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 2017, 2018, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 2017, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify
 it under the terms of the GNU General Public License, version 2.0,
@@ -44,6 +44,7 @@ the file COPYING.Google.
 #define sync0sharded_rw_h
 
 #include "sync0rw.h"
+#include "ut0cpu_cache.h"
 #include "ut0rnd.h"
 #include "ut0ut.h"
 
@@ -84,18 +85,35 @@ class Sharded_rw_lock {
 
   size_t s_lock() {
     const size_t shard_no = ut_rnd_interval(0, m_n_shards - 1);
-    rw_lock_s_lock(&m_shards[shard_no].lock);
+    rw_lock_s_lock(&m_shards[shard_no]);
     return shard_no;
   }
 
   ibool s_lock_nowait(size_t &shard_no, const char *file, ulint line) {
     shard_no = ut_rnd_interval(0, m_n_shards - 1);
-    return rw_lock_s_lock_nowait(&m_shards[shard_no].lock, file, line);
+    return rw_lock_s_lock_nowait(&m_shards[shard_no], file, line);
   }
 
   void s_unlock(size_t shard_no) {
     ut_a(shard_no < m_n_shards);
-    rw_lock_s_unlock(&m_shards[shard_no].lock);
+    rw_lock_s_unlock(&m_shards[shard_no]);
+  }
+
+  /**
+  Tries to obtain exclusive latch - similar to x_lock(), but non-blocking, and
+  thus can fail.
+  @return true iff succeeded to acquire the exclusive latch
+  */
+  bool try_x_lock() {
+    for (size_t shard_no = 0; shard_no < m_n_shards; ++shard_no) {
+      if (!rw_lock_x_lock_nowait(&m_shards[shard_no])) {
+        while (0 < shard_no--) {
+          rw_lock_x_unlock(&m_shards[shard_no]);
+        }
+        return (false);
+      }
+    }
+    return (true);
   }
 
   void x_lock() {
@@ -108,23 +126,18 @@ class Sharded_rw_lock {
 
 #ifdef UNIV_DEBUG
   bool s_own(size_t shard_no) const {
-    return rw_lock_own(&m_shards[shard_no].lock, RW_LOCK_S);
+    return rw_lock_own(&m_shards[shard_no], RW_LOCK_S);
   }
 
-  bool x_own() const { return rw_lock_own(&m_shards[0].lock, RW_LOCK_X); }
+  bool x_own() const { return rw_lock_own(&m_shards[0], RW_LOCK_X); }
 #endif /* !UNIV_DEBUG */
 
  private:
-  struct Shard {
-    rw_lock_t lock;
-
-    char pad[INNOBASE_CACHE_LINE_SIZE];
-  };
+  using Shard = ut::Cacheline_padded<rw_lock_t>;
 
   template <typename F>
   void for_each(F f) {
-    std::for_each(m_shards, m_shards + m_n_shards,
-                  [&f](Shard &shard) { f(shard.lock); });
+    std::for_each(m_shards, m_shards + m_n_shards, f);
   }
 
   Shard *m_shards = nullptr;
diff --git a/storage/innobase/include/sync0sync.h b/storage/innobase/include/sync0sync.h
index 1d8b2d3b132..cab905f8114 100644
--- a/storage/innobase/include/sync0sync.h
+++ b/storage/innobase/include/sync0sync.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1995, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1995, 2020, Oracle and/or its affiliates. All Rights Reserved.
 Copyright (c) 2008, Google Inc.
 Copyright (c) 2012, Facebook Inc.
 
@@ -155,7 +155,8 @@ extern mysql_pfs_key_t trx_mutex_key;
 extern mysql_pfs_key_t trx_pool_mutex_key;
 extern mysql_pfs_key_t trx_pool_manager_mutex_key;
 extern mysql_pfs_key_t temp_pool_manager_mutex_key;
-extern mysql_pfs_key_t lock_mutex_key;
+extern mysql_pfs_key_t lock_sys_page_mutex_key;
+extern mysql_pfs_key_t lock_sys_table_mutex_key;
 extern mysql_pfs_key_t lock_wait_mutex_key;
 extern mysql_pfs_key_t trx_sys_mutex_key;
 extern mysql_pfs_key_t srv_sys_mutex_key;
@@ -189,6 +190,7 @@ extern mysql_pfs_key_t buf_block_debug_latch_key;
 extern mysql_pfs_key_t dict_operation_lock_key;
 extern mysql_pfs_key_t undo_spaces_lock_key;
 extern mysql_pfs_key_t rsegs_lock_key;
+extern mysql_pfs_key_t lock_sys_global_rw_lock_key;
 extern mysql_pfs_key_t fil_space_latch_key;
 extern mysql_pfs_key_t fts_cache_rw_lock_key;
 extern mysql_pfs_key_t fts_cache_init_rw_lock_key;
diff --git a/storage/innobase/include/sync0types.h b/storage/innobase/include/sync0types.h
index cc387ea9019..31a42c5c98c 100644
--- a/storage/innobase/include/sync0types.h
+++ b/storage/innobase/include/sync0types.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1995, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1995, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -175,7 +175,15 @@ V
 lock_sys_wait_mutex			Mutex protecting lock timeout data
 |
 V
-lock_sys_mutex				Mutex protecting lock_sys_t
+lock_sys->global_sharded_latch		Sharded rw-latch protecting lock_sys_t
+|
+V
+lock_sys->table_mutexes			Mutexes protecting lock_sys_t table
+|					lock queues
+|
+V
+lock_sys->page_mutexes			Mutexes protecting lock_sys_t page
+|					lock queues
 |
 V
 trx_sys->mutex				Mutex protecting trx_sys_t
@@ -264,13 +272,13 @@ enum latch_level_t {
   SYNC_PAGE_CLEANER,
   SYNC_PURGE_QUEUE,
   SYNC_TRX_SYS_HEADER,
-  SYNC_REC_LOCK,
   SYNC_THREADS,
   SYNC_TRX,
   SYNC_POOL,
   SYNC_POOL_MANAGER,
   SYNC_TRX_SYS,
-  SYNC_LOCK_SYS,
+  SYNC_LOCK_SYS_SHARDED,
+  SYNC_LOCK_SYS_GLOBAL,
   SYNC_LOCK_WAIT_SYS,
 
   SYNC_INDEX_ONLINE_LOG,
@@ -369,6 +377,10 @@ enum latch_id_t {
   LATCH_ID_IBUF,
   LATCH_ID_IBUF_PESSIMISTIC_INSERT,
   LATCH_ID_LOCK_FREE_HASH,
+  LATCH_ID_LOCK_SYS_GLOBAL,
+  LATCH_ID_LOCK_SYS_PAGE,
+  LATCH_ID_LOCK_SYS_TABLE,
+  LATCH_ID_LOCK_SYS_WAIT,
   LATCH_ID_LOG_SN,
   LATCH_ID_LOG_CHECKPOINTER,
   LATCH_ID_LOG_CLOSER,
@@ -410,8 +422,6 @@ enum latch_id_t {
   LATCH_ID_TRX_POOL_MANAGER,
   LATCH_ID_TEMP_POOL_MANAGER,
   LATCH_ID_TRX,
-  LATCH_ID_LOCK_SYS,
-  LATCH_ID_LOCK_SYS_WAIT,
   LATCH_ID_TRX_SYS,
   LATCH_ID_SRV_SYS,
   LATCH_ID_SRV_SYS_TASKS,
diff --git a/storage/innobase/include/trx0i_s.h b/storage/innobase/include/trx0i_s.h
index b590d5e0a55..f1f5212227b 100644
--- a/storage/innobase/include/trx0i_s.h
+++ b/storage/innobase/include/trx0i_s.h
@@ -233,8 +233,10 @@ int trx_i_s_possibly_fetch_data_into_cache(
 
 /** Returns TRUE if the data in the cache is truncated due to the memory
  limit posed by TRX_I_S_MEM_LIMIT.
+ @param[in]   cache   The cache
  @return true if truncated */
-ibool trx_i_s_cache_is_truncated(trx_i_s_cache_t *cache); /*!< in: cache */
+bool trx_i_s_cache_is_truncated(trx_i_s_cache_t *cache);
+
 /** The maximum length of a resulting lock_id_size in
 trx_i_s_create_lock_id(), not including the terminating NUL.
 "%lu:%lu:%lu:%lu:%lu" -> 20*5+4 chars */
diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index d7cc4edfdc0..49aed07851c 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -162,10 +162,13 @@ UNIV_INLINE
 trx_id_t trx_read_trx_id(
     const byte *ptr); /*!< in: pointer to memory from where to read */
 
-/** Looks for the trx instance with the given id in the rw trx_list.
- @return	the trx handle or NULL if not found */
+/** Looks for the trx handle with the given id in rw trxs list.
+ The caller must be holding trx_sys->mutex.
+ @param[in]   trx_id   trx id to search for
+ @return the trx handle or NULL if not found */
 UNIV_INLINE
-trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id); /*!< in: trx id to search for */
+trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id);
+
 /** Returns the minimum trx id in rw trx list. This is the smallest id for which
  the trx can possibly be active. (But, you must look at the trx->state to
  find out if the minimum trx id transaction itself is active, or already
diff --git a/storage/innobase/include/trx0sys.ic b/storage/innobase/include/trx0sys.ic
index 474759866d3..3dab5984b1c 100644
--- a/storage/innobase/include/trx0sys.ic
+++ b/storage/innobase/include/trx0sys.ic
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1996, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1996, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -179,14 +179,8 @@ trx_id_t trx_read_trx_id(
   return (mach_read_from_6(ptr));
 }
 
-/** Looks for the trx handle with the given id in rw_trx_list.
- The caller must be holding trx_sys->mutex.
- @return the trx handle or NULL if not found;
- the pointer must not be dereferenced unless lock_sys->mutex was
- acquired before calling this function and is still being held */
 UNIV_INLINE
-trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id) /*!< in: trx id to search for */
-{
+trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id) {
   ut_ad(trx_id > 0);
   ut_ad(trx_sys_mutex_own());
 
diff --git a/storage/innobase/include/trx0trx.h b/storage/innobase/include/trx0trx.h
index fa9f9506f46..94d5f8ffef7 100644
--- a/storage/innobase/include/trx0trx.h
+++ b/storage/innobase/include/trx0trx.h
@@ -33,6 +33,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #ifndef trx0trx_h
 #define trx0trx_h
 
+#include <atomic>
 #include <set>
 
 #include "ha_prototypes.h"
@@ -208,13 +209,14 @@ int trx_recover_for_mysql(
     XA_recover_txn *txn_list, /*!< in/out: prepared transactions */
     ulint len,                /*!< in: number of slots in xid_list */
     MEM_ROOT *mem_root);      /*!< in: memory for table names */
+
 /** This function is used to find one X/Open XA distributed transaction
  which is in the prepared state
+ @param[in]   xid   X/Open XA transaction identifier
  @return trx or NULL; on match, the trx->xid will be invalidated;
- note that the trx may have been committed, unless the caller is
- holding lock_sys->mutex */
-trx_t *trx_get_trx_by_xid(
-    const XID *xid); /*!< in: X/Open XA transaction identifier */
+ note that the trx may have been committed */
+trx_t *trx_get_trx_by_xid(const XID *xid);
+
 /** If required, flushes the log to disk if we called trx_commit_for_mysql()
  with trx->flush_log_later == TRUE. */
 void trx_commit_complete_for_mysql(trx_t *trx); /*!< in/out: transaction */
@@ -260,20 +262,20 @@ void trx_print_low(FILE *f,
 /*!< in: mem_heap_get_size(trx->lock.lock_heap) */
 
 /** Prints info about a transaction.
- The caller must hold lock_sys->mutex and trx_sys->mutex.
- When possible, use trx_print() instead. */
-void trx_print_latched(
-    FILE *f,              /*!< in: output stream */
-    const trx_t *trx,     /*!< in: transaction */
-    ulint max_query_len); /*!< in: max query length to print,
-                          or 0 to use the default max length */
+The caller must hold lock_sys exclusive global latch and trx_sys->mutex.
+@param[in]  f               output stream
+@param[in]  trx             transaction
+@param[in]  max_query_len   max query length to print, or 0 to use the default
+                            max length */
+void trx_print_latched(FILE *f, const trx_t *trx, ulint max_query_len);
 
 /** Prints info about a transaction.
- Acquires and releases lock_sys->mutex and trx_sys->mutex. */
-void trx_print(FILE *f,              /*!< in: output stream */
-               const trx_t *trx,     /*!< in: transaction */
-               ulint max_query_len); /*!< in: max query length to print,
-                                     or 0 to use the default max length */
+Acquires and releases lock_sys exclusive global latch and trx_sys->mutex.
+@param[in]  f               output stream
+@param[in]  trx             transaction
+@param[in]  max_query_len   max query length to print, or 0 to use the default
+                            max length */
+void trx_print(FILE *f, const trx_t *trx, ulint max_query_len);
 
 /** Determine if a transaction is a dictionary operation.
  @return dictionary operation mode */
@@ -540,15 +542,16 @@ typedef std::vector<ib_lock_t *, ut_allocator<ib_lock_t *>> lock_pool_t;
  changed asynchronously.
 
  All these operations take place within the context of locking. Therefore state
- changes within the locking code must acquire both the lock mutex and the
- trx->mutex when changing trx->lock.que_state to TRX_QUE_LOCK_WAIT or
+ changes within the locking code must latch the shard with the wait_lock and
+ the trx->mutex when changing trx->lock.que_state to TRX_QUE_LOCK_WAIT or
  trx->lock.wait_lock to non-NULL but when the lock wait ends it is sufficient
  to only acquire the trx->mutex.
  To query the state either of the mutexes is sufficient within the locking
  code and no mutex is required when the query thread is no longer waiting. */
 
-/** The locks and state of an active transaction. Protected by
-lock_sys->mutex, trx->mutex or both. */
+/** The locks and state of an active transaction.
+Protected by exclusive lock_sys latch or trx->mutex combined with shared
+lock_sys latch (unless stated otherwise for particular field). */
 struct trx_lock_t {
   ulint n_active_thrs; /*!< number of active query threads */
 
@@ -556,38 +559,56 @@ struct trx_lock_t {
                        == TRX_STATE_ACTIVE: TRX_QUE_RUNNING,
                        TRX_QUE_LOCK_WAIT, ... */
 
+  /** Incremented each time a lock is added or removed from the
+  trx->lock.trx_locks, so that the thread which iterates over the list can spot
+  a change if it occurred while it was reacquiring latches.
+  Protected by trx->mutex. */
+  uint64_t trx_locks_version;
+
   /** If this transaction is waiting for a lock, then blocking_trx points to a
   transaction which holds a conflicting lock.
-  The opposite is not true sometimes, that is:
-  1. It is possible that the transaction has trx->lock.wait_lock == null, yet it
+  It is possible that the transaction has trx->lock.wait_lock == nullptr, yet it
   has non-null value of trx->lock.blocking_trx. For example this can happen when
   we are in the process of moving locks from one heap_no to another. This
-  however is always done while the lock_sys mutex is latched and conceptually it
-  is true that the blocking_trx is the one for which the transaction waits, even
-  though temporarily there is no pointer to a particular WAITING lock object.
-  2. If the trx is not waiting for any other transaction, this field might
-  contain some left-over value from previous wait, although we try to keep it
-  clean to make debugging easier it is not a requirement for correctness of the
-  deadlock detection, as it is performed only among transactions which are
-  waiting.
-
-  This field is changed from null to non-null, when holding trx_mutex_own(this)
-  and lock_sys mutex.
+  however is always done while the lock_sys shards which contain the queues
+  involved are latched and conceptually it is true that the blocking_trx is
+  the one for which the transaction waits, even though temporarily there is no
+  pointer to a particular WAITING lock object.
+
+  This field is changed from null to non-null, when holding this->mutex and
+  mutex for lock_sys shard containing the new value of trx->lock.wait_lock.
   The field is changed from non-null to different non-null value, while holding
-  lock_sys mutex.
-  The field is changed from non-null to null, while holding trx_mutex_own(this),
-  and lock_sys mutex.
+  mutex for lock_sys shard containing the trx->lock.wait_lock.
+  The field is changed from non-null to null, while holding this->mutex,
+  mutex for lock_sys shard containing the old value of trx->lock.wait_lock,
+  before it was changed to null.
+
   Readers might read it without any latch, but then they should validate the
   value, i.e. test if it is not-null, and points to a valid trx.
-  To make any definite judgments it one needs to latch the lock_sys mutex. */
+  To make any definite judgments one needs to latch the lock_sys shard
+  containing the trx->lock.wait_lock. */
   std::atomic<trx_t *> blocking_trx;
 
-  /** If trx execution state is TRX_QUE_LOCK_WAIT, this points to the lock
-  request, otherwise this is NULL; set to non-NULL when holding both trx->mutex
-  and lock_sys->mutex; set to NULL when holding lock_sys->mutex; readers should
-  hold lock_sys->mutex, except when they are holding trx->mutex and
-  wait_lock==NULL */
-  lock_t *wait_lock;
+  /** The lock request of this transaction is waiting for.
+  It might be NULL if the transaction is not currently waiting, or if the lock
+  was temporarily removed during B-tree reorganization and will be recreated in
+  a different queue. Such move is protected by latching the shards containing
+  both queues, so the intermediate state should not be observed by readers who
+  latch the old shard.
+
+  Changes from NULL to non-NULL while holding trx->mutex and latching the shard
+  containing the new wait_lock value.
+  Changes from non-NULL to NULL while latching the shard containing the old
+  wait_lock value.
+  Never changes from non-NULL to other non-NULL directly.
+
+  Readers should hold exclusive global latch on lock_sys, as in general they
+  can't know what shard the lock belongs to before reading it.
+  However, in debug assertions, where we strongly believe to know the value of
+  this field in advance, we can:
+  - read without any latch if we believe the value should be NULL
+  - read latching only the shard containing the wait_lock we expect */
+  std::atomic<lock_t *> wait_lock;
 
   /** Stores the type of the most recent lock for which this trx had to wait.
   Set to lock_get_type_low(wait_lock) together with wait_lock in
@@ -596,7 +617,7 @@ struct trx_lock_t {
   lock_reset_lock_and_trx_wait() as in lock_wait_suspend_thread() we are
   interested in reporting the last known value of this field via
   thd_wait_begin(). When a thread has to wait for a lock, it first releases
-  lock-sys mutex, and then calls lock_wait_suspend_thread() where among other
+  lock-sys latch, and then calls lock_wait_suspend_thread() where among other
   things it tries to report statistic via thd_wait_begin() about the kind of
   lock (THD_WAIT_ROW_LOCK vs THD_WAIT_TABLE_LOCK) that caused the wait. But
   there is a possibility that before it gets to call thd_wait_begin() some other
@@ -611,8 +632,8 @@ struct trx_lock_t {
   lock_reset_lock_and_trx_wait() which changes trx->lock.wait_lock to NULL, but
   then calls lock_rec_add_to_queue() -> RecLock::create() -> RecLock::lock_add()
   -> lock_set_lock_and_trx_wait() to set it again to the new lock. This all
-  happens while holding lock-sys mutex, but we read wait_lock_type without this
-  mutex, so we should not clear the wait_lock_type simply because somebody
+  happens while holding lock-sys latch, but we read wait_lock_type without this
+  latch, so we should not clear the wait_lock_type simply because somebody
   changed wait_lock to NULL.
   Protected by trx->mutex. */
   uint32_t wait_lock_type;
@@ -624,16 +645,19 @@ struct trx_lock_t {
   transaction as a victim in deadlock
   resolution, it sets this to true.
   Protected by trx->mutex. */
-  time_t wait_started; /*!< lock wait started at this time,
-                       protected only by lock_sys->mutex */
 
-  que_thr_t *wait_thr; /*!< query thread belonging to this
-                       trx that is in QUE_THR_LOCK_WAIT
-                       state. For threads suspended in a
-                       lock wait, this is protected by
-                       lock_sys->mutex. Otherwise, this may
-                       only be modified by the thread that is
-                       serving the running transaction. */
+  /** Lock wait started at this time.
+  Writes under shared lock_sys latch combined with trx->mutex.
+  Reads require either trx->mutex or exclusive lock_sys latch. */
+  time_t wait_started;
+
+  /** query thread belonging to this trx that is in QUE_THR_LOCK_WAIT state.
+  For threads suspended in a lock wait, this is protected by lock_sys latch for
+  the wait_lock's shard.
+  Otherwise, this may only be modified by the thread that is serving the running
+  transaction.
+  */
+  que_thr_t *wait_thr;
 
   /** Pre-allocated record locks. Protected by trx->mutex. */
   lock_pool_t rec_pool;
@@ -651,7 +675,7 @@ struct trx_lock_t {
   mem_heap_t *lock_heap;
 
   /** Locks requested by the transaction.
-  Modifications are protected by trx->mutex and lock_sys mutex.
+  Modifications are protected by trx->mutex and shard of lock_sys mutex.
   Reads can be performed while holding trx->mutex or exclusive lock_sys latch.
   One can also check if this list is empty or not from the thread running this
   transaction without holding any latches, keeping in mind that other threads
@@ -673,7 +697,9 @@ struct trx_lock_t {
   Protected by trx->mutex. */
   ib_vector_t *autoinc_locks;
 
-  /** number of rec locks in this trx */
+  /** Number of rec locks in this trx.
+  It is modified with shared lock_sys latch.
+  It is read with exclusive lock_sys latch. */
   std::atomic<ulint> n_rec_locks;
 
   /** Used to indicate that every lock of this transaction placed on a record
@@ -746,7 +772,7 @@ and lock_trx_release_locks() [invoked by trx_commit()].
 
 * Print of transactions may access transactions not associated with
 the current thread. The caller must be holding trx_sys->mutex and
-lock_sys->mutex.
+exclusive global lock_sys latch.
 
 * When a transaction handle is in the trx_sys->mysql_trx_list or
 trx_sys->trx_list, some of its fields must not be modified without
@@ -754,7 +780,7 @@ holding trx_sys->mutex exclusively.
 
 * The locking code (in particular, deadlock checking and implicit to
 explicit conversion) will access transactions associated to other
-connections. The locks of transactions are protected by lock_sys->mutex
+connections. The locks of transactions are protected by lock_sys latches
 and sometimes by trx->mutex.
 
 * Killing of asynchronous transactions. */
@@ -824,7 +850,7 @@ struct trx_t {
   };
 
   /** Mutex protecting the fields `state` and `lock` (except some fields of
-  `lock`,  which are protected by lock_sys->mutex) */
+  `lock`,  which are protected by lock_sys latches) */
   mutable TrxMutex mutex;
 
   /* Note: in_depth was split from in_innodb for fixing a RO
@@ -939,10 +965,10 @@ struct trx_t {
            to check for the view limit for
            transactions that are committing */
 
-  trx_lock_t lock;   /*!< Information about the transaction
-                     locks and state. Protected by
-                     trx->mutex or lock_sys->mutex
-                     or both */
+  /** Information about the transaction locks and state.
+  Protected by trx->mutex or lock_sys latches or both */
+  trx_lock_t lock;
+
   bool is_recovered; /*!< 0=normal transaction,
                      1=recovered, must be rolled back,
                      protected by trx_sys->mutex when
@@ -1259,16 +1285,53 @@ struct commit_node_t {
 /** Test if trx->mutex is owned by the current thread. */
 #define trx_mutex_own(t) mutex_own(&t->mutex)
 
-/** Acquire the trx->mutex. */
-#define trx_mutex_enter(t)  \
-  do {                      \
-    mutex_enter(&t->mutex); \
+#ifdef UNIV_DEBUG
+/**
+Verifies the invariants and records debug state related to latching rules.
+Called during trx_mutex_enter before the actual mutex acquisition.
+@param[in]  trx             The transaction for which trx_mutex_enter(trx) is
+                            called
+@param[in]  allow_another   If false, then no further calls to trx_mutex_enter
+                            are allowed, until trx_mutex_exit().
+                            If true, then this must be the first trx acquisition
+                            and we will allow one more.
+*/
+void trx_before_mutex_enter(const trx_t *trx, bool allow_another);
+
+/**
+Verifies the invariants and records debug state related to latching rules.
+Called during trx_mutex_exit before the actual mutex release.
+@param[in]  trx   The transaction for which trx_mutex_exit(trx) is called
+*/
+void trx_before_mutex_exit(const trx_t *trx);
+#endif
+
+/**
+Please do not use this low-level macro.
+Use trx_mutex_enter(t) instead.
+In rare cases where you need to take two trx->mutex-es, take the first one
+using trx_mutex_enter_first_of_two(t1), and the second one with
+trx_mutex(2)
+*/
+#define trx_mutex_enter_low(t, first_of_two)       \
+  do {                                             \
+    ut_ad(!trx_mutex_own(t));                      \
+    ut_d(trx_before_mutex_enter(t, first_of_two)); \
+    mutex_enter(&t->mutex);                        \
   } while (0)
 
+/** Acquire the trx->mutex (and promise not to request any more). */
+#define trx_mutex_enter(t) trx_mutex_enter_low(t, false)
+
+/** Acquire the trx->mutex (and indicate we might request one more). */
+#define trx_mutex_enter_first_of_two(t) trx_mutex_enter_low(t, true)
+
 /** Release the trx->mutex. */
-#define trx_mutex_exit(t)  \
-  do {                     \
-    mutex_exit(&t->mutex); \
+#define trx_mutex_exit(t)           \
+  do {                              \
+    ut_ad(trx_mutex_own(t));        \
+    ut_d(trx_before_mutex_exit(t)); \
+    mutex_exit(&t->mutex);          \
   } while (0)
 
 /** Track if a transaction is executing inside InnoDB code. It acts
diff --git a/storage/innobase/include/ut0class_life_cycle.h b/storage/innobase/include/ut0class_life_cycle.h
new file mode 100644
index 00000000000..c1d760977f0
--- /dev/null
+++ b/storage/innobase/include/ut0class_life_cycle.h
@@ -0,0 +1,52 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+
+/** @file include/ut0class_life_cycle.h
+Utilities related to class lifecycle. */
+
+#ifndef ut0class_life_cycle_h
+#define ut0class_life_cycle_h
+
+namespace ut {
+
+/**
+A utility class which, if inherited from, prevents the descendant class
+from being copied, moved, or assigned.
+This is useful for guard classes.
+*/
+class Non_copyable {
+ public:
+  Non_copyable(const Non_copyable &) = delete;
+  Non_copyable &operator=(const Non_copyable &) = delete;
+
+ protected:
+  Non_copyable() = default;
+  ~Non_copyable() = default;  /// Protected non-virtual destructor
+};
+
+} /* namespace ut */
+
+#endif /* ut0class_life_cycle_h */
\ No newline at end of file
diff --git a/storage/innobase/include/ut0counter.h b/storage/innobase/include/ut0counter.h
index 39efb13cf3a..e5ff3568aee 100644
--- a/storage/innobase/include/ut0counter.h
+++ b/storage/innobase/include/ut0counter.h
@@ -39,19 +39,13 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "univ.i"
 
 #include "os0thread.h"
+#include "ut0cpu_cache.h"
 #include "ut0dbg.h"
 
 #include <array>
 #include <atomic>
 #include <functional>
 
-/** CPU cache line size */
-#ifdef __powerpc__
-#define INNOBASE_CACHE_LINE_SIZE 128
-#else
-#define INNOBASE_CACHE_LINE_SIZE 64
-#endif /* __powerpc__ */
-
 /** Default number of slots to use in ib_counter_t */
 #define IB_N_SLOTS 64
 
@@ -62,7 +56,7 @@ struct generic_indexer_t {
 
   /** @return offset within m_counter */
   static size_t offset(size_t index) UNIV_NOTHROW {
-    return (((index % N) + 1) * (INNOBASE_CACHE_LINE_SIZE / sizeof(Type)));
+    return (((index % N) + 1) * (ut::INNODB_CACHE_LINE_SIZE / sizeof(Type)));
   }
 };
 
@@ -105,7 +99,7 @@ struct single_indexer_t {
   /** @return offset within m_counter */
   static size_t offset(size_t index) UNIV_NOTHROW {
     ut_ad(N == 1);
-    return ((INNOBASE_CACHE_LINE_SIZE / sizeof(Type)));
+    return ((ut::INNODB_CACHE_LINE_SIZE / sizeof(Type)));
   }
 
   /** @return 1 */
@@ -120,7 +114,7 @@ struct single_indexer_t {
 /** Class for using fuzzy counters. The counter is not protected by any
 mutex and the results are not guaranteed to be 100% accurate but close
 enough. Creates an array of counters and separates each element by the
-INNOBASE_CACHE_LINE_SIZE bytes */
+ut::INNODB_CACHE_LINE_SIZE bytes */
 template <typename Type, int N = IB_N_SLOTS,
           template <typename, int> class Indexer = default_indexer_t>
 class ib_counter_t {
@@ -133,7 +127,7 @@ class ib_counter_t {
 
   bool validate() UNIV_NOTHROW {
 #ifdef UNIV_DEBUG
-    size_t n = (INNOBASE_CACHE_LINE_SIZE / sizeof(Type));
+    size_t n = (ut::INNODB_CACHE_LINE_SIZE / sizeof(Type));
 
     /* Check that we aren't writing outside our defined bounds. */
     for (size_t i = 0; i < UT_ARR_SIZE(m_counter); i += n) {
@@ -219,7 +213,7 @@ class ib_counter_t {
   Indexer<Type, N> m_policy;
 
   /** Slot 0 is unused. */
-  Type m_counter[(N + 1) * (INNOBASE_CACHE_LINE_SIZE / sizeof(Type))];
+  Type m_counter[(N + 1) * (ut::INNODB_CACHE_LINE_SIZE / sizeof(Type))];
 };
 
 /** Sharded atomic counter. */
@@ -229,10 +223,10 @@ using Type = uint64_t;
 
 using N = std::atomic<Type>;
 
-static_assert(INNOBASE_CACHE_LINE_SIZE >= sizeof(N),
-              "Atomic counter size > INNOBASE_CACHE_LINE_SIZE");
+static_assert(ut::INNODB_CACHE_LINE_SIZE >= sizeof(N),
+              "Atomic counter size > ut::INNODB_CACHE_LINE_SIZE");
 
-using Pad = byte[INNOBASE_CACHE_LINE_SIZE - sizeof(N)];
+using Pad = byte[ut::INNODB_CACHE_LINE_SIZE - sizeof(N)];
 
 /** Counter shard. */
 struct Shard {
diff --git a/storage/innobase/include/ut0cpu_cache.h b/storage/innobase/include/ut0cpu_cache.h
new file mode 100644
index 00000000000..40053e02dc3
--- /dev/null
+++ b/storage/innobase/include/ut0cpu_cache.h
@@ -0,0 +1,56 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+
+/** @file include/ut0cpu_cache.h
+Utilities related to CPU cache. */
+
+#ifndef ut0cpu_cache_h
+#define ut0cpu_cache_h
+
+#include "ut0ut.h"
+namespace ut {
+
+/** CPU cache line size */
+#ifdef __powerpc__
+constexpr size_t INNODB_CACHE_LINE_SIZE = 128;
+#else
+constexpr size_t INNODB_CACHE_LINE_SIZE = 64;
+#endif /* __powerpc__ */
+
+/**
+A utility wrapper class, which adds padding at the end of the wrapped structure,
+so that the next object after it is guaranteed to be in the next cache line.
+This is to avoid false-sharing.
+Use this, as opposed to alignas(), to avoid problems with allocators which do
+not handle over-aligned types.
+ */
+template <typename T>
+struct Cacheline_padded : public T {
+  char pad[INNODB_CACHE_LINE_SIZE];
+};
+} /* namespace ut */
+
+#endif /* ut0cpu_cache_h */
diff --git a/storage/innobase/include/ut0link_buf.h b/storage/innobase/include/ut0link_buf.h
index b8278bc8997..f6e93927405 100644
--- a/storage/innobase/include/ut0link_buf.h
+++ b/storage/innobase/include/ut0link_buf.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 2017, 2018, Oracle and/or its affiliates. All rights reserved.
+Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
 
 This program is free software; you can redistribute it and/or modify
 it under the terms of the GNU General Public License, version 2.0,
@@ -185,7 +185,7 @@ class Link_buf {
   std::atomic<Distance> *m_links;
 
   /** Tail pointer in the buffer (expressed in original unit). */
-  alignas(INNOBASE_CACHE_LINE_SIZE) std::atomic<Position> m_tail;
+  alignas(ut::INNODB_CACHE_LINE_SIZE) std::atomic<Position> m_tail;
 };
 
 template <typename Position>
diff --git a/storage/innobase/include/ut0mpmcbq.h b/storage/innobase/include/ut0mpmcbq.h
index ed7c04ef35c..ce3e82e41d2 100644
--- a/storage/innobase/include/ut0mpmcbq.h
+++ b/storage/innobase/include/ut0mpmcbq.h
@@ -1,5 +1,5 @@
 /*****************************************************************************
-Copyright (c) 2017, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 2017, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify
 it under the terms of the GNU General Public License, version 2.0,
@@ -26,6 +26,8 @@ Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
 #ifndef ut0mpmcbq_h
 #define ut0mpmcbq_h
 
+#include "ut0cpu_cache.h"
+
 #include <atomic>
 
 /** Multiple producer consumer, bounded queue
@@ -182,7 +184,7 @@ class mpmc_bq {
   }
 
  private:
-  using Pad = byte[INNOBASE_CACHE_LINE_SIZE];
+  using Pad = byte[ut::INNODB_CACHE_LINE_SIZE];
 
   struct Cell {
     std::atomic<size_t> m_pos;
diff --git a/storage/innobase/include/ut0new.h b/storage/innobase/include/ut0new.h
index c8e97c7d262..838520b136c 100644
--- a/storage/innobase/include/ut0new.h
+++ b/storage/innobase/include/ut0new.h
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 2014, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 2014, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -143,8 +143,8 @@ InnoDB:
 #include "os0proc.h"
 #include "os0thread.h"
 #include "univ.i"
-#include "ut0byte.h"    /* ut_align */
-#include "ut0counter.h" /* INNOBASE_CACHE_LINE_SIZE */
+#include "ut0byte.h" /* ut_align */
+#include "ut0cpu_cache.h"
 #include "ut0ut.h"
 
 #define OUT_OF_MEMORY_MSG                                             \
@@ -1230,7 +1230,7 @@ class aligned_memory {
 /** Manages an object that is aligned to specified number of bytes.
 @tparam	T_Type		type of the object that is going to be managed
 @tparam T_Align_to	number of bytes to align to */
-template <typename T_Type, size_t T_Align_to = INNOBASE_CACHE_LINE_SIZE>
+template <typename T_Type, size_t T_Align_to = ut::INNODB_CACHE_LINE_SIZE>
 class aligned_pointer : public aligned_memory<T_Type, T_Align_to> {
  public:
   ~aligned_pointer() {
@@ -1257,7 +1257,7 @@ class aligned_pointer : public aligned_memory<T_Type, T_Align_to> {
 number of bytes.
 @tparam	T_Type		type of the object that is going to be managed
 @tparam T_Align_to	number of bytes to align to */
-template <typename T_Type, size_t T_Align_to = INNOBASE_CACHE_LINE_SIZE>
+template <typename T_Type, size_t T_Align_to = ut::INNODB_CACHE_LINE_SIZE>
 class aligned_array_pointer : public aligned_memory<T_Type, T_Align_to> {
  public:
   /** Allocates aligned memory for new objects. Objects must be trivially
diff --git a/storage/innobase/lock/lock0guards.cc b/storage/innobase/lock/lock0guards.cc
new file mode 100644
index 00000000000..9b0db062c00
--- /dev/null
+++ b/storage/innobase/lock/lock0guards.cc
@@ -0,0 +1,114 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+
+#define LOCK_MODULE_IMPLEMENTATION
+
+#include "lock0guards.h"
+#include "lock0priv.h"
+#include "sync0rw.h"
+
+namespace locksys {
+
+/* Global_exclusive_latch_guard */
+
+Global_exclusive_latch_guard::Global_exclusive_latch_guard() {
+  lock_sys->latches.global_latch.x_lock();
+}
+
+Global_exclusive_latch_guard::~Global_exclusive_latch_guard() {
+  lock_sys->latches.global_latch.x_unlock();
+}
+
+/* Global_exclusive_try_latch */
+
+Global_exclusive_try_latch::Global_exclusive_try_latch() {
+  m_owns_exclusive_global_latch = lock_sys->latches.global_latch.try_x_lock();
+}
+
+Global_exclusive_try_latch::~Global_exclusive_try_latch() {
+  if (m_owns_exclusive_global_latch) {
+    lock_sys->latches.global_latch.x_unlock();
+    m_owns_exclusive_global_latch = false;
+  }
+}
+
+/* Shard_naked_latch_guard */
+
+Shard_naked_latch_guard::Shard_naked_latch_guard(Lock_mutex &shard_mutex)
+    : m_shard_mutex{shard_mutex} {
+  ut_ad(owns_shared_global_latch());
+  mutex_enter(&m_shard_mutex);
+}
+
+Shard_naked_latch_guard::Shard_naked_latch_guard(const dict_table_t &table)
+    : Shard_naked_latch_guard{lock_sys->latches.table_shards.get_mutex(table)} {
+}
+
+Shard_naked_latch_guard::Shard_naked_latch_guard(const page_id_t &page_id)
+    : Shard_naked_latch_guard{
+          lock_sys->latches.page_shards.get_mutex(page_id)} {}
+
+Shard_naked_latch_guard::~Shard_naked_latch_guard() {
+  mutex_exit(&m_shard_mutex);
+}
+
+/* Global_shared_latch_guard */
+
+Global_shared_latch_guard::Global_shared_latch_guard() {
+  lock_sys->latches.global_latch.s_lock();
+}
+
+Global_shared_latch_guard::~Global_shared_latch_guard() {
+  lock_sys->latches.global_latch.s_unlock();
+}
+
+/* Shard_latches_guard */
+
+Shard_latches_guard::Shard_latches_guard(Lock_mutex &shard_mutex_a,
+                                         Lock_mutex &shard_mutex_b)
+    : m_global_shared_latch_guard{},
+      m_shard_mutex_1{*std::min(&shard_mutex_a, &shard_mutex_b, MUTEX_ORDER)},
+      m_shard_mutex_2{*std::max(&shard_mutex_a, &shard_mutex_b, MUTEX_ORDER)} {
+  if (&m_shard_mutex_1 != &m_shard_mutex_2) {
+    mutex_enter(&m_shard_mutex_1);
+  }
+  mutex_enter(&m_shard_mutex_2);
+}
+
+Shard_latches_guard::Shard_latches_guard(const buf_block_t &block_a,
+                                         const buf_block_t &block_b)
+    : Shard_latches_guard{
+          lock_sys->latches.page_shards.get_mutex(block_a.get_page_id()),
+          lock_sys->latches.page_shards.get_mutex(block_b.get_page_id())} {}
+
+Shard_latches_guard::~Shard_latches_guard() {
+  mutex_exit(&m_shard_mutex_2);
+  if (&m_shard_mutex_1 != &m_shard_mutex_2) {
+    mutex_exit(&m_shard_mutex_1);
+  }
+}
+
+}  // namespace locksys
diff --git a/storage/innobase/lock/lock0iter.cc b/storage/innobase/lock/lock0iter.cc
index 660b139f632..8e9d399340a 100644
--- a/storage/innobase/lock/lock0iter.cc
+++ b/storage/innobase/lock/lock0iter.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 2007, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 2007, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -55,7 +55,8 @@ void lock_queue_iterator_reset(
     ulint bit_no)                /*!< in: record number in the
                                  heap */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(lock != nullptr);
+  ut_ad(locksys::owns_lock_shard(lock));
 
   iter->current_lock = lock;
 
@@ -85,7 +86,8 @@ const lock_t *lock_queue_iterator_get_prev(
 {
   const lock_t *prev_lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(iter->current_lock != nullptr);
+  ut_ad(locksys::owns_lock_shard(iter->current_lock));
 
   switch (lock_get_type_low(iter->current_lock)) {
     case LOCK_REC:
diff --git a/storage/innobase/lock/lock0latches.cc b/storage/innobase/lock/lock0latches.cc
new file mode 100644
index 00000000000..f2a09ba2317
--- /dev/null
+++ b/storage/innobase/lock/lock0latches.cc
@@ -0,0 +1,107 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Oracle and/or its affiliates. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is also distributed with certain software (including but not
+limited to OpenSSL) that is licensed under separate terms, as designated in a
+particular file or component or in included license documentation. The authors
+of MySQL hereby grant you an additional permission to link the program and
+your derivative works with the separately licensed software that they have
+included with MySQL.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+You should have received a copy of the GNU General Public License along with
+this program; if not, write to the Free Software Foundation, Inc.,
+51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA
+
+*****************************************************************************/
+
+#define LOCK_MODULE_IMPLEMENTATION
+
+#include "lock0latches.h"
+#include "lock0lock.h"
+#include "lock0priv.h"
+
+namespace locksys {
+
+size_t Latches::Page_shards::get_shard(const page_id_t &page_id) {
+  /* We always use lock_rec_hash regardless of the exact type of the lock.
+  It may happen that the lock is a predicate lock, in which case,
+  it would make more sense to use hash_calc_hash with proper hash table
+  size. The current implementation works, because the size of all three
+  hashmaps is always the same. This allows an interface with less arguments.
+  */
+  ut_ad(lock_sys->rec_hash->n_cells == lock_sys->prdt_hash->n_cells);
+  ut_ad(lock_sys->rec_hash->n_cells == lock_sys->prdt_page_hash->n_cells);
+  return lock_rec_hash(page_id.space(), page_id.page_no()) % SHARDS_COUNT;
+}
+
+const Lock_mutex &Latches::Page_shards::get_mutex(
+    const page_id_t &page_id) const {
+  return mutexes[get_shard(page_id)];
+}
+
+Lock_mutex &Latches::Page_shards::get_mutex(const page_id_t &page_id) {
+  /* See "Effective C++ item 3: Use const whenever possible" for explanation of
+  this pattern, which avoids code duplication by reusing const version. */
+  return const_cast<Lock_mutex &>(
+      const_cast<const Latches::Page_shards *>(this)->get_mutex(page_id));
+}
+
+size_t Latches::Table_shards::get_shard(const dict_table_t &table) {
+  return table.id % SHARDS_COUNT;
+}
+
+const Lock_mutex &Latches::Table_shards::get_mutex(
+    const dict_table_t &table) const {
+  return mutexes[get_shard(table)];
+}
+
+Lock_mutex &Latches::Table_shards::get_mutex(const dict_table_t &table) {
+  /* See "Effective C++ item 3: Use const whenever possible" for explanation of
+  this pattern, which avoids code duplication by reusing const version. */
+  return const_cast<Lock_mutex &>(
+      const_cast<const Latches::Table_shards *>(this)->get_mutex(table));
+}
+
+thread_local size_t Latches::Unique_sharded_rw_lock::m_shard_id{NOT_IN_USE};
+
+Latches::Unique_sharded_rw_lock::Unique_sharded_rw_lock() {
+  rw_lock.create(lock_sys_global_rw_lock_key, SYNC_LOCK_SYS_GLOBAL, 64);
+}
+
+Latches::Unique_sharded_rw_lock::~Unique_sharded_rw_lock() { rw_lock.free(); }
+
+Latches::Page_shards::Page_shards() {
+  for (size_t i = 0; i < SHARDS_COUNT; ++i) {
+    mutex_create(LATCH_ID_LOCK_SYS_PAGE, mutexes + i);
+  }
+}
+
+Latches::Page_shards::~Page_shards() {
+  for (size_t i = 0; i < SHARDS_COUNT; ++i) {
+    mutex_destroy(mutexes + i);
+  }
+}
+
+Latches::Table_shards::Table_shards() {
+  for (size_t i = 0; i < SHARDS_COUNT; ++i) {
+    mutex_create(LATCH_ID_LOCK_SYS_TABLE, mutexes + i);
+  }
+}
+
+Latches::Table_shards::~Table_shards() {
+  for (size_t i = 0; i < SHARDS_COUNT; ++i) {
+    mutex_destroy(mutexes + i);
+  }
+}
+
+}  // namespace locksys
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index 64bed968ebe..779117852de 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -91,7 +91,9 @@ static const std::map<uint, const char *> lock_constant_names{
 };
 /** Used by lock_get_mode_str to cache results. Strings pointed by these
 pointers might be in use by performance schema and thus can not be freed
-until the very end */
+until the very end.
+Protected by exclusive global lock_sys latch.
+*/
 static std::unordered_map<uint, const char *> lock_cached_lock_mode_names;
 
 /** A static class for reporting notifications about deadlocks */
@@ -150,9 +152,32 @@ class Deadlock_notifier {
 };
 
 #ifdef UNIV_DEBUG
-/** Validates the lock system.
- @return true if ok */
-static bool lock_validate();
+namespace locksys {
+
+bool owns_exclusive_global_latch() {
+  return lock_sys->latches.owns_exclusive_global_latch();
+}
+
+bool owns_shared_global_latch() {
+  return lock_sys->latches.owns_shared_global_latch();
+}
+
+bool owns_page_shard(const page_id_t &page_id) {
+  return lock_sys->latches.owns_page_shard(page_id);
+}
+
+bool owns_table_shard(const dict_table_t &table) {
+  return lock_sys->latches.owns_table_shard(table);
+}
+
+bool owns_lock_shard(const lock_t *lock) {
+  if (lock->is_record_lock()) {
+    return lock_sys->latches.owns_page_shard(lock->rec_lock.get_page_id());
+  } else {
+    return lock_sys->latches.owns_table_shard(*lock->tab_lock.table);
+  }
+}
+}  // namespace locksys
 
 /** Validates the record lock queues on a page.
  @return true if ok */
@@ -294,14 +319,14 @@ void lock_sys_create(
 
   lock_sys = static_cast<lock_sys_t *>(ut_zalloc_nokey(lock_sys_sz));
 
+  new (lock_sys) lock_sys_t{};
+
   void *ptr = &lock_sys[1];
 
   lock_sys->waiting_threads = static_cast<srv_slot_t *>(ptr);
 
   lock_sys->last_slot = lock_sys->waiting_threads;
 
-  mutex_create(LATCH_ID_LOCK_SYS, &lock_sys->mutex);
-
   mutex_create(LATCH_ID_LOCK_SYS_WAIT, &lock_sys->wait_mutex);
 
   lock_sys->timeout_event = os_event_create(nullptr);
@@ -328,7 +353,10 @@ static ulint lock_rec_lock_fold(const lock_t *lock) {
 void lock_sys_resize(ulint n_cells) {
   hash_table_t *old_hash;
 
-  lock_mutex_enter();
+  /* We will rearrange locks between buckets and change the parameters of hash
+  function used in sharding of latches, so we have to prevent everyone from
+  accessing lock sys queues, or even computing shard id. */
+  locksys::Global_exclusive_latch_guard guard{};
 
   old_hash = lock_sys->rec_hash;
   lock_sys->rec_hash = hash_create(n_cells);
@@ -366,8 +394,6 @@ void lock_sys_resize(ulint n_cells) {
     }
     mutex_exit(&buf_pool->LRU_list_mutex);
   }
-
-  lock_mutex_exit();
 }
 
 /** Closes the lock system at database shutdown. */
@@ -383,7 +409,6 @@ void lock_sys_close(void) {
 
   os_event_destroy(lock_sys->timeout_event);
 
-  mutex_destroy(&lock_sys->mutex);
   mutex_destroy(&lock_sys->wait_mutex);
 
   srv_slot_t *slot = lock_sys->waiting_threads;
@@ -397,6 +422,9 @@ void lock_sys_close(void) {
     ut_free(const_cast<char *>(cached_lock_mode_name.second));
   }
   lock_cached_lock_mode_names.clear();
+
+  lock_sys->~lock_sys_t();
+
   ut_free(lock_sys);
 
   lock_sys = nullptr;
@@ -411,9 +439,9 @@ ulint lock_get_size(void) { return ((ulint)sizeof(lock_t)); }
 UNIV_INLINE
 void lock_set_lock_and_trx_wait(lock_t *lock) {
   auto trx = lock->trx;
-  ut_a(trx->lock.wait_lock == nullptr);
-  ut_ad(lock_mutex_own());
   ut_ad(trx_mutex_own(trx));
+  ut_a(trx->lock.wait_lock == nullptr);
+  ut_ad(locksys::owns_lock_shard(lock));
 
   trx->lock.wait_lock = lock;
   trx->lock.wait_lock_type = lock_get_type_low(lock);
@@ -630,19 +658,13 @@ void lock_rec_trx_wait(lock_t *lock, ulint i, ulint type) {
   }
 }
 
-/** Determines if there are explicit record locks on a page.
- @return an explicit record lock on the page, or NULL if there are none */
-lock_t *lock_rec_expl_exist_on_page(space_id_t space,  /*!< in: space id */
-                                    page_no_t page_no) /*!< in: page number */
-{
+bool lock_rec_expl_exist_on_page(space_id_t space, page_no_t page_no) {
   lock_t *lock;
-
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{page_id_t{space, page_no}};
   /* Only used in ibuf pages, so rec_hash is good enough */
   lock = lock_rec_get_first_on_page_addr(lock_sys->rec_hash, space, page_no);
-  lock_mutex_exit();
 
-  return (lock);
+  return (lock != nullptr);
 }
 
 /** Resets the record lock bitmap to zero. NOTE: does not touch the wait_lock
@@ -690,7 +712,7 @@ const lock_t *lock_rec_get_prev(
   lock_t *found_lock = nullptr;
   hash_table_t *hash;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(in_lock->rec_lock.get_page_id()));
   ut_ad(lock_get_type_low(in_lock) == LOCK_REC);
 
   space = in_lock->rec_lock.space;
@@ -726,7 +748,7 @@ const lock_t *lock_rec_get_prev(
 UNIV_INLINE
 const lock_t *lock_rec_has_expl(ulint precise_mode, const buf_block_t *block,
                                 ulint heap_no, const trx_t *trx) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad((precise_mode & LOCK_MODE_MASK) == LOCK_S ||
         (precise_mode & LOCK_MODE_MASK) == LOCK_X);
   ut_ad(
@@ -765,7 +787,7 @@ static const lock_t *lock_rec_other_has_expl_req(
                               requests by all transactions
                               are taken into account */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(mode == LOCK_X || mode == LOCK_S);
 
   /* Only GAP lock can be on SUPREMUM, and we are not looking
@@ -799,7 +821,7 @@ static const lock_t *lock_rec_other_has_conflicting(
     ulint heap_no,            /*!< in: heap number of the record */
     const trx_t *trx)         /*!< in: our transaction */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(!(mode & ~(ulint)(LOCK_MODE_MASK | LOCK_GAP | LOCK_REC_NOT_GAP |
                           LOCK_INSERT_INTENTION)));
   ut_ad(!(mode & LOCK_PREDICATE));
@@ -815,20 +837,20 @@ static const lock_t *lock_rec_other_has_conflicting(
 
 /** Checks if some transaction has an implicit x-lock on a record in a secondary
  index.
+ @param[in]   rec       user record
+ @param[in]   index     secondary index
+ @param[in]   offsets   rec_get_offsets(rec, index)
  @return transaction id of the transaction which has the x-lock, or 0;
  NOTE that this function can return false positives but never false
- negatives. The caller must confirm all positive results by calling
- trx_is_active(). */
-static trx_t *lock_sec_rec_some_has_impl(
-    const rec_t *rec,     /*!< in: user record */
-    dict_index_t *index,  /*!< in: secondary index */
-    const ulint *offsets) /*!< in: rec_get_offsets(rec, index) */
-{
+ negatives. The caller must confirm all positive results by checking if the trx
+ is still active. */
+static trx_t *lock_sec_rec_some_has_impl(const rec_t *rec, dict_index_t *index,
+                                         const ulint *offsets) {
   trx_t *trx;
   trx_id_t max_trx_id;
   const page_t *page = page_align(rec);
 
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   ut_ad(!trx_sys_mutex_own());
   ut_ad(!index->is_clustered());
   ut_ad(page_rec_is_user_rec(rec));
@@ -875,7 +897,8 @@ static bool lock_rec_other_trx_holds_expl(ulint precise_mode, const trx_t *trx,
                                           const buf_block_t *block) {
   bool holds = false;
 
-  lock_mutex_enter();
+  /* We will inspect locks from various shards when inspecting transactions. */
+  locksys::Global_exclusive_latch_guard guard{};
   /* If trx_rw_is_active returns non-null impl_trx it only means that impl_trx
   was active at some moment during the call, but might already be in
   TRX_STATE_COMMITTED_IN_MEMORY when we execute the body of the if.
@@ -903,20 +926,15 @@ static bool lock_rec_other_trx_holds_expl(ulint precise_mode, const trx_t *trx,
     mutex_exit(&trx_sys->mutex);
   }
 
-  lock_mutex_exit();
-
   return (holds);
 }
 #endif /* UNIV_DEBUG */
 
-/** Return approximate number or record locks (bits set in the bitmap) for
- this transaction. Since delete-marked records may be removed, the
- record count will not be precise.
- The caller must be holding lock_sys->mutex. */
-ulint lock_number_of_rows_locked(
-    const trx_lock_t *trx_lock) /*!< in: transaction locks */
-{
-  ut_ad(lock_mutex_own());
+ulint lock_number_of_rows_locked(const trx_lock_t *trx_lock) {
+  /* We need exclusive lock_sys access, because trx_lock->n_rec_locks is
+  modified while holding sharded lock only, so we need to disable all writers
+  for this number to be meaningful */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   return (trx_lock->n_rec_locks);
 }
@@ -932,7 +950,7 @@ ulint lock_number_of_tables_locked(const trx_t *trx) {
 /**
 Do some checks and prepare for creating a new record lock */
 void RecLock::prepare() const {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(m_rec_id.get_page_id()));
   ut_ad(m_trx == thr_get_trx(m_thr));
 
   /* Test if there already is some other reason to suspend thread:
@@ -969,7 +987,7 @@ Create the lock instance
 @return a record lock instance */
 lock_t *RecLock::lock_alloc(trx_t *trx, dict_index_t *index, ulint mode,
                             const RecID &rec_id, ulint size) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
   /* We are about to modify structures in trx->lock which needs trx->mutex */
   ut_ad(trx_mutex_own(trx));
 
@@ -991,7 +1009,7 @@ lock_t *RecLock::lock_alloc(trx_t *trx, dict_index_t *index, ulint mode,
   lock->index = index;
 
   /* Note the creation timestamp */
-  ut_d(lock->m_seq = ++lock_sys->m_seq);
+  ut_d(lock->m_seq = lock_sys->m_seq.fetch_add(1));
 
   /* Setup the lock attributes */
 
@@ -1013,9 +1031,9 @@ lock_t *RecLock::lock_alloc(trx_t *trx, dict_index_t *index, ulint mode,
     memset(&lock[1], 0x0, size);
   }
 
-  rec_lock.space = rec_id.m_space_id;
+  rec_lock.space = rec_id.get_page_id().space();
 
-  rec_lock.page_no = rec_id.m_page_no;
+  rec_lock.page_no = rec_id.get_page_id().page_no();
 
   /* Set the bit corresponding to rec */
 
@@ -1036,7 +1054,8 @@ static void lock_rec_insert_to_waiting(hash_table_t *lock_hash, lock_t *lock,
                                        const RecID &rec_id) {
   ut_ad(lock->is_waiting());
   ut_ad(rec_id.matches(lock));
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
+  ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
 
   const ulint fold = rec_id.fold();
   HASH_INSERT(lock_t, hash, lock_hash, fold, lock);
@@ -1049,7 +1068,8 @@ static void lock_rec_insert_to_waiting(hash_table_t *lock_hash, lock_t *lock,
 static void lock_rec_insert_to_granted(hash_table_t *lock_hash, lock_t *lock,
                                        const RecID &rec_id) {
   ut_ad(rec_id.matches(lock));
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
+  ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
   ut_ad(!lock->is_waiting());
 
   /* Move the target lock to the head of the list. */
@@ -1063,17 +1083,46 @@ static void lock_rec_insert_to_granted(hash_table_t *lock_hash, lock_t *lock,
   cell->node = lock;
   lock->hash = next;
 }
+namespace locksys {
+/**
+Adds the lock to the list of trx's locks.
+Requires lock->trx to be already set.
+Bumps the trx_lock_version.
+@param[in,out]  lock  The lock that we want to add to lock->trx->lock.trx_locks
+*/
+static void add_to_trx_locks(lock_t *lock) {
+  ut_ad(lock->trx != nullptr);
+  ut_ad(trx_mutex_own(lock->trx));
+  UT_LIST_ADD_LAST(lock->trx->lock.trx_locks, lock);
+  lock->trx->lock.trx_locks_version++;
+}
+
+/**
+Removes the lock from the list of trx's locks.
+Bumps the trx_lock_version.
+@param[in,out]  lock  The lock that we want to remove from
+                      lock->trx->lock.trx_locks
+*/
+static void remove_from_trx_locks(lock_t *lock) {
+  ut_ad(lock->trx != nullptr);
+  ut_ad(trx_mutex_own(lock->trx));
+  UT_LIST_REMOVE(lock->trx->lock.trx_locks, lock);
+  lock->trx->lock.trx_locks_version++;
+}
+}  // namespace locksys
 
 void RecLock::lock_add(lock_t *lock) {
   ut_ad((lock->type_mode | LOCK_REC) == (m_mode | LOCK_REC));
-  ut_ad(lock_mutex_own());
+  ut_ad(m_rec_id.matches(lock));
+  ut_ad(locksys::owns_page_shard(m_rec_id.get_page_id()));
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
   ut_ad(trx_mutex_own(lock->trx));
 
   bool wait = m_mode & LOCK_WAIT;
 
   hash_table_t *lock_hash = lock_hash_get(m_mode);
 
-  ++lock->index->table->n_rec_locks;
+  lock->index->table->n_rec_locks.fetch_add(1, std::memory_order_relaxed);
 
   if (!wait) {
     lock_rec_insert_to_granted(lock_hash, lock, m_rec_id);
@@ -1090,7 +1139,7 @@ void RecLock::lock_add(lock_t *lock) {
 #endif /* HAVE_PSI_DATA_LOCK_INTERFACE */
 #endif /* HAVE_PSI_THREAD_INTERFACE */
 
-  UT_LIST_ADD_LAST(lock->trx->lock.trx_locks, lock);
+  locksys::add_to_trx_locks(lock);
 
   if (wait) {
     lock_set_lock_and_trx_wait(lock);
@@ -1102,7 +1151,7 @@ void RecLock::lock_add(lock_t *lock) {
 @param[in] prdt			Predicate lock (optional)
 @return a new lock instance */
 lock_t *RecLock::create(trx_t *trx, const lock_prdt_t *prdt) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(m_rec_id.get_page_id()));
 
   /* Ensure that another transaction doesn't access the trx
   lock state and lock data structures while we are adding the
@@ -1193,7 +1242,7 @@ static void lock_mark_trx_for_rollback(hit_list_t &hit_list, trx_id_t hp_trx_id,
 static void lock_create_wait_for_edge(trx_t *waiter, trx_t *blocker) {
   ut_ad(trx_mutex_own(waiter));
   ut_ad(waiter->lock.wait_lock != nullptr);
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_lock_shard(waiter->lock.wait_lock));
   ut_ad(waiter->lock.blocking_trx.load() == nullptr);
   /* We don't call lock_wait_request_check_for_cycles() here as it
   would be slightly premature: the trx is not yet inserted into a slot of
@@ -1209,7 +1258,7 @@ static void lock_create_wait_for_edge(trx_t *waiter, trx_t *blocker) {
 Setup the requesting transaction state for lock grant
 @param[in,out] lock		Lock for which to change state */
 void RecLock::set_wait_state(lock_t *lock) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
   ut_ad(m_trx == lock->trx);
   ut_ad(trx_mutex_own(m_trx));
   ut_ad(lock_get_wait(lock));
@@ -1225,7 +1274,7 @@ void RecLock::set_wait_state(lock_t *lock) {
 }
 
 dberr_t RecLock::add_to_waitq(const lock_t *wait_for, const lock_prdt_t *prdt) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(m_rec_id.get_page_id()));
   ut_ad(m_trx == thr_get_trx(m_thr));
 
   /* It is not that the body of this function requires trx->mutex, but some of
@@ -1269,9 +1318,10 @@ rows (and thus: queues) this function moves it to the front of whole bucket.
 @param	[in]	lock	a granted lock to be moved
 @param	[in]	rec_id	record id which specifies particular queue and bucket */
 static void lock_rec_move_granted_to_front(lock_t *lock, const RecID &rec_id) {
-  ut_ad(lock_mutex_own());
   ut_ad(!lock->is_waiting());
   ut_ad(rec_id.matches(lock));
+  ut_ad(locksys::owns_page_shard(rec_id.get_page_id()));
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
 
   const auto hash_table = lock->hash_table();
   HASH_DELETE(lock_t, hash, hash_table, rec_id.fold(), lock);
@@ -1294,7 +1344,7 @@ UNIV_INLINE
 lock_t *lock_rec_find_similar_on_page(uint32_t type_mode, size_t heap_no,
                                       lock_t *lock, const trx_t *trx,
                                       bool &found_waiter_before_lock) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
   found_waiter_before_lock = false;
   for (/* No op */; lock != nullptr; lock = lock_rec_get_next_on_page(lock)) {
     if (lock->trx == trx && lock->type_mode == type_mode &&
@@ -1327,7 +1377,7 @@ static void lock_rec_add_to_queue(ulint type_mode, const buf_block_t *block,
                                   trx_t *trx,
                                   const bool we_own_trx_mutex = false) {
 #ifdef UNIV_DEBUG
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(we_own_trx_mutex == trx_mutex_own(trx));
 
   ut_ad(index->is_clustered() ||
@@ -1436,7 +1486,7 @@ lock_rec_req_status lock_rec_lock_fast(
     dict_index_t *index,      /*!< in: index of record */
     que_thr_t *thr)           /*!< in: query thread */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(!srv_read_only_mode);
   ut_ad((LOCK_MODE_MASK & mode) != LOCK_S ||
         lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
@@ -1549,7 +1599,7 @@ DB_SKIP_LOCKED, or DB_LOCK_NOWAIT */
 static dberr_t lock_rec_lock_slow(bool impl, select_mode sel_mode, ulint mode,
                                   const buf_block_t *block, ulint heap_no,
                                   dict_index_t *index, que_thr_t *thr) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(!srv_read_only_mode);
   ut_ad((LOCK_MODE_MASK & mode) != LOCK_S ||
         lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
@@ -1663,7 +1713,7 @@ DB_SKIP_LOCKED, or DB_LOCK_NOWAIT */
 static dberr_t lock_rec_lock(bool impl, select_mode sel_mode, ulint mode,
                              const buf_block_t *block, ulint heap_no,
                              dict_index_t *index, que_thr_t *thr) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(!srv_read_only_mode);
   ut_ad((LOCK_MODE_MASK & mode) != LOCK_S ||
         lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
@@ -1711,7 +1761,7 @@ static const lock_t *lock_rec_has_to_wait_in_queue(
   ulint bit_offset;
   hash_table_t *hash;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(wait_lock->rec_lock.get_page_id()));
   ut_ad(lock_get_wait(wait_lock));
   ut_ad(lock_get_type_low(wait_lock) == LOCK_REC);
 
@@ -1739,10 +1789,12 @@ static const lock_t *lock_rec_has_to_wait_in_queue(
 }
 
 /** Grants a lock to a waiting lock request and releases the waiting
- transaction. The caller must hold lock_sys->mutex but not lock->trx->mutex. */
-static void lock_grant(lock_t *lock) /*!< in/out: waiting lock request */
-{
-  ut_ad(lock_mutex_own());
+transaction. The caller must hold lock_sys latch for the shard containing the
+lock, but not the lock->trx->mutex.
+@param[in,out]    lock    waiting lock request
+ */
+static void lock_grant(lock_t *lock) {
+  ut_ad(locksys::owns_lock_shard(lock));
   ut_ad(!trx_mutex_own(lock->trx));
 
   trx_mutex_enter(lock->trx);
@@ -1754,6 +1806,7 @@ static void lock_grant(lock_t *lock) /*!< in/out: waiting lock request */
       ib::error(ER_IB_MSG_637) << "Transaction already had an"
                                << " AUTO-INC lock!";
     } else {
+      ut_ad(table->autoinc_trx == nullptr);
       table->autoinc_trx = lock->trx;
 
       ib_vector_push(lock->trx->lock.autoinc_locks, &lock);
@@ -1774,18 +1827,39 @@ void lock_make_trx_hit_list(trx_t *hp_trx, hit_list_t &hit_list) {
   const trx_id_t hp_trx_id = hp_trx->id;
   ut_ad(trx_can_be_handled_by_current_thread(hp_trx));
   ut_ad(trx_is_high_priority(hp_trx));
-  const lock_t *lock = hp_trx->lock.wait_lock;
-  bool waits_for_record = (nullptr != lock && lock->is_record_lock());
+  /* To avoid slow procedure involving global exclusive latch below, we first
+  check if this transaction is waiting for a lock at all. It's unsafe to read
+  hp->lock.wait_lock without latching whole lock_sys as it might temporarily
+  change to NULL during a concurrent B-tree reorganization, even though the
+  trx actually is still waiting.
+  TBD: Is it safe to use hp_trx->lock.que_state == TRX_QUE_LOCK_WAIT given that
+  que_state is not atomic, and writes to it happen without trx->mutex ? */
+  const bool is_waiting = (hp_trx->lock.blocking_trx.load() != nullptr);
   trx_mutex_exit(hp_trx);
-  if (!waits_for_record) {
+  if (!is_waiting) {
     return;
   }
 
-  lock_mutex_enter();
+  /* Current implementation of lock_make_trx_hit_list requires latching whole
+  lock_sys for following reasons:
+  1. it may call lock_cancel_waiting_and_release on a lock from completely
+  different shard of lock_sys than hp_trx->lock.wait_lock. Trying to latch
+  this other shard might create a deadlock cycle if it violates ordering of
+  shard latches (and there is 50% chance it will violate it). Moreover the
+  lock_cancel_waiting_and_release() requires an exclusive latch to avoid
+  deadlocks among trx->mutex-es, and trx->lock.wait_lock might be a table lock,
+  in which case exclusive latch is also needed to traverse table locks.
+  2. it may call trx_mutex_enter on a transaction which is waiting for a
+  lock, which violates one of assumptions used in the proof that a deadlock due
+  to acquiring trx->mutex-es is impossible
+  3. it attempts to read hp_trx->lock.wait_lock which might be modified by a
+  thread during B-tree reorganization when moving locks between queues
+  4. it attempts to operate on trx->lock.wait_lock of other transactions */
+  locksys::Global_exclusive_latch_guard guard{};
 
   /* Check again */
-  if (lock != hp_trx->lock.wait_lock) {
-    lock_mutex_exit();
+  const lock_t *lock = hp_trx->lock.wait_lock;
+  if (lock == nullptr || !lock->is_record_lock()) {
     return;
   }
   RecID rec_id{lock, lock_rec_find_set_bit(lock)};
@@ -1842,8 +1916,6 @@ void lock_make_trx_hit_list(trx_t *hp_trx, hit_list_t &hit_list) {
         return true;
       },
       lock->hash_table());
-
-  lock_mutex_exit();
 }
 
 /** Cancels a waiting record lock request and releases the waiting transaction
@@ -1852,8 +1924,8 @@ void lock_make_trx_hit_list(trx_t *hp_trx, hit_list_t &hit_list) {
 static void lock_rec_cancel(
     lock_t *lock) /*!< in: waiting record lock request */
 {
-  ut_ad(lock_mutex_own());
   ut_ad(lock_get_type_low(lock) == LOCK_REC);
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
 
   /* Reset the bit (there can be only one set bit) in the lock bitmap */
   lock_rec_reset_nth_bit(lock, lock_rec_find_set_bit(lock));
@@ -1873,7 +1945,8 @@ waiting_lock->trx points to blocking_lock->trx
                               wait */
 static void lock_update_wait_for_edge(const lock_t *waiting_lock,
                                       const lock_t *blocking_lock) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_lock_shard(waiting_lock));
+  ut_ad(locksys::owns_lock_shard(blocking_lock));
   ut_ad(waiting_lock->is_waiting());
   ut_ad(lock_has_to_wait(waiting_lock, blocking_lock));
   /* Still needs to wait, but perhaps the reason has changed */
@@ -1898,7 +1971,7 @@ static const lock_t *lock_rec_has_to_wait_for_granted(
     const size_t new_granted_index)
 
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(wait_lock->rec_lock.get_page_id()));
   ut_ad(wait_lock->is_record_lock());
 
   ut_ad(new_granted_index <= granted.size());
@@ -1945,8 +2018,8 @@ lock was (or still is) held */
 static void lock_rec_grant_by_heap_no(lock_t *in_lock, ulint heap_no) {
   const auto hash_table = in_lock->hash_table();
 
-  ut_ad(lock_mutex_own());
   ut_ad(in_lock->is_record_lock());
+  ut_ad(locksys::owns_page_shard(in_lock->rec_lock.get_page_id()));
 
   using LockDescriptorEx = std::pair<trx_schedule_weight_t, lock_t *>;
   /* Preallocate for 4 lists with 32 locks. */
@@ -2164,32 +2237,27 @@ static void lock_rec_dequeue_from_page(lock_t *in_lock) {
 void lock_rec_discard(lock_t *in_lock) {
   space_id_t space;
   page_no_t page_no;
-  trx_lock_t *trx_lock;
 
-  ut_ad(lock_mutex_own());
   ut_ad(lock_get_type_low(in_lock) == LOCK_REC);
-
-  trx_lock = &in_lock->trx->lock;
+  ut_ad(locksys::owns_page_shard(in_lock->rec_lock.get_page_id()));
 
   space = in_lock->rec_lock.space;
   page_no = in_lock->rec_lock.page_no;
 
-  ut_ad(in_lock->index->table->n_rec_locks > 0);
-  in_lock->index->table->n_rec_locks--;
+  ut_ad(in_lock->index->table->n_rec_locks.load() > 0);
+  in_lock->index->table->n_rec_locks.fetch_sub(1, std::memory_order_relaxed);
 
   /* We want the state of lock queue and trx_locks list to be synchronized
   atomically from the point of view of people using trx->mutex, so we perform
-  HASH_DELETE and UT_LIST_REMOVE while holding trx->mutex.
-  It might be the case that we already hold trx->mutex here, for example if we
-  came here from lock_release(trx). */
+  HASH_DELETE and UT_LIST_REMOVE while holding trx->mutex. */
 
   ut_ad(trx_mutex_own(in_lock->trx));
 
+  locksys::remove_from_trx_locks(in_lock);
+
   HASH_DELETE(lock_t, hash, lock_hash_get(in_lock->type_mode),
               lock_rec_fold(space, page_no), in_lock);
 
-  UT_LIST_REMOVE(trx_lock->trx_locks, in_lock);
-
   MONITOR_INC(MONITOR_RECLOCK_REMOVED);
   MONITOR_DEC(MONITOR_NUM_RECLOCK);
 }
@@ -2229,7 +2297,7 @@ void lock_rec_free_all_from_discard_page(
   space_id_t space;
   page_no_t page_no;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   space = block->page.id.space();
   page_no = block->page.id.page_no();
@@ -2252,7 +2320,7 @@ static void lock_rec_reset_and_release_wait_low(
 {
   lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   for (lock = lock_rec_get_first(hash, block, heap_no); lock != nullptr;
        lock = lock_rec_get_next(heap_no, lock)) {
@@ -2308,7 +2376,8 @@ static void lock_rec_inherit_to_gap(
 {
   lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(heir_block->get_page_id()));
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   /* If session is using READ COMMITTED or READ UNCOMMITTED isolation
   level, we do not want locks set by an UPDATE or a DELETE to be
@@ -2367,7 +2436,7 @@ static void lock_rec_inherit_to_gap_if_gap_lock(
 {
   lock_t *lock;
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
 
   for (lock = lock_rec_get_first(lock_sys->rec_hash, block, heap_no);
        lock != nullptr; lock = lock_rec_get_next(heap_no, lock)) {
@@ -2382,8 +2451,6 @@ static void lock_rec_inherit_to_gap_if_gap_lock(
                             heir_heap_no, lock->index, lock->trx);
     }
   }
-
-  lock_mutex_exit();
 }
 
 /** Moves the locks of a record to another record and resets the lock bits of
@@ -2403,7 +2470,8 @@ static void lock_rec_move_low(
 {
   lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(receiver->get_page_id()));
+  ut_ad(locksys::owns_page_shard(donator->get_page_id()));
 
   /* If the lock is predicate lock, it resides on INFIMUM record */
   ut_ad(lock_rec_get_first(lock_hash, receiver, receiver_heap_no) == nullptr ||
@@ -2492,109 +2560,97 @@ void lock_move_reorganize_page(
   UT_LIST_BASE_NODE_T(lock_t) old_locks;
   mem_heap_t *heap = nullptr;
   ulint comp;
+  {
+    /* We only process locks on block, not oblock */
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-  lock_mutex_enter();
-
-  /* FIXME: This needs to deal with predicate lock too */
-  lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block);
-
-  if (lock == nullptr) {
-    lock_mutex_exit();
-
-    return;
-  }
-
-  heap = mem_heap_create(256);
-
-  /* Copy first all the locks on the page to heap and reset the
-  bitmaps in the original locks; chain the copies of the locks
-  using the trx_locks field in them. */
+    /* FIXME: This needs to deal with predicate lock too */
+    lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block);
 
-  UT_LIST_INIT(old_locks, &lock_t::trx_locks);
+    if (lock == nullptr) {
+      return;
+    }
 
-  do {
-    /* Make a copy of the lock */
-    lock_t *old_lock = lock_rec_copy(lock, heap);
+    heap = mem_heap_create(256);
 
-    UT_LIST_ADD_LAST(old_locks, old_lock);
+    /* Copy first all the locks on the page to heap and reset the
+    bitmaps in the original locks; chain the copies of the locks
+    using the trx_locks field in them. */
 
-    /* Reset bitmap of lock */
-    lock_rec_bitmap_reset(lock);
+    UT_LIST_INIT(old_locks, &lock_t::trx_locks);
 
-    if (lock_get_wait(lock)) {
-      lock_reset_lock_and_trx_wait(lock);
-    }
+    do {
+      /* Make a copy of the lock */
+      lock_t *old_lock = lock_rec_copy(lock, heap);
 
-    lock = lock_rec_get_next_on_page(lock);
-  } while (lock != nullptr);
+      UT_LIST_ADD_LAST(old_locks, old_lock);
 
-  comp = page_is_comp(block->frame);
-  ut_ad(comp == page_is_comp(oblock->frame));
+      /* Reset bitmap of lock */
+      lock_rec_bitmap_reset(lock);
 
-  lock_move_granted_locks_to_front(old_locks);
+      if (lock_get_wait(lock)) {
+        lock_reset_lock_and_trx_wait(lock);
+      }
 
-  DBUG_EXECUTE_IF("do_lock_reverse_page_reorganize",
-                  UT_LIST_REVERSE(old_locks););
+      lock = lock_rec_get_next_on_page(lock);
+    } while (lock != nullptr);
 
-  for (lock = UT_LIST_GET_FIRST(old_locks); lock != nullptr;
-       lock = UT_LIST_GET_NEXT(trx_locks, lock)) {
-    /* NOTE: we copy also the locks set on the infimum and
-    supremum of the page; the infimum may carry locks if an
-    update of a record is occurring on the page, and its locks
-    were temporarily stored on the infimum */
-    const rec_t *rec1 = page_get_infimum_rec(buf_block_get_frame(block));
-    const rec_t *rec2 = page_get_infimum_rec(buf_block_get_frame(oblock));
-
-    /* Set locks according to old locks */
-    for (;;) {
-      ulint old_heap_no;
-      ulint new_heap_no;
+    comp = page_is_comp(block->frame);
+    ut_ad(comp == page_is_comp(oblock->frame));
 
-      if (comp) {
-        old_heap_no = rec_get_heap_no_new(rec2);
-        new_heap_no = rec_get_heap_no_new(rec1);
+    lock_move_granted_locks_to_front(old_locks);
 
-        rec1 = page_rec_get_next_low(rec1, true);
-        rec2 = page_rec_get_next_low(rec2, true);
-      } else {
-        old_heap_no = rec_get_heap_no_old(rec2);
-        new_heap_no = rec_get_heap_no_old(rec1);
-        ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
+    DBUG_EXECUTE_IF("do_lock_reverse_page_reorganize",
+                    UT_LIST_REVERSE(old_locks););
 
-        rec1 = page_rec_get_next_low(rec1, false);
-        rec2 = page_rec_get_next_low(rec2, false);
-      }
+    for (lock = UT_LIST_GET_FIRST(old_locks); lock != nullptr;
+         lock = UT_LIST_GET_NEXT(trx_locks, lock)) {
+      /* NOTE: we copy also the locks set on the infimum and
+      supremum of the page; the infimum may carry locks if an
+      update of a record is occurring on the page, and its locks
+      were temporarily stored on the infimum */
+      const rec_t *rec1 = page_get_infimum_rec(buf_block_get_frame(block));
+      const rec_t *rec2 = page_get_infimum_rec(buf_block_get_frame(oblock));
+
+      /* Set locks according to old locks */
+      for (;;) {
+        ulint old_heap_no;
+        ulint new_heap_no;
+
+        if (comp) {
+          old_heap_no = rec_get_heap_no_new(rec2);
+          new_heap_no = rec_get_heap_no_new(rec1);
+
+          rec1 = page_rec_get_next_low(rec1, true);
+          rec2 = page_rec_get_next_low(rec2, true);
+        } else {
+          old_heap_no = rec_get_heap_no_old(rec2);
+          new_heap_no = rec_get_heap_no_old(rec1);
+          ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
+
+          rec1 = page_rec_get_next_low(rec1, false);
+          rec2 = page_rec_get_next_low(rec2, false);
+        }
 
-      /* Clear the bit in old_lock. */
-      if (old_heap_no < lock->rec_lock.n_bits &&
-          lock_rec_reset_nth_bit(lock, old_heap_no)) {
-        /* NOTE that the old lock bitmap could be too
-        small for the new heap number! */
+        /* Clear the bit in old_lock. */
+        if (old_heap_no < lock->rec_lock.n_bits &&
+            lock_rec_reset_nth_bit(lock, old_heap_no)) {
+          /* NOTE that the old lock bitmap could be too
+          small for the new heap number! */
 
-        lock_rec_add_to_queue(lock->type_mode, block, new_heap_no, lock->index,
-                              lock->trx);
-      }
+          lock_rec_add_to_queue(lock->type_mode, block, new_heap_no,
+                                lock->index, lock->trx);
+        }
 
-      if (new_heap_no == PAGE_HEAP_NO_SUPREMUM) {
-        ut_ad(old_heap_no == PAGE_HEAP_NO_SUPREMUM);
-        break;
+        if (new_heap_no == PAGE_HEAP_NO_SUPREMUM) {
+          ut_ad(old_heap_no == PAGE_HEAP_NO_SUPREMUM);
+          break;
+        }
       }
-    }
 
-#ifdef UNIV_DEBUG
-    {
-      ulint i = lock_rec_find_set_bit(lock);
-
-      /* Check that all locks were moved. */
-      if (i != ULINT_UNDEFINED) {
-        ib::fatal(ER_IB_MSG_640) << "lock_move_reorganize_page(): " << i
-                                 << " not moved in " << (void *)lock;
-      }
+      ut_ad(lock_rec_find_set_bit(lock) == ULINT_UNDEFINED);
     }
-#endif /* UNIV_DEBUG */
-  }
-
-  lock_mutex_exit();
+  } /* Shard_latches_guard */
 
   mem_heap_free(heap);
 
@@ -2617,75 +2673,75 @@ void lock_move_rec_list_end(
   ut_ad(buf_block_get_frame(block) == page_align(rec));
   ut_ad(comp == page_is_comp(buf_block_get_frame(new_block)));
 
-  lock_mutex_enter();
+  {
+    locksys::Shard_latches_guard guard{*block, *new_block};
 
-  for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
-       lock = lock_rec_get_next_on_page(lock)) {
-    const rec_t *rec1 = rec;
-    const rec_t *rec2;
-    const ulint type_mode = lock->type_mode;
+    for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
+         lock = lock_rec_get_next_on_page(lock)) {
+      const rec_t *rec1 = rec;
+      const rec_t *rec2;
+      const ulint type_mode = lock->type_mode;
 
-    if (comp) {
-      if (page_offset(rec1) == PAGE_NEW_INFIMUM) {
-        rec1 = page_rec_get_next_low(rec1, true);
-      }
+      if (comp) {
+        if (page_offset(rec1) == PAGE_NEW_INFIMUM) {
+          rec1 = page_rec_get_next_low(rec1, true);
+        }
 
-      rec2 = page_rec_get_next_low(
-          buf_block_get_frame(new_block) + PAGE_NEW_INFIMUM, true);
-    } else {
-      if (page_offset(rec1) == PAGE_OLD_INFIMUM) {
-        rec1 = page_rec_get_next_low(rec1, false);
+        rec2 = page_rec_get_next_low(
+            buf_block_get_frame(new_block) + PAGE_NEW_INFIMUM, true);
+      } else {
+        if (page_offset(rec1) == PAGE_OLD_INFIMUM) {
+          rec1 = page_rec_get_next_low(rec1, false);
+        }
+
+        rec2 = page_rec_get_next_low(
+            buf_block_get_frame(new_block) + PAGE_OLD_INFIMUM, false);
       }
 
-      rec2 = page_rec_get_next_low(
-          buf_block_get_frame(new_block) + PAGE_OLD_INFIMUM, false);
-    }
+      /* Copy lock requests on user records to new page and
+      reset the lock bits on the old */
 
-    /* Copy lock requests on user records to new page and
-    reset the lock bits on the old */
+      for (;;) {
+        ulint rec1_heap_no;
+        ulint rec2_heap_no;
 
-    for (;;) {
-      ulint rec1_heap_no;
-      ulint rec2_heap_no;
+        if (comp) {
+          rec1_heap_no = rec_get_heap_no_new(rec1);
 
-      if (comp) {
-        rec1_heap_no = rec_get_heap_no_new(rec1);
+          if (rec1_heap_no == PAGE_HEAP_NO_SUPREMUM) {
+            break;
+          }
 
-        if (rec1_heap_no == PAGE_HEAP_NO_SUPREMUM) {
-          break;
-        }
+          rec2_heap_no = rec_get_heap_no_new(rec2);
+          rec1 = page_rec_get_next_low(rec1, true);
+          rec2 = page_rec_get_next_low(rec2, true);
+        } else {
+          rec1_heap_no = rec_get_heap_no_old(rec1);
 
-        rec2_heap_no = rec_get_heap_no_new(rec2);
-        rec1 = page_rec_get_next_low(rec1, true);
-        rec2 = page_rec_get_next_low(rec2, true);
-      } else {
-        rec1_heap_no = rec_get_heap_no_old(rec1);
+          if (rec1_heap_no == PAGE_HEAP_NO_SUPREMUM) {
+            break;
+          }
 
-        if (rec1_heap_no == PAGE_HEAP_NO_SUPREMUM) {
-          break;
-        }
+          rec2_heap_no = rec_get_heap_no_old(rec2);
 
-        rec2_heap_no = rec_get_heap_no_old(rec2);
+          ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
 
-        ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
+          rec1 = page_rec_get_next_low(rec1, false);
+          rec2 = page_rec_get_next_low(rec2, false);
+        }
 
-        rec1 = page_rec_get_next_low(rec1, false);
-        rec2 = page_rec_get_next_low(rec2, false);
-      }
+        if (rec1_heap_no < lock->rec_lock.n_bits &&
+            lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
+          if (type_mode & LOCK_WAIT) {
+            lock_reset_lock_and_trx_wait(lock);
+          }
 
-      if (rec1_heap_no < lock->rec_lock.n_bits &&
-          lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
-        if (type_mode & LOCK_WAIT) {
-          lock_reset_lock_and_trx_wait(lock);
+          lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
+                                lock->trx);
         }
-
-        lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
-                              lock->trx);
       }
     }
-  }
-
-  lock_mutex_exit();
+  } /* Shard_latches_guard */
 
 #ifdef UNIV_DEBUG_LOCK_VALIDATE
   ut_ad(lock_rec_validate_page(block));
@@ -2714,73 +2770,70 @@ void lock_move_rec_list_start(const buf_block_t *new_block, /*!< in: index page
   ut_ad(new_block->frame == page_align(old_end));
   ut_ad(comp == page_rec_is_comp(old_end));
 
-  lock_mutex_enter();
-
-  for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
-       lock = lock_rec_get_next_on_page(lock)) {
-    const rec_t *rec1;
-    const rec_t *rec2;
-    const ulint type_mode = lock->type_mode;
+  {
+    locksys::Shard_latches_guard guard{*block, *new_block};
 
-    if (comp) {
-      rec1 = page_rec_get_next_low(
-          buf_block_get_frame(block) + PAGE_NEW_INFIMUM, true);
-      rec2 = page_rec_get_next_low(old_end, true);
-    } else {
-      rec1 = page_rec_get_next_low(
-          buf_block_get_frame(block) + PAGE_OLD_INFIMUM, false);
-      rec2 = page_rec_get_next_low(old_end, false);
-    }
+    for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
+         lock = lock_rec_get_next_on_page(lock)) {
+      const rec_t *rec1;
+      const rec_t *rec2;
+      const ulint type_mode = lock->type_mode;
 
-    /* Copy lock requests on user records to new page and
-    reset the lock bits on the old */
+      if (comp) {
+        rec1 = page_rec_get_next_low(
+            buf_block_get_frame(block) + PAGE_NEW_INFIMUM, true);
+        rec2 = page_rec_get_next_low(old_end, true);
+      } else {
+        rec1 = page_rec_get_next_low(
+            buf_block_get_frame(block) + PAGE_OLD_INFIMUM, false);
+        rec2 = page_rec_get_next_low(old_end, false);
+      }
 
-    while (rec1 != rec) {
-      ulint rec1_heap_no;
-      ulint rec2_heap_no;
+      /* Copy lock requests on user records to new page and
+      reset the lock bits on the old */
 
-      if (comp) {
-        rec1_heap_no = rec_get_heap_no_new(rec1);
-        rec2_heap_no = rec_get_heap_no_new(rec2);
+      while (rec1 != rec) {
+        ulint rec1_heap_no;
+        ulint rec2_heap_no;
 
-        rec1 = page_rec_get_next_low(rec1, true);
-        rec2 = page_rec_get_next_low(rec2, true);
-      } else {
-        rec1_heap_no = rec_get_heap_no_old(rec1);
-        rec2_heap_no = rec_get_heap_no_old(rec2);
+        if (comp) {
+          rec1_heap_no = rec_get_heap_no_new(rec1);
+          rec2_heap_no = rec_get_heap_no_new(rec2);
 
-        ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
+          rec1 = page_rec_get_next_low(rec1, true);
+          rec2 = page_rec_get_next_low(rec2, true);
+        } else {
+          rec1_heap_no = rec_get_heap_no_old(rec1);
+          rec2_heap_no = rec_get_heap_no_old(rec2);
 
-        rec1 = page_rec_get_next_low(rec1, false);
-        rec2 = page_rec_get_next_low(rec2, false);
-      }
+          ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
 
-      if (rec1_heap_no < lock->rec_lock.n_bits &&
-          lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
-        if (type_mode & LOCK_WAIT) {
-          lock_reset_lock_and_trx_wait(lock);
+          rec1 = page_rec_get_next_low(rec1, false);
+          rec2 = page_rec_get_next_low(rec2, false);
         }
 
-        lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
-                              lock->trx);
+        if (rec1_heap_no < lock->rec_lock.n_bits &&
+            lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
+          if (type_mode & LOCK_WAIT) {
+            lock_reset_lock_and_trx_wait(lock);
+          }
+
+          lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
+                                lock->trx);
+        }
       }
-    }
 
 #ifdef UNIV_DEBUG
-    if (page_rec_is_supremum(rec)) {
-      ulint i;
+      if (page_rec_is_supremum(rec)) {
+        ulint i;
 
-      for (i = PAGE_HEAP_NO_USER_LOW; i < lock_rec_get_n_bits(lock); i++) {
-        if (lock_rec_get_nth_bit(lock, i)) {
-          ib::fatal(ER_IB_MSG_641) << "lock_move_rec_list_start():" << i
-                                   << " not moved in " << (void *)lock;
+        for (i = PAGE_HEAP_NO_USER_LOW; i < lock_rec_get_n_bits(lock); i++) {
+          ut_a(!lock_rec_get_nth_bit(lock, i));
         }
       }
-    }
 #endif /* UNIV_DEBUG */
-  }
-
-  lock_mutex_exit();
+    }
+  } /* Shard_latches_guard */
 
 #ifdef UNIV_DEBUG_LOCK_VALIDATE
   ut_ad(lock_rec_validate_page(block));
@@ -2809,53 +2862,53 @@ void lock_rtr_move_rec_list(const buf_block_t *new_block, /*!< in: index page to
   ut_ad(new_block->frame == page_align(rec_move[0].new_rec));
   ut_ad(comp == page_rec_is_comp(rec_move[0].new_rec));
 
-  lock_mutex_enter();
+  {
+    locksys::Shard_latches_guard guard{*new_block, *block};
 
-  for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
-       lock = lock_rec_get_next_on_page(lock)) {
-    ulint moved = 0;
-    const rec_t *rec1;
-    const rec_t *rec2;
-    const ulint type_mode = lock->type_mode;
+    for (lock = lock_rec_get_first_on_page(lock_sys->rec_hash, block); lock;
+         lock = lock_rec_get_next_on_page(lock)) {
+      ulint moved = 0;
+      const rec_t *rec1;
+      const rec_t *rec2;
+      const ulint type_mode = lock->type_mode;
 
-    /* Copy lock requests on user records to new page and
-    reset the lock bits on the old */
+      /* Copy lock requests on user records to new page and
+      reset the lock bits on the old */
 
-    while (moved < num_move) {
-      ulint rec1_heap_no;
-      ulint rec2_heap_no;
+      while (moved < num_move) {
+        ulint rec1_heap_no;
+        ulint rec2_heap_no;
 
-      rec1 = rec_move[moved].old_rec;
-      rec2 = rec_move[moved].new_rec;
+        rec1 = rec_move[moved].old_rec;
+        rec2 = rec_move[moved].new_rec;
 
-      if (comp) {
-        rec1_heap_no = rec_get_heap_no_new(rec1);
-        rec2_heap_no = rec_get_heap_no_new(rec2);
+        if (comp) {
+          rec1_heap_no = rec_get_heap_no_new(rec1);
+          rec2_heap_no = rec_get_heap_no_new(rec2);
 
-      } else {
-        rec1_heap_no = rec_get_heap_no_old(rec1);
-        rec2_heap_no = rec_get_heap_no_old(rec2);
+        } else {
+          rec1_heap_no = rec_get_heap_no_old(rec1);
+          rec2_heap_no = rec_get_heap_no_old(rec2);
 
-        ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
-      }
-
-      if (rec1_heap_no < lock->rec_lock.n_bits &&
-          lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
-        if (type_mode & LOCK_WAIT) {
-          lock_reset_lock_and_trx_wait(lock);
+          ut_ad(!memcmp(rec1, rec2, rec_get_data_size_old(rec2)));
         }
 
-        lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
-                              lock->trx);
+        if (rec1_heap_no < lock->rec_lock.n_bits &&
+            lock_rec_reset_nth_bit(lock, rec1_heap_no)) {
+          if (type_mode & LOCK_WAIT) {
+            lock_reset_lock_and_trx_wait(lock);
+          }
 
-        rec_move[moved].moved = true;
-      }
+          lock_rec_add_to_queue(type_mode, new_block, rec2_heap_no, lock->index,
+                                lock->trx);
 
-      moved++;
-    }
-  }
+          rec_move[moved].moved = true;
+        }
 
-  lock_mutex_exit();
+        moved++;
+      }
+    }
+  } /* Shard_latches_guard */
 
 #ifdef UNIV_DEBUG_LOCK_VALIDATE
   ut_ad(lock_rec_validate_page(block));
@@ -2868,7 +2921,7 @@ void lock_update_split_right(
 {
   ulint heap_no = lock_get_min_heap_no(right_block);
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*left_block, *right_block};
 
   /* Move the locks on the supremum of the left page to the supremum
   of the right page */
@@ -2881,8 +2934,6 @@ void lock_update_split_right(
 
   lock_rec_inherit_to_gap(left_block, right_block, PAGE_HEAP_NO_SUPREMUM,
                           heap_no);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a page is merged to the right. */
@@ -2897,7 +2948,7 @@ void lock_update_merge_right(
                                     index  page which
                                     will be  discarded */
 {
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*left_block, *right_block};
 
   /* Inherit the locks from the supremum of the left page to the
   original successor of infimum on the right page, to which the left
@@ -2923,8 +2974,6 @@ void lock_update_merge_right(
 #endif /* UNIV_DEBUG */
 
   lock_rec_free_all_from_discard_page(left_block);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when the root page is copied to another in
@@ -2937,13 +2986,12 @@ void lock_update_root_raise(
     const buf_block_t *block, /*!< in: index page to which copied */
     const buf_block_t *root)  /*!< in: root page */
 {
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*block, *root};
 
   /* Move the locks on the supremum of the root to the supremum
   of block */
 
   lock_rec_move(block, root, PAGE_HEAP_NO_SUPREMUM, PAGE_HEAP_NO_SUPREMUM);
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a page is copied to another and the original
@@ -2954,15 +3002,13 @@ void lock_update_copy_and_discard(
     const buf_block_t *block)     /*!< in: index page;
                                   NOT the root! */
 {
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*new_block, *block};
 
   /* Move the locks on the supremum of the old page to the supremum
   of new_page */
 
   lock_rec_move(new_block, block, PAGE_HEAP_NO_SUPREMUM, PAGE_HEAP_NO_SUPREMUM);
   lock_rec_free_all_from_discard_page(block);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a page is split to the left. */
@@ -2972,15 +3018,13 @@ void lock_update_split_left(
 {
   ulint heap_no = lock_get_min_heap_no(right_block);
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*left_block, *right_block};
 
   /* Inherit the locks to the supremum of the left page from the
   successor of the infimum on the right page */
 
   lock_rec_inherit_to_gap(left_block, right_block, PAGE_HEAP_NO_SUPREMUM,
                           heap_no);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a page is merged to the left. */
@@ -2997,7 +3041,7 @@ void lock_update_merge_left(
 
   ut_ad(left_block->frame == page_align(orig_pred));
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*left_block, *right_block};
 
   left_next_rec = page_rec_get_next_const(orig_pred);
 
@@ -3033,8 +3077,6 @@ void lock_update_merge_left(
 #endif /* UNIV_DEBUG */
 
   lock_rec_free_all_from_discard_page(right_block);
-
-  lock_mutex_exit();
 }
 
 /** Resets the original locks on heir and replaces them with gap type locks
@@ -3051,13 +3093,11 @@ void lock_rec_reset_and_inherit_gap_locks(
     ulint heap_no)                 /*!< in: heap_no of the
                                    donating record */
 {
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*heir_block, *block};
 
   lock_rec_reset_and_release_wait(heir_block, heir_heap_no);
 
   lock_rec_inherit_to_gap(heir_block, block, heir_heap_no, heap_no);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a page is discarded. */
@@ -3073,13 +3113,12 @@ void lock_update_discard(
   ulint heap_no;
   const page_t *page = block->frame;
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*heir_block, *block};
 
   if (!lock_rec_get_first_on_page(lock_sys->rec_hash, block) &&
       (!lock_rec_get_first_on_page(lock_sys->prdt_page_hash, block)) &&
       (!lock_rec_get_first_on_page(lock_sys->prdt_hash, block))) {
     /* No locks exist on page, nothing to do */
-    lock_mutex_exit();
 
     return;
   }
@@ -3114,8 +3153,6 @@ void lock_update_discard(
   }
 
   lock_rec_free_all_from_discard_page(block);
-
-  lock_mutex_exit();
 }
 
 /** Updates the lock table when a new user record is inserted. */
@@ -3161,7 +3198,7 @@ void lock_update_delete(
     next_heap_no = rec_get_heap_no_old(page + rec_get_next_offs(rec, false));
   }
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
 
   /* Let the next record inherit the locks from rec, in gap mode */
 
@@ -3170,8 +3207,6 @@ void lock_update_delete(
   /* Reset the lock bits on rec and release waiting transactions */
 
   lock_rec_reset_and_release_wait(block, heap_no);
-
-  lock_mutex_exit();
 }
 
 /** Stores on the page infimum record the explicit locks of another record.
@@ -3192,11 +3227,9 @@ void lock_rec_store_on_page_infimum(
 
   ut_ad(block->frame == page_align(rec));
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
 
   lock_rec_move(block, block, PAGE_HEAP_NO_INFIMUM, heap_no);
-
-  lock_mutex_exit();
 }
 
 /** Restores the state of explicit lock requests on a single record, where the
@@ -3213,11 +3246,9 @@ void lock_rec_restore_from_page_infimum(
 {
   ulint heap_no = page_rec_get_heap_no(rec);
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*block, *donator};
 
   lock_rec_move(block, donator, heap_no, PAGE_HEAP_NO_INFIMUM);
-
-  lock_mutex_exit();
 }
 
 /*========================= TABLE LOCKS ==============================*/
@@ -3242,7 +3273,7 @@ lock_t *lock_table_create(dict_table_t *table, /*!< in/out: database table
   lock_t *lock;
 
   ut_ad(table && trx);
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_table_shard(*table));
   ut_ad(trx_mutex_own(trx));
   ut_ad(trx_can_be_handled_by_current_thread(trx));
 
@@ -3253,7 +3284,7 @@ lock_t *lock_table_create(dict_table_t *table, /*!< in/out: database table
   from the transaction lock heap. */
   if (type_mode == LOCK_AUTO_INC) {
     lock = table->autoinc_lock;
-
+    ut_ad(table->autoinc_trx == nullptr);
     table->autoinc_trx = trx;
 
     ib_vector_push(trx->lock.autoinc_locks, &lock);
@@ -3264,9 +3295,9 @@ lock_t *lock_table_create(dict_table_t *table, /*!< in/out: database table
     lock = static_cast<lock_t *>(
         mem_heap_alloc(trx->lock.lock_heap, sizeof(*lock)));
   }
-
   lock->type_mode = uint32_t(type_mode | LOCK_TABLE);
   lock->trx = trx;
+  ut_d(lock->m_seq = lock_sys->m_seq.fetch_add(1));
 
   lock->tab_lock.table = table;
 
@@ -3281,7 +3312,7 @@ lock_t *lock_table_create(dict_table_t *table, /*!< in/out: database table
 #endif /* HAVE_PSI_DATA_LOCK_INTERFACE */
 #endif /* HAVE_PSI_THREAD_INTERFACE */
 
-  UT_LIST_ADD_LAST(trx->lock.trx_locks, lock);
+  locksys::add_to_trx_locks(lock);
 
   ut_list_append(table->locks, lock, TableLockGetNode());
 
@@ -3304,7 +3335,6 @@ UNIV_INLINE
 void lock_table_pop_autoinc_locks(
     trx_t *trx) /*!< in/out: transaction that owns the AUTOINC locks */
 {
-  ut_ad(lock_mutex_own());
   /* We will access and modify trx->lock.autoinc_locks so we need trx->mutex */
   ut_ad(trx_mutex_own(trx));
   ut_ad(!ib_vector_is_empty(trx->lock.autoinc_locks));
@@ -3333,9 +3363,9 @@ void lock_table_remove_autoinc_lock(
   lock_t *autoinc_lock;
   lint i = ib_vector_size(trx->lock.autoinc_locks) - 1;
 
-  ut_ad(lock_mutex_own());
   ut_ad(lock_get_mode(lock) == LOCK_AUTO_INC);
   ut_ad(lock_get_type_low(lock) & LOCK_TABLE);
+  ut_ad(locksys::owns_table_shard(*lock->tab_lock.table));
   ut_ad(!ib_vector_is_empty(trx->lock.autoinc_locks));
 
   /* With stored functions and procedures the user may drop
@@ -3381,18 +3411,17 @@ void lock_table_remove_low(lock_t *lock) /*!< in/out: table lock */
   trx_t *trx;
   dict_table_t *table;
 
-  ut_ad(lock_mutex_own());
-
   trx = lock->trx;
   /* We will modify trx->lock.trx_locks so we need trx->mutex */
   ut_ad(trx_mutex_own(trx));
   table = lock->tab_lock.table;
+  ut_ad(locksys::owns_table_shard(*table));
   const auto lock_mode = lock_get_mode(lock);
   /* Remove the table from the transaction's AUTOINC vector, if
   the lock that is being released is an AUTOINC lock. */
   if (lock_mode == LOCK_AUTO_INC) {
-    /* The table's AUTOINC lock can get transferred to
-    another transaction before we get here. */
+    /* The table's AUTOINC lock could not be granted to us yet. */
+    ut_ad(table->autoinc_trx == trx || lock->is_waiting());
     if (table->autoinc_trx == trx) {
       table->autoinc_trx = nullptr;
     }
@@ -3403,17 +3432,17 @@ void lock_table_remove_low(lock_t *lock) /*!< in/out: table lock */
 
     We only store locks that were granted in the
     trx->autoinc_locks vector (see lock_table_create()
-    and lock_grant()). Therefore it can be empty and we
-    need to check for that. */
+    and lock_grant()). */
 
-    if (!lock_get_wait(lock) && !ib_vector_is_empty(trx->lock.autoinc_locks)) {
+    if (!lock_get_wait(lock)) {
       lock_table_remove_autoinc_lock(lock, trx);
     }
   }
   ut_a(0 < table->count_by_mode[lock_mode]);
   --table->count_by_mode[lock_mode];
 
-  UT_LIST_REMOVE(trx->lock.trx_locks, lock);
+  locksys::remove_from_trx_locks(lock);
+
   ut_list_remove(table->locks, lock, TableLockGetNode());
 
   MONITOR_INC(MONITOR_TABLELOCK_REMOVED);
@@ -3431,7 +3460,7 @@ static dberr_t lock_table_enqueue_waiting(
 {
   trx_t *trx;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_table_shard(*table));
   ut_ad(!srv_read_only_mode);
 
   trx = thr_get_trx(thr);
@@ -3491,7 +3520,7 @@ const lock_t *lock_table_other_has_incompatible(
 {
   const lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_table_shard(*table));
 
   // According to lock_compatibility_matrix, an intention lock can wait only
   // for LOCK_S or LOCK_X. If there are no LOCK_S nor LOCK_X locks in the queue,
@@ -3500,7 +3529,7 @@ const lock_t *lock_table_other_has_incompatible(
   // as then there are almost no LOCK_S nor LOCK_X, but many DML queries still
   // need to get an intention lock to perform their action - while this never
   // causes them to wait for a "data lock", it might cause them to wait for
-  // lock_sys->mutex if the operation takes Omega(n).
+  // lock_sys table shard latch for the duration of table lock queue operation.
 
   if ((mode == LOCK_IS || mode == LOCK_IX) &&
       table->count_by_mode[LOCK_S] == 0 && table->count_by_mode[LOCK_X] == 0) {
@@ -3568,7 +3597,8 @@ dberr_t lock_table(ulint flags, /*!< in: if BTR_NO_LOCKING_FLAG bit is set,
             (there is some long explanation starting with "How do we prevent
             crashes caused by ongoing operations...")
     lock_remove_recovered_trx_record_locks
-      (this seems to be used during recovery, and recovery is single-threaded)
+      (this seems to be used to remove locks of recovered transactions from
+      table being dropped, and recovered transactions shouldn't call lock_table)
   Also the InnoDB Memcached plugin causes a callchain:
   innodb_store -> innodb_conn_init -> innodb_api_begin -> innodb_cb_cursor_lock
   -> ib_cursor_set_lock_mode -> ib_cursor_lock -> ib_trx_lock_table_with_retry
@@ -3594,7 +3624,7 @@ dberr_t lock_table(ulint flags, /*!< in: if BTR_NO_LOCKING_FLAG bit is set,
     trx_set_rw_mode(trx);
   }
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard table_latch_guard{*table};
 
   /* We have to check if the new lock is compatible with any locks
   other transactions have in the table lock queue. */
@@ -3619,7 +3649,6 @@ dberr_t lock_table(ulint flags, /*!< in: if BTR_NO_LOCKING_FLAG bit is set,
     err = DB_SUCCESS;
   }
 
-  lock_mutex_exit();
   trx_mutex_exit(trx);
 
   ut_ad(err == DB_SUCCESS || err == DB_LOCK_WAIT || err == DB_DEADLOCK);
@@ -3635,9 +3664,7 @@ void lock_table_ix_resurrect(dict_table_t *table, /*!< in/out: table */
   if (lock_table_has(trx, table, LOCK_IX)) {
     return;
   }
-
-  lock_mutex_enter();
-
+  locksys::Shard_latch_guard table_latch_guard{*table};
   /* We have to check if the new lock is compatible with any locks
   other transactions have in the table lock queue. */
 
@@ -3645,7 +3672,6 @@ void lock_table_ix_resurrect(dict_table_t *table, /*!< in/out: table */
 
   trx_mutex_enter(trx);
   lock_table_create(table, LOCK_IX, trx);
-  lock_mutex_exit();
   trx_mutex_exit(trx);
 }
 
@@ -3663,10 +3689,10 @@ static const lock_t *lock_table_has_to_wait_in_queue(
   const dict_table_t *table;
   const lock_t *lock;
 
-  ut_ad(lock_mutex_own());
   ut_ad(lock_get_wait(wait_lock));
 
   table = wait_lock->tab_lock.table;
+  ut_ad(locksys::owns_table_shard(*table));
 
   const auto mode = lock_get_mode(wait_lock);
 
@@ -3722,10 +3748,10 @@ static void lock_table_dequeue(
                      behind will get their lock requests granted, if
                      they are now qualified to it */
 {
-  ut_ad(lock_mutex_own());
   /* This is needed for lock_table_remove_low(), but it's easier to understand
   the code if we assert it here as well */
   ut_ad(trx_mutex_own(in_lock->trx));
+  ut_ad(locksys::owns_table_shard(*in_lock->tab_lock.table));
   ut_a(lock_get_type_low(in_lock) == LOCK_TABLE);
 
   const auto mode = lock_get_mode(in_lock);
@@ -3743,7 +3769,7 @@ static void lock_table_dequeue(
   // as then there are almost no LOCK_S nor LOCK_X, but many DML queries still
   // need to get an intention lock to perform their action - while this never
   // causes them to wait for a "data lock", it might cause them to wait for
-  // lock_sys->mutex if the operation takes Omega(n) or even Omega(n^2)
+  // lock_sys table shard latch for the duration of table lock queue operation.
   if ((mode == LOCK_IS || mode == LOCK_IX) &&
       table->count_by_mode[LOCK_S] == 0 && table->count_by_mode[LOCK_X] == 0) {
     return;
@@ -3815,7 +3841,7 @@ run_again:
 @param[in]	lock		Lock that was unlocked
 @param[in]	heap_no		Heap no within the page for the lock. */
 static void lock_rec_release(lock_t *lock, ulint heap_no) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
   ut_ad(!lock_get_wait(lock));
   ut_ad(lock_get_type_low(lock) == LOCK_REC);
   ut_ad(lock_rec_get_nth_bit(lock, heap_no));
@@ -3838,47 +3864,47 @@ void lock_rec_unlock(
     const rec_t *rec,         /*!< in: record */
     lock_mode lock_mode)      /*!< in: LOCK_S or LOCK_X */
 {
-  ut_ad(!trx->lock.wait_lock);
   ut_ad(block->frame == page_align(rec));
   ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE));
   ut_ad(lock_mode == LOCK_S || lock_mode == LOCK_X);
 
   ulint heap_no = page_rec_get_heap_no(rec);
 
-  lock_mutex_enter();
-  trx_mutex_enter(trx);
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
+    trx_mutex_enter_first_of_two(trx);
+    ut_ad(!trx->lock.wait_lock);
 
-  lock_t *first_lock;
+    lock_t *first_lock;
 
-  first_lock = lock_rec_get_first(lock_sys->rec_hash, block, heap_no);
+    first_lock = lock_rec_get_first(lock_sys->rec_hash, block, heap_no);
 
-  /* Find the last lock with the same lock_mode and transaction
-  on the record. */
+    /* Find the last lock with the same lock_mode and transaction
+    on the record. */
 
-  for (auto lock = first_lock; lock != nullptr;
-       lock = lock_rec_get_next(heap_no, lock)) {
-    if (lock->trx == trx && lock_get_mode(lock) == lock_mode &&
-        lock_rec_get_rec_not_gap(lock)) {
+    for (auto lock = first_lock; lock != nullptr;
+         lock = lock_rec_get_next(heap_no, lock)) {
+      if (lock->trx == trx && lock_get_mode(lock) == lock_mode &&
+          lock_rec_get_rec_not_gap(lock)) {
 #ifdef UNIV_DEBUG
-      /* Since we actually found the first, not the last lock, lets check
-         that it is also the last one */
-      for (auto lock2 = lock_rec_get_next(heap_no, lock); lock2 != nullptr;
-           lock2 = lock_rec_get_next(heap_no, lock2)) {
-        ut_ad(!(lock2->trx == trx && lock_get_mode(lock2) == lock_mode &&
-                lock_rec_get_rec_not_gap(lock2)));
-      }
+        /* Since we actually found the first, not the last lock, lets check
+           that it is also the last one */
+        for (auto lock2 = lock_rec_get_next(heap_no, lock); lock2 != nullptr;
+             lock2 = lock_rec_get_next(heap_no, lock2)) {
+          ut_ad(!(lock2->trx == trx && lock_get_mode(lock2) == lock_mode &&
+                  lock_rec_get_rec_not_gap(lock2)));
+        }
 #endif
-      lock_rec_release(lock, heap_no);
+        lock_rec_release(lock, heap_no);
 
-      lock_mutex_exit();
-      trx_mutex_exit(trx);
+        trx_mutex_exit(trx);
 
-      return;
+        return;
+      }
     }
-  }
 
-  lock_mutex_exit();
-  trx_mutex_exit(trx);
+    trx_mutex_exit(trx);
+  } /* Shard_latch_guard */
 
   {
     size_t stmt_len;
@@ -3940,20 +3966,164 @@ static void lock_release_read_lock(lock_t *lock, bool only_gap) {
   }
 }
 
+namespace locksys {
+
+/** A helper function which solves a chicken-and-egg problem occurring when one
+needs to iterate over trx's locks and perform some actions on them. Iterating
+over this list requires trx->mutex (or exclusive global lock_sys latch), and
+operating on a lock requires lock_sys latches, yet the latching order requires
+lock_sys latches to be taken before trx->mutex.
+One way around it is to use exclusive global lock_sys latch, which heavily
+deteriorates concurrency. Another is to try to reacquire the latches in needed
+order, veryfing that the list wasn't modified meanwhile.
+This function performs following steps:
+1. releases trx->mutex,
+2. acquires proper lock_sys shard latch,
+3. reaquires trx->mutex
+4. executes f unless trx's locks list has changed
+Before and after this function following should hold:
+- the shared global lock_sys latch is held
+- the trx->mutex is held
+@param[in]    trx     the trx, locks of which we are interested in
+@param[in]    shard   description of the shard we want to latch
+@param[in]    f       the function to execute when the shard is latched
+@return true if f was called, false if it couldn't be called because trx locks
+        have changed while relatching trx->mutex
+*/
+template <typename S, typename F>
+static bool try_relatch_trx_and_shard_and_do(const trx_t *const trx,
+                                             const S &shard, F &&f) {
+  ut_ad(locksys::owns_shared_global_latch());
+  ut_ad(trx_mutex_own(trx));
+
+  const auto expected_version = trx->lock.trx_locks_version;
+  trx_mutex_exit(trx);
+  DEBUG_SYNC_C("try_relatch_trx_and_shard_and_do_noted_expected_version");
+  locksys::Shard_naked_latch_guard guard{shard};
+  trx_mutex_enter_first_of_two(trx);
+
+  /* Check that list was not modified while we were reacquiring latches */
+  if (expected_version != trx->lock.trx_locks_version) {
+    /* Someone has modified the list while we were re-acquiring the latches so,
+    it is unsafe to operate on the lock. It might have been released, or maybe
+    even assigned to another transaction (in case of AUTOINC lock). More
+    importantly, we need to let know the caller that the list it is iterating
+    over has been modified, which affects next/prev pointers. */
+    return false;
+  }
+
+  std::forward<F>(f)();
+  return true;
+}
+
+/** A helper function which solves a chicken-and-egg problem occurring when one
+needs to iterate over trx's locks and perform some actions on them. Iterating
+over this list requires trx->mutex (or exclusive global lock_sys latch), and
+operating on a lock requires lock_sys latches, yet the latching order requires
+lock_sys latches to be taken before trx->mutex.
+One way around it is to use exclusive global lock_sys latch, which heavily
+deteriorates concurrency. Another is to try to reacquire the latches in needed
+order, veryfing that the list wasn't modified meanwhile.
+This function performs following steps:
+1. releases trx->mutex,
+2. acquires proper lock_sys shard latch for given lock,
+3. reaquires trx->mutex
+4. executes f unless trx's locks list has changed
+Before and after this function following should hold:
+- the shared global lock_sys latch is held
+- the trx->mutex is held
+@param[in]    lock    the lock we are interested in
+@param[in]    f       the function to execute when the shard is latched
+@return true if f was called, false if it couldn't be called because trx locks
+        have changed while relatching trx->mutex
+*/
+template <typename F>
+static bool try_relatch_trx_and_shard_and_do(const lock_t *lock, F &&f) {
+  if (lock_get_type_low(lock) == LOCK_REC) {
+    return try_relatch_trx_and_shard_and_do(
+        lock->trx, lock->rec_lock.get_page_id(), std::forward<F>(f));
+  }
+
+  ut_ad(lock_get_type_low(lock) == LOCK_TABLE);
+  return try_relatch_trx_and_shard_and_do(lock->trx, *lock->tab_lock.table,
+                                          std::forward<F>(f));
+}
+
+/** Tries to release read locks of a transaction without latching the whole
+lock sys. This may fail, if there are many concurrent threads editing the
+list of locks of this transaction (for example due to B-tree pages being
+merged or split, or due to implicit-to-explicit conversion).
+It is called during XA prepare to release locks early.
+@param[in,out]	trx		transaction
+@param[in]	only_gap	release only GAP locks
+@return true if and only if it succeeded to do the job*/
+static bool try_release_read_locks_in_s_mode(trx_t *trx, bool only_gap) {
+  /* In order to access trx->lock.trx_locks safely we need to hold trx->mutex.
+  So, conceptually we'd love to hold trx->mutex while iterating through
+  trx->lock.trx_locks.
+  However the latching order only allows us to obtain trx->mutex AFTER any
+  lock_sys latch.
+  One way around this problem is to simply latch the whole lock_sys in exclusive
+  mode (which also prevents any changes to trx->lock.trx_locks), however this
+  impacts performance in appliers (TPS drops by up to 10%).
+  Here we use a different approach:
+  1. we extract lock from the list when holding the trx->mutex,
+  2. identify the shard of lock_sys it belongs to,
+  3. store the current version of trx->lock.trx_locks
+  4. release the trx->mutex,
+  5. acquire the lock_sys shard's latch,
+  6. and reacquire the trx->mutex,
+  7. verify that the version of trx->lock.trx_locks has not changed
+  8. and only then perform any action on the lock.
+  */
+  ut_ad(trx_mutex_own(trx));
+  ut_ad(locksys::owns_shared_global_latch());
+  lock_t *lock = UT_LIST_GET_FIRST(trx->lock.trx_locks);
+
+  while (lock != nullptr) {
+    ut_ad(trx_mutex_own(trx));
+    /* We didn't latch the lock_sys shard this `lock` is in, so we only read a
+    bare minimum set of information from the `lock`, such as the type, space,
+    page_no, and next pointer, which, as long as we hold trx->mutex, should be
+    immutable.
+
+    Store the pointer to the next lock in the list, because in some cases we are
+    going to remove `lock` from the list, which clears the pointer to next lock
+    */
+    auto next_lock = UT_LIST_GET_NEXT(trx_locks, lock);
+    if (lock_get_type_low(lock) == LOCK_REC) {
+      /* Following call temporarily releases trx->mutex */
+      if (!try_relatch_trx_and_shard_and_do(
+              lock, [=]() { lock_release_read_lock(lock, only_gap); })) {
+        /* Someone has modified the list while we were re-acquiring the latches
+        so we need to start over again. */
+        return false;
+      }
+    }
+    /* As we have verified that the version has not changed, it must be the case
+    that the next_lock is still the next lock as well */
+    lock = next_lock;
+  }
+  return true;
+}
+}  // namespace locksys
+
 /** Release read locks of a transacion latching the whole lock-sys in
-exclusive mode.
+exclusive mode, which is a bit too expensive to do by default.
 It is called during XA prepare to release locks early.
 @param[in,out]	trx		transaction
 @param[in]	only_gap	release only GAP locks */
 static void lock_trx_release_read_locks_in_x_mode(trx_t *trx, bool only_gap) {
   ut_ad(!trx_mutex_own(trx));
 
-  lock_mutex_enter();
-  trx_mutex_enter(trx);
+  /* We will iterate over locks from various shards. */
+  locksys::Global_exclusive_latch_guard guard{};
+  trx_mutex_enter_first_of_two(trx);
 
   lock_t *lock = UT_LIST_GET_FIRST(trx->lock.trx_locks);
 
   while (lock != nullptr) {
+    DEBUG_SYNC_C("lock_trx_release_read_locks_in_x_mode_will_release");
     /* Store the pointer to the next lock in the list, because in some cases
     we are going to remove `lock` from the list, which clears the pointer to
     next lock */
@@ -3964,26 +4134,32 @@ static void lock_trx_release_read_locks_in_x_mode(trx_t *trx, bool only_gap) {
     lock = next_lock;
   }
 
-  lock_mutex_exit();
   trx_mutex_exit(trx);
 }
 
 void lock_trx_release_read_locks(trx_t *trx, bool only_gap) {
-  /* Avoid taking lock_sys if trx didn't acquire any lock.
-  We do not hold trx->mutex nor lock_sys latch while checking the emptiness of
-  trx->lock.trx_locks, but this is OK, because even if other threads are
-  modifying this list in parallel, they do not change the emptiness of it:
-  implicit-to-explicit conversion only occurs if the trx already has a table
-  intention lock, B-tree modification related operations always first create
-  a copy of old lock before removing old lock, and removal of wait lock can not
-  happen since we are not waiting. */
   ut_ad(trx_can_be_handled_by_current_thread(trx));
-  ut_ad(trx->lock.wait_lock == nullptr);
-  if (UT_LIST_GET_LEN(trx->lock.trx_locks) == 0) {
-    return;
+
+  size_t failures;
+  const size_t MAX_FAILURES = 5;
+
+  {
+    locksys::Global_shared_latch_guard shared_latch_guard{};
+    trx_mutex_enter(trx);
+    ut_ad(trx->lock.wait_lock == nullptr);
+
+    for (failures = 0; failures < MAX_FAILURES; ++failures) {
+      if (locksys::try_release_read_locks_in_s_mode(trx, only_gap)) {
+        break;
+      }
+    }
+
+    trx_mutex_exit(trx);
   }
 
-  lock_trx_release_read_locks_in_x_mode(trx, only_gap);
+  if (failures == MAX_FAILURES) {
+    lock_trx_release_read_locks_in_x_mode(trx, only_gap);
+  }
 }
 
 /** Releases transaction locks, and releases possible other transactions waiting
@@ -3991,16 +4167,12 @@ void lock_trx_release_read_locks(trx_t *trx, bool only_gap) {
 @param[in,out]  trx   transaction */
 static void lock_release(trx_t *trx) {
   lock_t *lock;
-
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   ut_ad(!trx_mutex_own(trx));
   ut_ad(!trx->is_dd_trx);
 
-  /* Don't take lock_sys mutex if trx didn't acquire any lock.
-  We want to check if trx->lock.trx_lock is empty without holding trx->mutex
-  nor lock_sys->mutex.
-  In order to access trx->lock.trx_locks safely we should hold at least
-  trx->mutex. But:
+  locksys::Global_shared_latch_guard shared_latch_guard{};
+  /* In order to access trx->lock.trx_locks safely we need to hold trx->mutex.
   The transaction is already in TRX_STATE_COMMITTED_IN_MEMORY state and is no
   longer referenced, so we are not afraid of implicit-to-explicit conversions,
   nor a cancellation of a wait_lock (we are running, not waiting). Still, there
@@ -4008,29 +4180,37 @@ static void lock_release(trx_t *trx) {
   locks to be moved from one page to another, which at the low level means that
   a new lock is created (and added to trx->lock.trx_locks) and the old one is
   removed (also from trx->lock.trx_locks) in that specific order.
-  Actually, there is no situation in our code, where some other thread can
-  change the number of explicit locks from 0 to non-zero, or vice-versa.
-  Even the implicit-to-explicit conversion presumes that our trx holds at least
-  an explicit IX table lock (since it was allowed to modify the table).
-  Thus, if the only thing we want to do is comparing with zero, then there is
-  no real risk here. */
-  if (UT_LIST_GET_LEN(trx->lock.trx_locks) == 0) {
-    return;
-  }
-
-  lock_mutex_enter();
+  So, conceptually we'd love to hold trx->mutex while iterating through
+  trx->lock.trx_locks.
+  However the latching order only allows us to obtain trx->mutex AFTER any
+  lock_sys latch. One way around this problem is to simply latch the whole
+  lock_sys in exclusive mode (which also prevents any changes to
+  trx->lock.trx_locks), however this impacts performance (TPS drops on
+  sysbench {pareto,uniform}-2S-{128,1024}-usrs tests by 3% to 11%) Here we
+  use a different approach:
+  1. we extract lock from the list when holding the trx->mutex,
+  2. identify the shard of lock_sys it belongs to,
+  3. release the trx->mutex,
+  4. acquire the lock_sys shard's latch,
+  5. and reacquire the trx->mutex,
+  6. verify that the lock pointer is still in trx->lock.trx_locks (so it is
+  safe to access it),
+  7. and only then perform any action on the lock.
+  */
   trx_mutex_enter(trx);
 
-  for (lock = UT_LIST_GET_LAST(trx->lock.trx_locks); lock != nullptr;
-       lock = UT_LIST_GET_LAST(trx->lock.trx_locks)) {
-    if (lock_get_type_low(lock) == LOCK_REC) {
-      lock_rec_dequeue_from_page(lock);
-    } else {
-      lock_table_dequeue(lock);
-    }
+  ut_ad(trx->lock.wait_lock == nullptr);
+  while ((lock = UT_LIST_GET_LAST(trx->lock.trx_locks)) != nullptr) {
+    /* Following call temporarily releases trx->mutex */
+    locksys::try_relatch_trx_and_shard_and_do(lock, [=]() {
+      if (lock_get_type_low(lock) == LOCK_REC) {
+        lock_rec_dequeue_from_page(lock);
+      } else {
+        lock_table_dequeue(lock);
+      }
+    });
   }
 
-  lock_mutex_exit();
   trx_mutex_exit(trx);
 }
 
@@ -4043,7 +4223,7 @@ static void lock_release(trx_t *trx) {
 static void lock_trx_table_locks_remove(const lock_t *lock_to_remove) {
   trx_t *trx = lock_to_remove->trx;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_table_shard(*lock_to_remove->tab_lock.table));
   /* We will modify trx->lock.table_locks so we need trx->mutex */
   ut_ad(trx_mutex_own(trx));
 
@@ -4075,7 +4255,7 @@ static void lock_remove_all_on_table_for_trx(
 
   /* This is used when we drop a table and indeed have exclusive lock_sys
   access. */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
   /* We need trx->mutex to iterate over trx->lock.trx_lock and it is needed by
   lock_trx_table_locks_remove() and lock_table_remove_low() but we haven't
   acquired it yet. */
@@ -4112,10 +4292,9 @@ static ulint lock_remove_recovered_trx_record_locks(
                          table itself */
 {
   ut_a(table != nullptr);
-  /* This is used in recovery where indeed we hold an exclusive lock_sys latch,
-  which is needed as we are about to iterate over locks held by multiple
-  transactions while they might be operating. */
-  ut_ad(lock_mutex_own());
+  /* We need exclusive lock_sys latch, as we are about to iterate over locks
+  held by multiple transactions while they might be operating. */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   ulint n_recovered_trx = 0;
 
@@ -4133,7 +4312,7 @@ static ulint lock_remove_recovered_trx_record_locks(
     acquired it yet. */
     ut_ad(!trx_mutex_own(trx));
     trx_mutex_enter(trx);
-    /* Because we are holding the lock_sys->mutex,
+    /* Because we are holding the exclusive global lock_sys latch,
     implicit locks cannot be converted to explicit ones
     while we are scanning the explicit locks. */
 
@@ -4186,7 +4365,8 @@ void lock_remove_all_on_table(
 {
   lock_t *lock;
 
-  lock_mutex_enter();
+  /* We will iterate over locks (including record locks) from various shards */
+  locksys::Global_exclusive_latch_guard guard{};
 
   for (lock = UT_LIST_GET_FIRST(table->locks); lock != nullptr;
        /* No op */) {
@@ -4233,8 +4413,6 @@ void lock_remove_all_on_table(
       lock_remove_recovered_trx_record_locks(table) == 0) {
     lock_sys->rollback_complete = true;
   }
-
-  lock_mutex_exit();
 }
 
 /*===================== VALIDATION AND DEBUGGING ====================*/
@@ -4243,8 +4421,9 @@ void lock_remove_all_on_table(
 static void lock_table_print(FILE *file,         /*!< in: file where to print */
                              const lock_t *lock) /*!< in: table type lock */
 {
-  ut_ad(lock_mutex_own());
   ut_a(lock_get_type_low(lock) == LOCK_TABLE);
+  /* We actually hold exclusive latch here, but we require just the shard */
+  ut_ad(locksys::owns_table_shard(*lock->tab_lock.table));
 
   fputs("TABLE LOCK table ", file);
   ut_print_name(file, lock->trx, lock->tab_lock.table->name.m_name);
@@ -4280,13 +4459,11 @@ static void lock_rec_print(FILE *file,         /*!< in: file where to print */
   space_id_t space;
   page_no_t page_no;
   mtr_t mtr;
-  mem_heap_t *heap = nullptr;
-  ulint offsets_[REC_OFFS_NORMAL_SIZE];
-  ulint *offsets = offsets_;
-  rec_offs_init(offsets_);
+  Rec_offsets offsets;
 
-  ut_ad(lock_mutex_own());
   ut_a(lock_get_type_low(lock) == LOCK_REC);
+  /* We actually hold exclusive latch here, but we require just the shard */
+  ut_ad(locksys::owns_page_shard(lock->rec_lock.get_page_id()));
 
   space = lock->rec_lock.space;
   page_no = lock->rec_lock.page_no;
@@ -4343,21 +4520,14 @@ static void lock_rec_print(FILE *file,         /*!< in: file where to print */
 
       rec = page_find_rec_with_heap_no(buf_block_get_frame(block), i);
 
-      offsets =
-          rec_get_offsets(rec, lock->index, offsets, ULINT_UNDEFINED, &heap);
-
       putc(' ', file);
-      rec_print_new(file, rec, offsets);
+      rec_print_new(file, rec, offsets.compute(rec, lock->index));
     }
 
     putc('\n', file);
   }
 
   mtr_commit(&mtr);
-
-  if (heap) {
-    mem_heap_free(heap);
-  }
 }
 
 #ifdef UNIV_DEBUG
@@ -4375,7 +4545,7 @@ static ulint lock_get_n_rec_locks(void) {
   ulint i;
 
   /* We need exclusive access to lock_sys to iterate over all buckets */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   for (i = 0; i < hash_get_n_cells(lock_sys->rec_hash); i++) {
     const lock_t *lock;
@@ -4392,25 +4562,8 @@ static ulint lock_get_n_rec_locks(void) {
 }
 #endif /* PRINT_NUM_OF_LOCK_STRUCTS */
 
-/** Prints info of locks for all transactions.
- @return false if not able to obtain lock mutex
- and exits without printing info */
-bool lock_print_info_summary(
-    FILE *file,   /*!< in: file where to print */
-    ibool nowait) /*!< in: whether to wait for the lock mutex */
-{
-  /* if nowait is false, wait on the lock mutex,
-  otherwise return immediately if fail to obtain the
-  mutex. */
-  if (!nowait) {
-    lock_mutex_enter();
-  } else if (lock_mutex_enter_nowait()) {
-    fputs(
-        "FAIL TO OBTAIN LOCK MUTEX,"
-        " SKIP LOCK INFO PRINTING\n",
-        file);
-    return (false);
-  }
+void lock_print_info_summary(FILE *file) {
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   if (lock_deadlock_found) {
     fputs(
@@ -4476,7 +4629,6 @@ bool lock_print_info_summary(
   fprintf(file, "Total number of lock structs in row lock hash table %lu\n",
           (ulong)lock_get_n_rec_locks());
 #endif /* PRINT_NUM_OF_LOCK_STRUCTS */
-  return (true);
 }
 
 /** Functor to print not-started transaction from the mysql_trx_list. */
@@ -4486,7 +4638,7 @@ struct PrintNotStarted {
 
   void operator()(const trx_t *trx) {
     /* We require exclusive access to lock_sys */
-    ut_ad(lock_mutex_own());
+    ut_ad(locksys::owns_exclusive_global_latch());
     ut_ad(trx->in_mysql_trx_list);
     ut_ad(mutex_own(&trx_sys->mutex));
 
@@ -4515,9 +4667,10 @@ class TrxLockIterator {
   const lock_t *current(const trx_t *trx) const {
     lock_t *lock;
     ulint i = 0;
-    /* trx->lock.trx_locks is protected by trx->mutex and lock_sys mutex, and we
-    assume we have the exclusive latch on lock_sys here */
-    ut_ad(lock_mutex_own());
+    /* Writes to trx->lock.trx_locks are protected by trx->mutex combined with a
+    shared lock_sys global latch, and we assume we have the exclusive latch on
+    lock_sys here. */
+    ut_ad(locksys::owns_exclusive_global_latch());
     for (lock = UT_LIST_GET_FIRST(trx->lock.trx_locks);
          lock != nullptr && i < m_index;
          lock = UT_LIST_GET_NEXT(trx_locks, lock), ++i) {
@@ -4602,8 +4755,8 @@ class TrxListIterator {
 @param[in]	trx	transaction */
 void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx) {
   /* We require exclusive lock_sys access so that trx->lock.wait_lock is
-  not being modified */
-  ut_ad(lock_mutex_own());
+  not being modified, and to access trx->lock.wait_started without trx->mutex.*/
+  ut_ad(locksys::owns_exclusive_global_latch());
   fprintf(file, "---");
 
   trx_print_latched(file, trx, 600);
@@ -4630,11 +4783,12 @@ void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx) {
   }
 }
 
-/** Prints info of locks for a transaction. This function will release the
- lock mutex and the trx_sys_t::mutex if the page was read from disk.
- @return true if page was read from the tablespace */
-static bool lock_rec_fetch_page(const lock_t *lock) /*!< in: record lock */
-{
+/** Reads the page containing the record protected by the given lock.
+This function will temporarily release the exclusive global latch and the
+trx_sys_t::mutex if the page was read from disk.
+@param[in]  lock  the record lock
+@return true if a page was successfully read from the tablespace */
+static bool lock_rec_fetch_page(const lock_t *lock) {
   ut_ad(lock_get_type_low(lock) == LOCK_REC);
 
   space_id_t space_id = lock->rec_lock.space;
@@ -4647,7 +4801,7 @@ static bool lock_rec_fetch_page(const lock_t *lock) /*!< in: record lock */
   if (found) {
     mtr_t mtr;
 
-    lock_mutex_exit();
+    locksys::Unsafe_global_latch_manipulator::exclusive_unlatch();
 
     mutex_exit(&trx_sys->mutex);
 
@@ -4665,7 +4819,7 @@ static bool lock_rec_fetch_page(const lock_t *lock) /*!< in: record lock */
       fil_space_release(space);
     }
 
-    lock_mutex_enter();
+    locksys::Unsafe_global_latch_manipulator::exclusive_latch();
 
     mutex_enter(&trx_sys->mutex);
 
@@ -4686,16 +4840,14 @@ static bool lock_trx_print_locks(
 {
   const lock_t *lock;
   /* We require exclusive access to lock_sys */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   /* Iterate over the transaction's locks. */
   while ((lock = iter.current(trx)) != nullptr) {
     if (lock_get_type_low(lock) == LOCK_REC) {
       if (load_block) {
-        /* Note: lock_rec_fetch_page() will
-        release both the lock mutex and the
-        trx_sys_t::mutex if it does a read
-        from disk. */
+        /* Note: lock_rec_fetch_page() will release both the exclusive global
+        latch and the trx_sys_t::mutex if it does a read from disk. */
 
         if (lock_rec_fetch_page(lock)) {
           /* We need to resync the
@@ -4741,14 +4893,9 @@ static bool lock_trx_print_locks(
   return (true);
 }
 
-/** Prints info of locks for each transaction. This function assumes that the
- caller holds the lock mutex and more importantly it will release the lock
- mutex on behalf of the caller. (This should be fixed in the future). */
-void lock_print_info_all_transactions(
-    FILE *file) /*!< in/out: file where to print */
-{
+void lock_print_info_all_transactions(FILE *file) {
   /* We require exclusive access to lock_sys */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   fprintf(file, "LIST OF TRANSACTIONS FOR EACH SESSION:\n");
 
@@ -4791,13 +4938,11 @@ void lock_print_info_all_transactions(
       TrxLockIterator &lock_iter = trx_iter.lock_iter();
 
       if (!lock_trx_print_locks(file, trx, lock_iter, load_block)) {
-        /* Resync trx_iter, the trx_sys->mutex and
-        the lock mutex were released. A page was
-        successfully read in.  We need to print its
-        contents on the next call to
-        lock_trx_print_locks(). On the next call to
-        lock_trx_print_locks() we should simply print
-        the contents of the page just read in.*/
+        /* Resync trx_iter, the trx_sys->mutex and exclusive global latch were
+        temporarily released. A page was successfully read in. We need to print
+        its contents on the next call to lock_trx_print_locks(). On the next
+        call to lock_trx_print_locks() we should simply print the contents of
+        the page just read in.*/
         load_block = false;
 
         continue;
@@ -4811,10 +4956,7 @@ void lock_print_info_all_transactions(
     trx_iter.next();
   }
 
-  lock_mutex_exit();
   mutex_exit(&trx_sys->mutex);
-
-  ut_ad(lock_validate());
 }
 
 #ifdef UNIV_DEBUG
@@ -4845,15 +4987,18 @@ static bool lock_table_queue_validate(
 {
   const lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  /* We actually hold exclusive latch here, but we require just the shard */
+  ut_ad(locksys::owns_table_shard(*table));
   ut_ad(trx_sys_mutex_own());
 
   for (lock = UT_LIST_GET_FIRST(table->locks); lock != nullptr;
        lock = UT_LIST_GET_NEXT(tab_lock.locks, lock)) {
     /* lock->trx->state cannot change from or to NOT_STARTED
     while we are holding the trx_sys->mutex. It may change
-    from ACTIVE to PREPARED, but it may not change to
-    COMMITTED, because we are holding the lock_sys->mutex. */
+    from ACTIVE to PREPARED. It may become COMMITTED_IN_MEMORY even though we
+    hold trx_sys->mutex in case it has trx->id==0, but even in this case it
+    will not be freed until it can release the table lock, and we prevent
+    this by latching its shard. */
     ut_ad(trx_assert_started(lock->trx));
 
     if (!lock_get_wait(lock)) {
@@ -4868,34 +5013,27 @@ static bool lock_table_queue_validate(
 
   return (true);
 }
-
+namespace locksys {
 /** Validates the lock queue on a single record.
- @return true if ok */
-static bool lock_rec_queue_validate(
-    bool locked_lock_trx_sys,
-    /*!< in: if the caller holds
-    both the lock mutex and
-    trx_sys_t->lock. */
-    const buf_block_t *block,  /*!< in: buffer block containing rec */
-    const rec_t *rec,          /*!< in: record to look at */
-    const dict_index_t *index, /*!< in: index, or NULL if not known */
-    const ulint *offsets)      /*!< in: rec_get_offsets(rec, index) */
-{
+@param[in]  block     buffer block containing rec
+@param[in]  rec       record to look at
+@param[in]  index     index, or NULL if not known
+@param[in]  offsets   rec_get_offsets(rec, index) */
+static void rec_queue_validate_latched(const buf_block_t *block,
+                                       const rec_t *rec,
+                                       const dict_index_t *index,
+                                       const ulint *offsets) {
+  ut_ad(owns_page_shard(block->get_page_id()));
+  ut_ad(mutex_own(&trx_sys->mutex));
   ut_a(rec);
   ut_a(block->frame == page_align(rec));
   ut_ad(rec_offs_validate(rec, index, offsets));
   ut_ad(!page_rec_is_comp(rec) == !rec_offs_comp(offsets));
-  ut_ad(lock_mutex_own() == locked_lock_trx_sys);
   ut_ad(!index || index->is_clustered() || !dict_index_is_online_ddl(index));
 
   ulint heap_no = page_rec_get_heap_no(rec);
   RecID rec_id{block, heap_no};
 
-  if (!locked_lock_trx_sys) {
-    lock_mutex_enter();
-    mutex_enter(&trx_sys->mutex);
-  }
-
   if (!page_rec_is_user_rec(rec)) {
     Lock_iter::for_each(rec_id, [&](lock_t *lock) {
       ut_ad(!trx_is_ac_nl_ro(lock->trx));
@@ -4911,12 +5049,7 @@ static bool lock_rec_queue_validate(
       return (true);
     });
 
-    if (!locked_lock_trx_sys) {
-      lock_mutex_exit();
-      mutex_exit(&trx_sys->mutex);
-    }
-
-    return (true);
+    return;
   }
 
   if (index == nullptr) {
@@ -4926,13 +5059,13 @@ static bool lock_rec_queue_validate(
     trx_id_t trx_id;
 
     /* Unlike the non-debug code, this invariant can only succeed
-    if the check and assertion are covered by the lock mutex. */
+    if the check and assertion are covered by the lock_sys latch. */
 
     trx_id = lock_clust_rec_some_has_impl(rec, index, offsets);
 
     const trx_t *impl_trx = trx_rw_is_active_low(trx_id, nullptr);
     if (impl_trx != nullptr) {
-      ut_ad(lock_mutex_own());
+      ut_ad(owns_page_shard(block->get_page_id()));
       ut_ad(trx_sys_mutex_own());
       /* impl_trx cannot become TRX_STATE_COMMITTED_IN_MEMORY nor removed from
       rw_trx_set until we release trx_sys->mutex, which means that currently all
@@ -4983,15 +5116,37 @@ static bool lock_rec_queue_validate(
 
     return (true);
   });
+}
 
-  if (!locked_lock_trx_sys) {
-    lock_mutex_exit();
-
-    mutex_exit(&trx_sys->mutex);
-  }
+/** Validates the lock queue on a single record.
+@param[in]  block     buffer block containing rec
+@param[in]  rec       record to look at
+@param[in]  index     index, or NULL if not known
+@param[in]  offsets   rec_get_offsets(rec, index) */
+static void rec_queue_latch_and_validate(const buf_block_t *block,
+                                         const rec_t *rec,
+                                         const dict_index_t *index,
+                                         const ulint *offsets) {
+  ut_ad(!owns_exclusive_global_latch());
+  ut_ad(!mutex_own(&trx_sys->mutex));
+
+  Shard_latch_guard guard{block->get_page_id()};
+  mutex_enter(&trx_sys->mutex);
+  rec_queue_validate_latched(block, rec, index, offsets);
+  mutex_exit(&trx_sys->mutex);
+}
 
-  return (true);
+/** Validates the lock queue on a single record.
+@param[in]  block     buffer block containing rec
+@param[in]  rec       record to look at
+@param[in]  index     index, or NULL if not known */
+static void rec_queue_latch_and_validate(const buf_block_t *block,
+                                         const rec_t *rec,
+                                         const dict_index_t *index) {
+  rec_queue_latch_and_validate(block, rec, index,
+                               Rec_offsets().compute(rec, index));
 }
+}  // namespace locksys
 
 /** Validates the record lock queues on a page.
  @return true if ok */
@@ -5003,14 +5158,11 @@ static bool lock_rec_validate_page(
   ulint nth_lock = 0;
   ulint nth_bit = 0;
   ulint i;
-  mem_heap_t *heap = nullptr;
-  ulint offsets_[REC_OFFS_NORMAL_SIZE];
-  ulint *offsets = offsets_;
-  rec_offs_init(offsets_);
+  Rec_offsets offsets;
 
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
   mutex_enter(&trx_sys->mutex);
 loop:
   lock = lock_rec_get_first_on_page_addr(
@@ -5037,15 +5189,14 @@ loop:
       if (i == 1 || lock_rec_get_nth_bit(lock, i)) {
         rec = page_find_rec_with_heap_no(block->frame, i);
         ut_a(rec);
-        offsets =
-            rec_get_offsets(rec, lock->index, offsets, ULINT_UNDEFINED, &heap);
 
         /* If this thread is holding the file space
         latch (fil_space_t::latch), the following
         check WILL break the latching order and may
         cause a deadlock of threads. */
 
-        lock_rec_queue_validate(true, block, rec, lock->index, offsets);
+        locksys::rec_queue_validate_latched(block, rec, lock->index,
+                                            offsets.compute(rec, lock->index));
 
         nth_bit = i + 1;
 
@@ -5059,12 +5210,8 @@ loop:
   goto loop;
 
 function_exit:
-  lock_mutex_exit();
   mutex_exit(&trx_sys->mutex);
 
-  if (heap != nullptr) {
-    mem_heap_free(heap);
-  }
   return (true);
 }
 
@@ -5076,7 +5223,7 @@ static bool lock_validate_table_locks(
   const trx_t *trx;
 
   /* We need exclusive access to lock_sys to iterate over trxs' locks */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   ut_ad(trx_list == &trx_sys->rw_trx_list);
@@ -5106,7 +5253,9 @@ static MY_ATTRIBUTE((warn_unused_result)) const lock_t *lock_rec_validate(
     uint64_t *limit) /*!< in/out: upper limit of
                      (space, page_no) */
 {
-  ut_ad(lock_mutex_own());
+  /* Actually we only require to latch the start-th shard, but we happen to
+  hold exclusive latch here, which is easier to assert */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   for (const lock_t *lock = static_cast<const lock_t *>(
@@ -5157,41 +5306,41 @@ static void lock_rec_block_validate(space_id_t space_id, page_no_t page_no) {
   }
 }
 
-/** Validates the lock system.
- @return true if ok */
-static bool lock_validate() {
+bool lock_validate() {
   typedef std::pair<space_id_t, page_no_t> page_addr_t;
   typedef std::set<page_addr_t, std::less<page_addr_t>,
                    ut_allocator<page_addr_t>>
       page_addr_set;
 
   page_addr_set pages;
+  {
+    /* lock_validate_table_locks() needs exclusive global latch, and we will
+    inspect record locks from all shards */
+    locksys::Global_exclusive_latch_guard guard{};
+    mutex_enter(&trx_sys->mutex);
 
-  lock_mutex_enter();
-  mutex_enter(&trx_sys->mutex);
-
-  ut_a(lock_validate_table_locks(&trx_sys->rw_trx_list));
+    ut_a(lock_validate_table_locks(&trx_sys->rw_trx_list));
 
-  /* Iterate over all the record locks and validate the locks. We
-  don't want to hog the lock_sys_t::mutex and the trx_sys_t::mutex.
-  Release both mutexes during the validation check. */
+    /* Iterate over all the record locks and validate the locks. We
+    don't want to hog the lock_sys global latch and the trx_sys_t::mutex.
+    Thus we release both latches before the validation check. */
 
-  for (ulint i = 0; i < hash_get_n_cells(lock_sys->rec_hash); i++) {
-    const lock_t *lock;
-    uint64_t limit = 0;
+    for (ulint i = 0; i < hash_get_n_cells(lock_sys->rec_hash); i++) {
+      const lock_t *lock;
+      uint64_t limit = 0;
 
-    while ((lock = lock_rec_validate(i, &limit)) != nullptr) {
-      page_no_t page_no;
-      space_id_t space = lock->rec_lock.space;
+      while ((lock = lock_rec_validate(i, &limit)) != nullptr) {
+        page_no_t page_no;
+        space_id_t space = lock->rec_lock.space;
 
-      page_no = lock->rec_lock.page_no;
+        page_no = lock->rec_lock.page_no;
 
-      pages.insert(std::make_pair(space, page_no));
+        pages.insert(std::make_pair(space, page_no));
+      }
     }
-  }
 
-  mutex_exit(&trx_sys->mutex);
-  lock_mutex_exit();
+    mutex_exit(&trx_sys->mutex);
+  }
 
   for (page_addr_set::const_iterator it = pages.begin(); it != pages.end();
        ++it) {
@@ -5232,78 +5381,58 @@ dberr_t lock_rec_insert_check_and_lock(
 
   ut_ad(!index->table->is_temporary());
 
-  dberr_t err;
+  dberr_t err = DB_SUCCESS;
   lock_t *lock;
   ibool inherit_in = *inherit;
   trx_t *trx = thr_get_trx(thr);
   const rec_t *next_rec = page_rec_get_next_const(rec);
   ulint heap_no = page_rec_get_heap_no(next_rec);
 
-  lock_mutex_enter();
-  /* Because this code is invoked for a running transaction by
-  the thread that is serving the transaction, it is not necessary
-  to hold trx->mutex here. */
-
-  /* When inserting a record into an index, the table must be at
-  least IX-locked. When we are building an index, we would pass
-  BTR_NO_LOCKING_FLAG and skip the locking altogether. */
-  ut_ad(lock_table_has(trx, index->table, LOCK_IX));
-
-  lock = lock_rec_get_first(lock_sys->rec_hash, block, heap_no);
-
-  if (lock == nullptr) {
-    /* We optimize CPU time usage in the simplest case */
-
-    lock_mutex_exit();
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-    if (inherit_in && !index->is_clustered()) {
-      /* Update the page max trx id field */
-      page_update_max_trx_id(block, buf_block_get_page_zip(block), trx->id,
-                             mtr);
-    }
+    /* When inserting a record into an index, the table must be at
+    least IX-locked. When we are building an index, we would pass
+    BTR_NO_LOCKING_FLAG and skip the locking altogether. */
+    ut_ad(lock_table_has(trx, index->table, LOCK_IX));
 
-    *inherit = false;
+    /* Spatial index does not use GAP lock protection. It uses
+    "predicate lock" to protect the "range" */
+    ut_ad(!dict_index_is_spatial(index));
 
-    return (DB_SUCCESS);
-  }
+    lock = lock_rec_get_first(lock_sys->rec_hash, block, heap_no);
 
-  /* Spatial index does not use GAP lock protection. It uses
-  "predicate lock" to protect the "range" */
-  if (dict_index_is_spatial(index)) {
-    return (DB_SUCCESS);
-  }
-
-  *inherit = true;
+    if (lock == nullptr) {
+      *inherit = false;
+    } else {
+      *inherit = true;
 
-  /* If another transaction has an explicit lock request which locks
-  the gap, waiting or granted, on the successor, the insert has to wait.
+      /* If another transaction has an explicit lock request which locks
+      the gap, waiting or granted, on the successor, the insert has to wait.
 
-  An exception is the case where the lock by the another transaction
-  is a gap type lock which it placed to wait for its turn to insert. We
-  do not consider that kind of a lock conflicting with our insert. This
-  eliminates an unnecessary deadlock which resulted when 2 transactions
-  had to wait for their insert. Both had waiting gap type lock requests
-  on the successor, which produced an unnecessary deadlock. */
+      An exception is the case where the lock by the another transaction
+      is a gap type lock which it placed to wait for its turn to insert. We
+      do not consider that kind of a lock conflicting with our insert. This
+      eliminates an unnecessary deadlock which resulted when 2 transactions
+      had to wait for their insert. Both had waiting gap type lock requests
+      on the successor, which produced an unnecessary deadlock. */
 
-  const ulint type_mode = LOCK_X | LOCK_GAP | LOCK_INSERT_INTENTION;
+      const ulint type_mode = LOCK_X | LOCK_GAP | LOCK_INSERT_INTENTION;
 
-  const lock_t *wait_for =
-      lock_rec_other_has_conflicting(type_mode, block, heap_no, trx);
+      const lock_t *wait_for =
+          lock_rec_other_has_conflicting(type_mode, block, heap_no, trx);
 
-  if (wait_for != nullptr) {
-    RecLock rec_lock(thr, index, block, heap_no, type_mode);
+      if (wait_for != nullptr) {
+        RecLock rec_lock(thr, index, block, heap_no, type_mode);
 
-    trx_mutex_enter(trx);
+        trx_mutex_enter(trx);
 
-    err = rec_lock.add_to_waitq(wait_for);
+        err = rec_lock.add_to_waitq(wait_for);
 
-    trx_mutex_exit(trx);
-
-  } else {
-    err = DB_SUCCESS;
-  }
-
-  lock_mutex_exit();
+        trx_mutex_exit(trx);
+      }
+    }
+  } /* Shard_latch_guard */
 
   switch (err) {
     case DB_SUCCESS_LOCKED_REC:
@@ -5322,24 +5451,8 @@ dberr_t lock_rec_insert_check_and_lock(
       break;
   }
 
-#ifdef UNIV_DEBUG
-  {
-    mem_heap_t *heap = nullptr;
-    ulint offsets_[REC_OFFS_NORMAL_SIZE];
-    const ulint *offsets;
-    rec_offs_init(offsets_);
-
-    offsets =
-        rec_get_offsets(next_rec, index, offsets_, ULINT_UNDEFINED, &heap);
-
-    ut_ad(lock_rec_queue_validate(false, block, next_rec, index, offsets));
-
-    if (heap != nullptr) {
-      mem_heap_free(heap);
-    }
-  }
+  ut_d(locksys::rec_queue_latch_and_validate(block, next_rec, index));
   ut_ad(err == DB_SUCCESS || err == DB_LOCK_WAIT || err == DB_DEADLOCK);
-#endif /* UNIV_DEBUG */
 
   return (err);
 }
@@ -5359,55 +5472,45 @@ static void lock_rec_convert_impl_to_expl_for_trx(
   ut_ad(trx_is_referenced(trx));
 
   DEBUG_SYNC_C("before_lock_rec_convert_impl_to_expl_for_trx");
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
+    /* This trx->mutex acquisition here is not really needed.
+    Its purpose is to prevent a state transition between calls to trx_state_eq()
+    and lock_rec_add_to_queue().
+    But one can prove, that even if the state did change, it is not
+    a big problem, because we still keep reference count from dropping
+    to zero, so the trx object is still in use, and we hold the shard latched,
+    so trx can not release its explicit lock (if it has any) so we will
+    notice the explicit lock in lock_rec_has_expl.
+    On the other hand if trx does not have explicit lock, then we would create
+    one on its behalf, which is wasteful, but does not cause a problem, as once
+    the reference count drops to zero the trx will notice and remove this new
+    explicit lock. Also, even if some other trx had observed that trx is already
+    removed from rw trxs list and thus ignored the implicit lock and decided to
+    add its own lock, it will still have to wait for shard latch before adding
+    her lock. However it does not cost us much to simply take the trx->mutex
+    and avoid this whole shaky reasoning. */
+    trx_mutex_enter(trx);
 
-  lock_mutex_enter();
-  /* This trx->mutex acquisition here is not really needed.
-  Its purpose is to prevent a state transition between calls to trx_state_eq()
-  and lock_rec_add_to_queue().
-  But one can prove, that even if the state did change, it is not
-  a big problem, because we still keep reference count from dropping
-  to zero, so the trx object is still in use, and we hold the lock mutex
-  so trx can not release its explicit lock (if it has any) so we will
-  notice the explicit lock in lock_rec_has_expl.
-  On the other hand if trx does not have explicit lock, then we would create one
-  on its behalf, which is wasteful, but does not cause a problem, as once the
-  reference count drops to zero the trx will notice and remove this new explicit
-  lock.
-  Also, even if some other trx had observed that trx is already removed from
-  rw trxs list and thus ignored the implicit lock and decided to add its own
-  lock, it will still have to wait for lock_mutex before adding her lock.
-  However it does not cost us much to simply take the trx->mutex
-  and avoid this whole shaky reasoning. */
-  trx_mutex_enter(trx);
+    ut_ad(!index->is_clustered() ||
+          trx->id ==
+              lock_clust_rec_some_has_impl(
+                  rec, index,
+                  offsets ? offsets : Rec_offsets().compute(rec, index)));
 
-#ifdef UNIV_DEBUG
-  if (index->is_clustered()) {
-    mem_heap_t *heap = nullptr;
-    ulint offsets_[REC_OFFS_NORMAL_SIZE];
-    if (!offsets) {
-      rec_offs_init(offsets_);
-      offsets = rec_get_offsets(rec, index, offsets_, ULINT_UNDEFINED, &heap);
-    }
-    auto implicit_owner_id = lock_clust_rec_some_has_impl(rec, index, offsets);
-    ut_a(implicit_owner_id == trx->id);
-    if (heap != nullptr) {
-      mem_heap_free(heap);
-    }
-  }
-#endif
-  ut_ad(!trx_state_eq(trx, TRX_STATE_NOT_STARTED));
+    ut_ad(!trx_state_eq(trx, TRX_STATE_NOT_STARTED));
 
-  if (!trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY) &&
-      !lock_rec_has_expl(LOCK_X | LOCK_REC_NOT_GAP, block, heap_no, trx)) {
-    ulint type_mode;
+    if (!trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY) &&
+        !lock_rec_has_expl(LOCK_X | LOCK_REC_NOT_GAP, block, heap_no, trx)) {
+      ulint type_mode;
 
-    type_mode = (LOCK_REC | LOCK_X | LOCK_REC_NOT_GAP);
+      type_mode = (LOCK_REC | LOCK_X | LOCK_REC_NOT_GAP);
 
-    lock_rec_add_to_queue(type_mode, block, heap_no, index, trx, true);
-  }
+      lock_rec_add_to_queue(type_mode, block, heap_no, index, trx, true);
+    }
 
-  lock_mutex_exit();
-  trx_mutex_exit(trx);
+    trx_mutex_exit(trx);
+  }
 
   trx_release_reference(trx);
 
@@ -5425,7 +5528,7 @@ static void lock_rec_convert_impl_to_expl(const buf_block_t *block,
                                           const ulint *offsets) {
   trx_t *trx;
 
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   ut_ad(page_rec_is_user_rec(rec));
   ut_ad(rec_offs_validate(rec, index, offsets));
   ut_ad(!page_rec_is_comp(rec) == !rec_offs_comp(offsets));
@@ -5509,18 +5612,17 @@ dberr_t lock_clust_rec_modify_check_and_lock(
 
   lock_rec_convert_impl_to_expl(block, rec, index, offsets);
 
-  lock_mutex_enter();
-
-  ut_ad(lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
-
-  err = lock_rec_lock(true, SELECT_ORDINARY, LOCK_X | LOCK_REC_NOT_GAP, block,
-                      heap_no, index, thr);
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
+    ut_ad(lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
 
-  MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+    err = lock_rec_lock(true, SELECT_ORDINARY, LOCK_X | LOCK_REC_NOT_GAP, block,
+                        heap_no, index, thr);
 
-  lock_mutex_exit();
+    MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+  }
 
-  ut_ad(lock_rec_queue_validate(false, block, rec, index, offsets));
+  ut_d(locksys::rec_queue_latch_and_validate(block, rec, index, offsets));
 
   if (err == DB_SUCCESS_LOCKED_REC) {
     err = DB_SUCCESS;
@@ -5564,34 +5666,18 @@ dberr_t lock_sec_rec_modify_check_and_lock(
   because when we come here, we already have modified the clustered
   index record, and this would not have been possible if another active
   transaction had modified this secondary index record. */
-
-  lock_mutex_enter();
-
-  ut_ad(lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
-
-  err = lock_rec_lock(true, SELECT_ORDINARY, LOCK_X | LOCK_REC_NOT_GAP, block,
-                      heap_no, index, thr);
-
-  MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
-
-  lock_mutex_exit();
-
-#ifdef UNIV_DEBUG
   {
-    mem_heap_t *heap = nullptr;
-    ulint offsets_[REC_OFFS_NORMAL_SIZE];
-    const ulint *offsets;
-    rec_offs_init(offsets_);
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-    offsets = rec_get_offsets(rec, index, offsets_, ULINT_UNDEFINED, &heap);
+    ut_ad(lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
 
-    ut_ad(lock_rec_queue_validate(false, block, rec, index, offsets));
+    err = lock_rec_lock(true, SELECT_ORDINARY, LOCK_X | LOCK_REC_NOT_GAP, block,
+                        heap_no, index, thr);
 
-    if (heap != nullptr) {
-      mem_heap_free(heap);
-    }
+    MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
   }
-#endif /* UNIV_DEBUG */
+
+  ut_d(locksys::rec_queue_latch_and_validate(block, rec, index));
 
   if (err == DB_SUCCESS || err == DB_SUCCESS_LOCKED_REC) {
     /* Update the page max trx id field */
@@ -5635,27 +5721,26 @@ dberr_t lock_sec_rec_read_check_and_lock(
       !page_rec_is_supremum(rec)) {
     lock_rec_convert_impl_to_expl(block, rec, index, offsets);
   }
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-  lock_mutex_enter();
-
-  if (duration == lock_duration_t::AT_LEAST_STATEMENT) {
-    lock_protect_locks_till_statement_end(thr);
-  }
-
-  ut_ad(mode != LOCK_X ||
-        lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
-  ut_ad(mode != LOCK_S ||
-        lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
+    if (duration == lock_duration_t::AT_LEAST_STATEMENT) {
+      lock_protect_locks_till_statement_end(thr);
+    }
 
-  err = lock_rec_lock(false, sel_mode, mode | gap_mode, block, heap_no, index,
-                      thr);
+    ut_ad(mode != LOCK_X ||
+          lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
+    ut_ad(mode != LOCK_S ||
+          lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
 
-  MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+    err = lock_rec_lock(false, sel_mode, mode | gap_mode, block, heap_no, index,
+                        thr);
 
-  lock_mutex_exit();
+    MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+  }
   DEBUG_SYNC_C("lock_sec_rec_read_check_and_lock_has_locked");
 
-  ut_ad(lock_rec_queue_validate(false, block, rec, index, offsets));
+  ut_d(locksys::rec_queue_latch_and_validate(block, rec, index, offsets));
   ut_ad(err == DB_SUCCESS || err == DB_SUCCESS_LOCKED_REC ||
         err == DB_LOCK_WAIT || err == DB_DEADLOCK || err == DB_SKIP_LOCKED ||
         err == DB_LOCK_NOWAIT);
@@ -5687,25 +5772,25 @@ dberr_t lock_clust_rec_read_check_and_lock(
   }
 
   DEBUG_SYNC_C("after_lock_clust_rec_read_check_and_lock_impl_to_expl");
-  lock_mutex_enter();
-
-  if (duration == lock_duration_t::AT_LEAST_STATEMENT) {
-    lock_protect_locks_till_statement_end(thr);
-  }
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-  ut_ad(mode != LOCK_X ||
-        lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
-  ut_ad(mode != LOCK_S ||
-        lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
+    if (duration == lock_duration_t::AT_LEAST_STATEMENT) {
+      lock_protect_locks_till_statement_end(thr);
+    }
 
-  err = lock_rec_lock(false, sel_mode, mode | gap_mode, block, heap_no, index,
-                      thr);
+    ut_ad(mode != LOCK_X ||
+          lock_table_has(thr_get_trx(thr), index->table, LOCK_IX));
+    ut_ad(mode != LOCK_S ||
+          lock_table_has(thr_get_trx(thr), index->table, LOCK_IS));
 
-  MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+    err = lock_rec_lock(false, sel_mode, mode | gap_mode, block, heap_no, index,
+                        thr);
 
-  lock_mutex_exit();
+    MONITOR_INC(MONITOR_NUM_RECLOCK_REQ);
+  }
 
-  ut_ad(lock_rec_queue_validate(false, block, rec, index, offsets));
+  ut_d(locksys::rec_queue_latch_and_validate(block, rec, index, offsets));
 
   DEBUG_SYNC_C("after_lock_clust_rec_read_check_and_lock");
   ut_ad(err == DB_SUCCESS || err == DB_SUCCESS_LOCKED_REC ||
@@ -5738,19 +5823,9 @@ dberr_t lock_clust_rec_read_check_and_lock_alt(
                              LOCK_REC_NOT_GAP */
     que_thr_t *thr)           /*!< in: query thread */
 {
-  mem_heap_t *tmp_heap = nullptr;
-  ulint offsets_[REC_OFFS_NORMAL_SIZE];
-  ulint *offsets = offsets_;
-  dberr_t err;
-  rec_offs_init(offsets_);
-
-  offsets = rec_get_offsets(rec, index, offsets, ULINT_UNDEFINED, &tmp_heap);
-  err = lock_clust_rec_read_check_and_lock(lock_duration_t::REGULAR, block, rec,
-                                           index, offsets, SELECT_ORDINARY,
-                                           mode, gap_mode, thr);
-  if (tmp_heap) {
-    mem_heap_free(tmp_heap);
-  }
+  dberr_t err = lock_clust_rec_read_check_and_lock(
+      lock_duration_t::REGULAR, block, rec, index,
+      Rec_offsets().compute(rec, index), SELECT_ORDINARY, mode, gap_mode, thr);
 
   if (err == DB_SUCCESS_LOCKED_REC) {
     err = DB_SUCCESS;
@@ -5770,7 +5845,10 @@ void lock_release_autoinc_last_lock(trx_t *trx) {
   ut_ad(trx_mutex_own(trx));
   ib_vector_t *autoinc_locks = trx->lock.autoinc_locks;
 
-  ut_ad(lock_mutex_own());
+  /* Since we do not know for which table the trx has created the last lock
+  we can not narrow the required latch to any particular shard, and thus we
+  require exclusive access to lock_sys here */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_a(!ib_vector_is_empty(autoinc_locks));
 
   /* The lock to be release must be the last lock acquired. */
@@ -5805,7 +5883,10 @@ static bool lock_trx_holds_autoinc_locks(
 /** Release all the transaction's autoinc locks. */
 static void lock_release_autoinc_locks(trx_t *trx) /*!< in/out: transaction */
 {
-  ut_ad(lock_mutex_own());
+  /* Since we do not know for which table(s) the trx has created the lock(s)
+  we can not narrow the required latch to any particular shard, and thus we
+  require exclusive access to lock_sys here */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_mutex_own(trx));
 
   ut_a(trx->lock.autoinc_locks != nullptr);
@@ -5865,9 +5946,10 @@ void lock_get_psi_event(const lock_t *lock, ulonglong *thread_id,
 @return The first lock
 */
 const lock_t *lock_get_first_trx_locks(const trx_lock_t *trx_lock) {
-  /* trx->lock.trx_locks is protected by trx->mutex and lock_sys mutex, and we
-  assume we have the exclusive latch on lock_sys here */
-  ut_ad(lock_mutex_own());
+  /* Writes to trx->lock.trx_locks are protected by trx->mutex combined with a
+  shared global lock_sys latch, and we assume we have the exclusive latch on
+  lock_sys here */
+  ut_ad(locksys::owns_exclusive_global_latch());
   const lock_t *result = UT_LIST_GET_FIRST(trx_lock->trx_locks);
   return (result);
 }
@@ -5877,9 +5959,10 @@ const lock_t *lock_get_first_trx_locks(const trx_lock_t *trx_lock) {
 @return The next lock
 */
 const lock_t *lock_get_next_trx_locks(const lock_t *lock) {
-  /* trx->lock.trx_locks is protected by trx->mutex and lock_sys mutex, and we
-  assume we have the exclusive latch on lock_sys here */
-  ut_ad(lock_mutex_own());
+  /* Writes to trx->lock.trx_locks are protected by trx->mutex combined with a
+  shared global lock_sys latch, and we assume we have the exclusive latch on
+  lock_sys here */
+  ut_ad(locksys::owns_exclusive_global_latch());
   const lock_t *result = UT_LIST_GET_NEXT(trx_locks, lock);
   return (result);
 }
@@ -5899,10 +5982,9 @@ const lock_t *lock_get_next_trx_locks(const lock_t *lock) {
  @return lock mode */
 const char *lock_get_mode_str(const lock_t *lock) /*!< in: lock */
 {
-  /* We might need to modify lock_cached_lock_mode_names, so we need exclusive
-  access. Thankfully lock_get_mode_str is used only while holding the
-  lock_sys->mutex so we don't need dedicated mutex */
-  ut_ad(lock_mutex_own());
+  /* We use exclusive global lock_sys latch to protect the global
+  lock_cached_lock_mode_names mapping. */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   const auto type_mode = lock->type_mode;
   const auto mode = lock->mode();
@@ -6044,7 +6126,24 @@ page_no_t lock_rec_get_page_no(const lock_t *lock) /*!< in: lock */
 waiting behind it.
 @param[in,out]	lock		Waiting lock request */
 void lock_cancel_waiting_and_release(lock_t *lock) {
-  ut_ad(lock_mutex_own());
+  /* Requiring exclusive global latch serves several purposes here.
+
+  1. In case of table LOCK_TABLE we will call lock_release_autoinc_locks(),
+  which iterates over locks held by this transaction and it is not clear if
+  these locks are from the same table. Frankly it is not clear why we even
+  release all of them here (note that none of them is our `lock` because we
+  don't store waiting locks in the trx->autoinc_locks vector, only granted).
+  Perhaps this is because this trx is going to be rolled back anyway, and this
+  seemed to be a good moment to release them?
+
+  2. During lock_rec_dequeue_from_page() and lock_table_dequeue() we might latch
+  trx mutex of another transaction to grant it a lock. The rules meant to avoid
+  deadlocks between trx mutex require us to either use an exclusive global
+  latch, or to first latch trx which is has trx->lock.wait_lock == nullptr.
+  As `lock == lock->trx->lock.wait_lock` and thus is not nullptr, we have to use
+  the first approach, or complicate the proof of deadlock avoidance enormously.
+  */
+  ut_ad(locksys::owns_exclusive_global_latch());
   /* We will access lock->trx->lock.autoinc_locks which requires trx->mutex */
   ut_ad(trx_mutex_own(lock->trx));
 
@@ -6054,7 +6153,6 @@ void lock_cancel_waiting_and_release(lock_t *lock) {
     ut_ad(lock_get_type_low(lock) & LOCK_TABLE);
 
     if (lock->trx->lock.autoinc_locks != nullptr) {
-      /* Release the transaction's AUTOINC locks. */
       lock_release_autoinc_locks(lock->trx);
     }
 
@@ -6069,9 +6167,8 @@ void lock_cancel_waiting_and_release(lock_t *lock) {
  connection thread that owns the transaction (trx->mysql_thd). */
 void lock_unlock_table_autoinc(trx_t *trx) /*!< in/out: transaction */
 {
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   ut_ad(!trx_mutex_own(trx));
-  ut_ad(!trx->lock.wait_lock);
 
   /* This can be invoked on NOT_STARTED, ACTIVE, PREPARED,
   but not COMMITTED transactions. */
@@ -6104,14 +6201,17 @@ void lock_unlock_table_autoinc(trx_t *trx) /*!< in/out: transaction */
     2. trx->mutex is cheap
   */
   trx_mutex_enter(trx);
+  ut_ad(!trx->lock.wait_lock);
   bool might_have_autoinc_locks = lock_trx_holds_autoinc_locks(trx);
   trx_mutex_exit(trx);
 
   if (might_have_autoinc_locks) {
-    lock_mutex_enter();
+    /* lock_release_autoinc_locks() requires exclusive global latch as the
+    AUTOINC locks might be on tables from different shards. Identifying and
+    latching them in correct order would complicate this rarely-taken path. */
+    locksys::Global_exclusive_latch_guard guard{};
     trx_mutex_enter(trx);
     lock_release_autoinc_locks(trx);
-    lock_mutex_exit();
     trx_mutex_exit(trx);
   }
 }
@@ -6195,7 +6295,10 @@ dberr_t lock_trx_handle_wait(trx_t *trx) /*!< in/out: trx lock state */
 {
   dberr_t err;
 
-  lock_mutex_enter();
+  /* lock_cancel_waiting_and_release() requires exclusive global latch, and so
+  does reading the trx->lock.wait_lock to prevent races with B-tree page
+  reorganization */
+  locksys::Global_exclusive_latch_guard guard{};
 
   trx_mutex_enter(trx);
 
@@ -6209,7 +6312,6 @@ dberr_t lock_trx_handle_wait(trx_t *trx) /*!< in/out: trx lock state */
     err = DB_SUCCESS;
   }
 
-  lock_mutex_exit();
   trx_mutex_exit(trx);
 
   return (err);
@@ -6228,7 +6330,11 @@ static const lock_t *lock_table_locks_lookup(
   const trx_t *trx;
 
   ut_a(table != nullptr);
-  ut_ad(lock_mutex_own());
+  /* We are going to iterate over multiple transactions, so even though we know
+  which table we are looking for we can not narrow required latch to just the
+  shard which contains the table, because accessing trx->lock.trx_locks would be
+  unsafe */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
@@ -6257,18 +6363,18 @@ static const lock_t *lock_table_locks_lookup(
 }
 #endif /* UNIV_DEBUG */
 
-/** Check if there are any locks (table or rec) against table.
- @return true if table has either table or record locks. */
-bool lock_table_has_locks(
-    const dict_table_t *table) /*!< in: check if there are any locks
-                               held on records in this table or on the
-                               table itself */
-{
-  ibool has_locks;
+bool lock_table_has_locks(const dict_table_t *table) {
+  /** The n_rec_locks field might be modified by operation on any page shard,
+  so we need to latch everything. Note, that the results of this function will
+  be obsolete, as soon as we release the latch. It is called in contexts where
+  we believe that the number of locks should either be zero or decreasing. For
+  such scenario of usage, we might perhaps read the n_rec_locks without latch
+  and restrict latch just to a table shard. But that would complicate the debug
+  version of the code for no significant gain as this is not a hot path. */
+  locksys::Global_exclusive_latch_guard guard{};
 
-  lock_mutex_enter();
-
-  has_locks = UT_LIST_GET_LEN(table->locks) > 0 || table->n_rec_locks > 0;
+  bool has_locks =
+      UT_LIST_GET_LEN(table->locks) > 0 || table->n_rec_locks.load() > 0;
 
 #ifdef UNIV_DEBUG
   if (!has_locks) {
@@ -6280,8 +6386,6 @@ bool lock_table_has_locks(
   }
 #endif /* UNIV_DEBUG */
 
-  lock_mutex_exit();
-
   return (has_locks);
 }
 
@@ -6309,11 +6413,10 @@ bool lock_trx_has_rec_x_lock(que_thr_t *thr, const dict_table_t *table,
   ut_ad(heap_no > PAGE_HEAP_NO_SUPREMUM);
 
   const trx_t *trx = thr_get_trx(thr);
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
   ut_a(lock_table_has(trx, table, LOCK_IX) || table->is_temporary());
   ut_a(lock_rec_has_expl(LOCK_X | LOCK_REC_NOT_GAP, block, heap_no, trx) ||
        table->is_temporary());
-  lock_mutex_exit();
   return (true);
 }
 #endif /* UNIV_DEBUG */
@@ -6324,7 +6427,7 @@ is enabled. */
 void Deadlock_notifier::start_print() {
   /* I/O operations on lock_latest_err_file require exclusive latch on
   lock_sys */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   rewind(lock_latest_err_file);
   ut_print_timestamp(lock_latest_err_file);
@@ -6340,7 +6443,7 @@ void Deadlock_notifier::start_print() {
 void Deadlock_notifier::print(const char *msg) {
   /* I/O operations on lock_latest_err_file require exclusive latch on
   lock_sys */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
   fputs(msg, lock_latest_err_file);
 
   if (srv_print_all_deadlocks) {
@@ -6357,7 +6460,7 @@ void Deadlock_notifier::print(const trx_t *trx, ulint max_query_len) {
     2. lock_number_of_rows_locked()
     3. Accessing trx->lock fields requires either holding trx->mutex or latching
     the lock sys. */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   trx_mutex_enter(trx);
   ulint n_rec_locks = lock_number_of_rows_locked(&trx->lock);
@@ -6383,7 +6486,7 @@ void Deadlock_notifier::print(const trx_t *trx, ulint max_query_len) {
 void Deadlock_notifier::print(const lock_t *lock) {
   /* I/O operations on lock_latest_err_file require exclusive latch on
   lock_sys. */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   if (lock_get_type_low(lock) == LOCK_REC) {
     lock_rec_print(lock_latest_err_file, lock);
@@ -6403,7 +6506,7 @@ void Deadlock_notifier::print(const lock_t *lock) {
 void Deadlock_notifier::print_title(size_t pos_on_cycle, const char *title) {
   /* I/O operations on lock_latest_err_file require exclusive latch on
   lock_sys */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut::ostringstream buff;
   buff << "\n*** (" << (pos_on_cycle + 1) << ") " << title << ":\n";
   print(buff.str().c_str());
@@ -6411,7 +6514,7 @@ void Deadlock_notifier::print_title(size_t pos_on_cycle, const char *title) {
 
 void Deadlock_notifier::notify(const ut::vector<const trx_t *> &trxs_on_cycle,
                                const trx_t *victim_trx) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   start_print();
   const auto n = trxs_on_cycle.size();
diff --git a/storage/innobase/lock/lock0prdt.cc b/storage/innobase/lock/lock0prdt.cc
index 684bfead787..9d0700c1cbf 100644
--- a/storage/innobase/lock/lock0prdt.cc
+++ b/storage/innobase/lock/lock0prdt.cc
@@ -223,7 +223,7 @@ lock_t *lock_prdt_has_lock(ulint precise_mode, /*!< in: LOCK_S or LOCK_X */
 {
   lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad((precise_mode & LOCK_MODE_MASK) == LOCK_S ||
         (precise_mode & LOCK_MODE_MASK) == LOCK_X);
   ut_ad(!(precise_mode & LOCK_INSERT_INTENTION));
@@ -271,7 +271,7 @@ static const lock_t *lock_prdt_other_has_conflicting(
                              the new lock will be on */
     const trx_t *trx)         /*!< in: our transaction */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   for (const lock_t *lock =
            lock_rec_get_first(lock_hash_get(mode), block, PRDT_HEAPNO);
@@ -350,7 +350,7 @@ static lock_t *lock_prdt_find_on_page(
 {
   lock_t *lock;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   for (lock = lock_rec_get_first_on_page(lock_hash_get(type_mode), block);
        lock != nullptr; lock = lock_rec_get_next_on_page(lock)) {
@@ -384,7 +384,7 @@ static lock_t *lock_prdt_add_to_queue(
     lock_prdt_t *prdt)        /*!< in: Minimum Bounding Rectangle
                               the new lock will be on */
 {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
   ut_ad(!index->is_clustered() && !dict_index_is_online_ddl(index));
   ut_ad(type_mode & (LOCK_PREDICATE | LOCK_PRDT_PAGE));
   ut_ad(!trx_mutex_own(trx));
@@ -451,65 +451,54 @@ dberr_t lock_prdt_insert_check_and_lock(
 
   trx_t *trx = thr_get_trx(thr);
 
-  lock_mutex_enter();
-
-  /* Because this code is invoked for a running transaction by
-  the thread that is serving the transaction, it is not necessary
-  to hold trx->mutex here. */
-
-  ut_ad(lock_table_has(trx, index->table, LOCK_IX));
-
-  lock_t *lock;
-
-  /* Only need to check locks on prdt_hash */
-  lock = lock_rec_get_first(lock_sys->prdt_hash, block, PRDT_HEAPNO);
+  dberr_t err = DB_SUCCESS;
+  {
+    locksys::Shard_latch_guard guard{block->get_page_id()};
 
-  if (lock == nullptr) {
-    lock_mutex_exit();
+    /* Because this code is invoked for a running transaction by
+    the thread that is serving the transaction, it is not necessary
+    to hold trx->mutex here. */
 
-    /* Update the page max trx id field */
-    page_update_max_trx_id(block, buf_block_get_page_zip(block), trx->id, mtr);
+    ut_ad(lock_table_has(trx, index->table, LOCK_IX));
 
-    return (DB_SUCCESS);
-  }
+    lock_t *lock;
 
-  ut_ad(lock->type_mode & LOCK_PREDICATE);
+    /* Only need to check locks on prdt_hash */
+    lock = lock_rec_get_first(lock_sys->prdt_hash, block, PRDT_HEAPNO);
 
-  dberr_t err;
+    if (lock != nullptr) {
+      ut_ad(lock->type_mode & LOCK_PREDICATE);
 
-  /* If another transaction has an explicit lock request which locks
-  the predicate, waiting or granted, on the successor, the insert
-  has to wait.
+      /* If another transaction has an explicit lock request which locks
+      the predicate, waiting or granted, on the successor, the insert
+      has to wait.
 
-  Similar to GAP lock, we do not consider lock from inserts conflicts
-  with each other */
+      Similar to GAP lock, we do not consider lock from inserts conflicts
+      with each other */
 
-  const ulint mode = LOCK_X | LOCK_PREDICATE | LOCK_INSERT_INTENTION;
+      const ulint mode = LOCK_X | LOCK_PREDICATE | LOCK_INSERT_INTENTION;
 
-  const lock_t *wait_for =
-      lock_prdt_other_has_conflicting(mode, block, prdt, trx);
+      const lock_t *wait_for =
+          lock_prdt_other_has_conflicting(mode, block, prdt, trx);
 
-  if (wait_for != nullptr) {
-    rtr_mbr_t *mbr = prdt_get_mbr_from_prdt(prdt);
+      if (wait_for != nullptr) {
+        rtr_mbr_t *mbr = prdt_get_mbr_from_prdt(prdt);
 
-    trx_mutex_enter(trx);
+        trx_mutex_enter(trx);
 
-    /* Allocate MBR on the lock heap */
-    lock_init_prdt_from_mbr(prdt, mbr, 0, trx->lock.lock_heap);
+        /* Allocate MBR on the lock heap */
+        lock_init_prdt_from_mbr(prdt, mbr, 0, trx->lock.lock_heap);
 
-    RecLock rec_lock(thr, index, block, PRDT_HEAPNO, mode);
+        RecLock rec_lock(thr, index, block, PRDT_HEAPNO, mode);
 
-    /* Note that we may get DB_SUCCESS also here! */
+        /* Note that we may get DB_SUCCESS also here! */
 
-    err = rec_lock.add_to_waitq(wait_for, prdt);
+        err = rec_lock.add_to_waitq(wait_for, prdt);
 
-    trx_mutex_exit(trx);
-
-  } else {
-    err = DB_SUCCESS;
-  }
-
-  lock_mutex_exit();
+        trx_mutex_exit(trx);
+      }
+    }
+  }  // release block latch
 
   switch (err) {
     case DB_SUCCESS_LOCKED_REC:
@@ -540,7 +529,9 @@ void lock_prdt_update_parent(
 {
   lock_t *lock;
 
-  lock_mutex_enter();
+  /* We will operate on three blocks (left, right, parent). Latching their
+  shards without deadlock is easiest using exlusive global latch. */
+  locksys::Global_exclusive_latch_guard guard{};
 
   /* Get all locks in parent */
   for (lock =
@@ -576,8 +567,6 @@ void lock_prdt_update_parent(
                              lock->trx, lock_prdt);
     }
   }
-
-  lock_mutex_exit();
 }
 
 /** Update predicate lock when page splits */
@@ -593,8 +582,7 @@ static void lock_prdt_update_split_low(
 {
   lock_t *lock;
 
-  lock_mutex_enter();
-
+  locksys::Shard_latches_guard guard{*block, *new_block};
   for (lock = lock_rec_get_first_on_page_addr(lock_hash_get(type_mode), space,
                                               page_no);
        lock; lock = lock_rec_get_next_on_page(lock)) {
@@ -639,8 +627,6 @@ static void lock_prdt_update_split_low(
                              lock_prdt);
     }
   }
-
-  lock_mutex_exit();
 }
 
 /** Update predicate lock when page splits */
@@ -715,7 +701,7 @@ dberr_t lock_prdt_lock(buf_block_t *block,  /*!< in/out: buffer block of rec */
   index record, and this would not have been possible if another active
   transaction had modified this secondary index record. */
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{block->get_page_id()};
 
   const ulint prdt_mode = mode | type_mode;
   lock_t *lock = lock_rec_get_first_on_page(hash, block);
@@ -765,8 +751,6 @@ dberr_t lock_prdt_lock(buf_block_t *block,  /*!< in/out: buffer block of rec */
     }
   }
 
-  lock_mutex_exit();
-
   if (status == LOCK_REC_SUCCESS_CREATED && type_mode == LOCK_PREDICATE) {
     /* Append the predicate in the lock record */
     lock_prdt_set_prdt(lock, prdt);
@@ -796,7 +780,8 @@ dberr_t lock_place_prdt_page_lock(
   index record, and this would not have been possible if another active
   transaction had modified this secondary index record. */
 
-  lock_mutex_enter();
+  RecID rec_id(page_id_t{space, page_no}, PRDT_HEAPNO);
+  locksys::Shard_latch_guard guard{rec_id.get_page_id()};
 
   const lock_t *lock =
       lock_rec_get_first_on_page_addr(lock_sys->prdt_page_hash, space, page_no);
@@ -820,7 +805,6 @@ dberr_t lock_place_prdt_page_lock(
   }
 
   if (lock == nullptr) {
-    RecID rec_id(space, page_no, PRDT_HEAPNO);
     RecLock rec_lock(index, rec_id, mode);
 
     trx_mutex_enter(trx);
@@ -832,8 +816,6 @@ dberr_t lock_place_prdt_page_lock(
 #endif /* PRDT_DIAG */
   }
 
-  lock_mutex_exit();
-
   return (DB_SUCCESS);
 }
 
@@ -847,13 +829,11 @@ bool lock_test_prdt_page_lock(const trx_t *trx, space_id_t space,
                               page_no_t page_no) {
   lock_t *lock;
 
-  lock_mutex_enter();
+  locksys::Shard_latch_guard guard{page_id_t{space, page_no}};
 
   lock =
       lock_rec_get_first_on_page_addr(lock_sys->prdt_page_hash, space, page_no);
 
-  lock_mutex_exit();
-
   return (lock == nullptr || trx == lock->trx);
 }
 
@@ -871,7 +851,7 @@ void lock_prdt_rec_move(
     return;
   }
 
-  lock_mutex_enter();
+  locksys::Shard_latches_guard guard{*receiver, *donator};
 
   for (lock = lock_rec_get_first(lock_sys->prdt_hash, donator, PRDT_HEAPNO);
        lock != nullptr; lock = lock_rec_get_next(PRDT_HEAPNO, lock)) {
@@ -883,8 +863,6 @@ void lock_prdt_rec_move(
     lock_prdt_add_to_queue(type_mode, receiver, lock->index, lock->trx,
                            lock_prdt);
   }
-
-  lock_mutex_exit();
 }
 
 /** Removes predicate lock objects set on an index page which is discarded.
@@ -897,7 +875,7 @@ void lock_prdt_page_free_from_discard(const buf_block_t *block,
   space_id_t space;
   page_no_t page_no;
 
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_page_shard(block->get_page_id()));
 
   space = block->page.id.space();
   page_no = block->page.id.page_no();
diff --git a/storage/innobase/lock/lock0wait.cc b/storage/innobase/lock/lock0wait.cc
index 40e5f9ccbb9..ba17c6242f4 100644
--- a/storage/innobase/lock/lock0wait.cc
+++ b/storage/innobase/lock/lock0wait.cc
@@ -72,10 +72,10 @@ static void lock_wait_table_release_slot(
 #endif /* UNIV_DEBUG */
 
   lock_wait_mutex_enter();
-  /* We omit trx_mutex_enter and lock_mutex_enter here, because we are only
+  /* We omit trx_mutex_enter and a lock_sys latches here, because we are only
   going to touch thr->slot, which is a member used only by lock0wait.cc and is
   sufficiently protected by lock_wait_mutex. Yes, there are readers who read
-  the thr->slot holding only trx->mutex and lock_sys->mutex, but they do so,
+  the thr->slot holding only trx->mutex and a lock_sys latch, but they do so,
   when they are sure that we were not woken up yet, so our thread can't be here.
   See comments in lock_wait_release_thread_if_suspended() for more details. */
 
@@ -379,15 +379,22 @@ static void lock_wait_release_thread_if_suspended(que_thr_t *thr) {
     2. the only call to os_event_set is in lock_wait_release_thread_if_suspended
     3. calls to lock_wait_release_thread_if_suspended are always performed after
     a call to lock_reset_lock_and_trx_wait(lock), and the sequence of the two is
-    in a critical section guarded by lock_mutex_enter
+    in a critical section guarded by lock_sys latch for the shard containing the
+    waiting lock
     4. the lock_reset_lock_and_trx_wait(lock) asserts that
     lock->trx->lock.wait_lock == lock and sets lock->trx->lock.wait_lock = NULL
   Together all this facts imply, that it is impossible for a single trx to be
   woken up twice (unless it got to sleep again) because doing so requires
   reseting wait_lock to NULL.
 
-  We now hold exclusive lock_sys latch. */
-  ut_ad(lock_mutex_own());
+  We now hold either an exclusive lock_sys latch, or just for the shard which
+  contains the lock which used to be trx->lock.wait_lock, but we can not assert
+  that because trx->lock.wait_lock is now NULL so we don't know for which shard
+  we hold the latch here. So, please imagine something like:
+
+    ut_ad(locksys::owns_lock_shard(lock->trx->lock.wait_lock));
+  */
+
   ut_ad(trx_mutex_own(trx));
 
   /* We don't need the lock_wait_mutex here, because we know that the thread
@@ -419,14 +426,10 @@ static void lock_wait_release_thread_if_suspended(que_thr_t *thr) {
 }
 
 void lock_reset_wait_and_release_thread_if_suspended(lock_t *lock) {
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_lock_shard(lock));
   ut_ad(trx_mutex_own(lock->trx));
   ut_ad(lock->trx->lock.wait_lock == lock);
 
-  /* Reset the wait flag and the back pointer to lock in trx */
-
-  lock_reset_lock_and_trx_wait(lock);
-
   /* We clear blocking_trx here and not in lock_reset_lock_and_trx_wait(), as
   lock_reset_lock_and_trx_wait() is called also when the wait_lock is being
   moved from one page to another during B-tree reorganization, in which case
@@ -434,24 +437,30 @@ void lock_reset_wait_and_release_thread_if_suspended(lock_t *lock) {
   and assigned to trx->lock.wait_lock, but the information about blocking trx
   is not so easy to restore, so it is easier to simply not clear blocking_trx
   until we are 100% sure that we want to wake up the trx, which is now.
-  Actually, clearing blocking_trx is not strictly required from correctness
-  perspective, it rather serves for:
+  Clearing blocking_trx helps with:
   1. performance optimization, as lock_wait_snapshot_waiting_threads() can omit
      this trx when building wait-for-graph
   2. debugging, as reseting blocking_trx makes it easier to spot it was not
-     properly set on subsequent waits. */
+     properly set on subsequent waits.
+  3. helping lock_make_trx_hit_list() notice that HP trx is no longer waiting
+     for a lock, so it can take a fast path */
   lock->trx->lock.blocking_trx.store(nullptr);
 
-  /* We only release locks for which someone is waiting, and the trx which
-  decided to wait for the lock should have already set trx->lock.que_state to
-  TRX_QUE_LOCK_WAIT and called que_thr_stop() before releasing the lock-sys
-  latch. */
+  /* We only release locks for which someone is waiting, and we posses a latch
+  on the shard in which the lock is stored, and the trx which decided to wait
+  for the lock should have already set trx->lock.que_state to TRX_QUE_LOCK_WAIT
+  and called que_thr_stop() before releasing the latch on this shard. */
   ut_ad(lock->trx_que_state() == TRX_QUE_LOCK_WAIT);
 
   /* The following function releases the trx from lock wait */
 
   que_thr_t *thr = que_thr_end_lock_wait(lock->trx);
 
+  /* Reset the wait flag and the back pointer to lock in trx.
+  It is important to call it only after we obtain lock->trx->mutex, because
+  trx_mutex_enter makes some assertions based on trx->lock.wait_lock value */
+  lock_reset_lock_and_trx_wait(lock);
+
   if (thr != nullptr) {
     lock_wait_release_thread_if_suspended(thr);
   }
@@ -480,14 +489,14 @@ static void lock_wait_check_and_cancel(
   if (trx_is_interrupted(trx) ||
       (slot->wait_timeout < 100000000 &&
        (wait_time > (int64_t)slot->wait_timeout || wait_time < 0))) {
-    /* Timeout exceeded or a wrap-around in system
-    time counter: cancel the lock request queued
-    by the transaction and release possible
-    other transactions waiting behind; it is
-    possible that the lock has already been
-    granted: in that case do nothing */
-
-    lock_mutex_enter();
+    /* Timeout exceeded or a wrap-around in system time counter: cancel the lock
+    request queued by the transaction and release possible other transactions
+    waiting behind; it is possible that the lock has already been granted: in
+    that case do nothing.
+    The lock_cancel_waiting_and_release() needs exclusive global latch.
+    Also, we need to latch the shard containing wait_lock to read the field and
+    access the lock itself. */
+    locksys::Global_exclusive_latch_guard guard{};
 
     trx_mutex_enter(trx);
 
@@ -497,8 +506,6 @@ static void lock_wait_check_and_cancel(
       lock_cancel_waiting_and_release(trx->lock.wait_lock);
     }
 
-    lock_mutex_exit();
-
     trx_mutex_exit(trx);
   }
 }
@@ -520,7 +527,7 @@ struct waiting_trx_info_t {
 sorting criterion which is based on trx only. We use the pointer address, as
 any deterministic rule without ties will do. */
 bool operator<(const waiting_trx_info_t &a, const waiting_trx_info_t &b) {
-  return a.trx < b.trx;
+  return std::less<trx_t *>{}(a.trx, b.trx);
 }
 
 /** Check all slots for user threads that are waiting on locks, and if they have
@@ -531,8 +538,9 @@ static void lock_wait_check_slots_for_timeouts() {
 
   for (auto slot = lock_sys->waiting_threads; slot < lock_sys->last_slot;
        ++slot) {
-    /* We are doing a read without the lock mutex and/or the trx mutex. This is
-    OK because a slot can't be freed or reserved without the lock wait mutex. */
+    /* We are doing a read without latching the lock_sys or the trx mutex.
+    This is OK, because a slot can't be freed or reserved without the lock wait
+    mutex. */
     if (slot->in_use) {
       lock_wait_check_and_cancel(slot);
     }
@@ -657,7 +665,10 @@ static void lock_wait_build_wait_for_graph(
   sort(infos.begin(), infos.end());
   waiting_trx_info_t needle{};
   for (uint from = 0; from < n; ++from) {
-    ut_ad(from == 0 || infos[from - 1].trx < infos[from].trx);
+    /* Assert that the order used by sort and lower_bound depends only on the
+    trx field, as this is the only one we will initialize in the needle. */
+    ut_ad(from == 0 ||
+          std::less<trx_t *>{}(infos[from - 1].trx, infos[from].trx));
     needle.trx = infos[from].waits_for;
     auto it = std::lower_bound(infos.begin(), infos.end(), needle);
 
@@ -675,11 +686,13 @@ static void lock_wait_build_wait_for_graph(
 static void lock_wait_rollback_deadlock_victim(trx_t *chosen_victim) {
   ut_ad(!trx_mutex_own(chosen_victim));
   /* The call to lock_cancel_waiting_and_release requires exclusive latch on
-  whole lock_sys in case of table locks.*/
-  ut_ad(lock_mutex_own());
+  whole lock_sys.
+  Also, we need to latch the shard containing wait_lock to read it and access
+  the lock itself.*/
+  ut_ad(locksys::owns_exclusive_global_latch());
   trx_mutex_enter(chosen_victim);
   chosen_victim->lock.was_chosen_as_deadlock_victim = true;
-  ut_a(chosen_victim->lock.wait_lock);
+  ut_a(chosen_victim->lock.wait_lock != nullptr);
   ut_a(chosen_victim->lock.que_state == TRX_QUE_LOCK_WAIT);
   lock_cancel_waiting_and_release(chosen_victim->lock.wait_lock);
   trx_mutex_exit(chosen_victim);
@@ -905,7 +918,7 @@ static trx_t *lock_wait_choose_victim(
   on the whole lock_sys. In theory number of locks should not change while the
   transaction is waiting, but instead of proving that they can not wake up, it
   is easier to assert that we hold the mutex */
-  ut_ad(lock_mutex_own());
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(!cycle_ids.empty());
   trx_t *chosen_victim = nullptr;
   auto sorted_trxs = lock_wait_order_for_choosing_victim(cycle_ids, infos);
@@ -969,7 +982,8 @@ static bool lock_wait_trxs_are_still_in_slots(
 in it which form a deadlock cycle, checks if the transactions allegedly forming
 the deadlock have actually still wait for a lock, as opposed to being already
 notified about lock being granted or timeout, but still being present in the
-slot. This is done by checking trx->lock.wait_lock under lock_sys mutex.
+slot. This is done by checking trx->lock.wait_lock under exclusive global
+lock_sys latch.
 @param[in]    cycle_ids   indexes in `infos` array, of transactions forming the
                           deadlock cycle
 @param[in]    infos       information about all waiting transactions
@@ -980,8 +994,9 @@ static bool lock_wait_trxs_are_still_waiting(
   ut_ad(lock_wait_mutex_own());
   /* We are iterating over various transaction which may have locks in different
   tables/rows, thus we need exclusive latch on the whole lock_sys to make sure
-  no one will wake them up (say, a high priority trx could abort them) */
-  ut_ad(lock_mutex_own());
+  no one will wake them up (say, a high priority trx could abort them) or change
+  the wait_lock to NULL temporarily during B-tree page reorganization. */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   for (auto id : cycle_ids) {
     const auto trx = infos[id].trx;
@@ -1146,7 +1161,7 @@ static bool lock_wait_check_candidate_cycle(
     ut::vector<uint> &cycle_ids, const ut::vector<waiting_trx_info_t> &infos,
     ut::vector<trx_schedule_weight_t> &new_weights) {
   ut_ad(!lock_wait_mutex_own());
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   lock_wait_mutex_enter();
   /*
   We have released all mutexes after we have built the `infos` snapshot and
@@ -1160,8 +1175,8 @@ static bool lock_wait_check_candidate_cycle(
   If it has not changed, then we know that the trx's pointer still points to the
   same trx as the trx is sleeping, and thus has not finished and wasn't freed.
   So, we start by first checking that the slots still contain the trxs we are
-  interested in. This requires lock_wait_mutex, but not lock_mutex.
-  */
+  interested in. This requires lock_wait_mutex, but does not require the
+  exclusive global latch. */
   if (!lock_wait_trxs_are_still_in_slots(cycle_ids, infos)) {
     lock_wait_mutex_exit();
     return false;
@@ -1182,11 +1197,10 @@ static bool lock_wait_check_candidate_cycle(
   situation by looking at trx->lock.wait_lock, as each call to
   lock_wait_release_thread_if_suspended() is performed only after
   lock_reset_lock_and_trx_wait() resets trx->lock.wait_lock to NULL.
-  Checking trx->lock.wait_lock must be done under lock_mutex.
+  Checking trx->lock.wait_lock in reliable way requires global exclusive latch.
   */
-  lock_mutex_enter();
+  locksys::Global_exclusive_latch_guard gurad{};
   if (!lock_wait_trxs_are_still_waiting(cycle_ids, infos)) {
-    lock_mutex_exit();
     lock_wait_mutex_exit();
     return false;
   }
@@ -1195,11 +1209,11 @@ static bool lock_wait_check_candidate_cycle(
   We can now release lock_wait_mutex, because:
 
   1. we have verified that trx->lock.wait_lock is not NULL for cycle_ids
-  2. we hold lock_sys->mutex
-  3. lock_sys->mutex is required to change trx->lock.wait_lock to NULL
+  2. we hold exclusive global lock_sys latch
+  3. lock_sys latch is required to change trx->lock.wait_lock to NULL
   4. only after changing trx->lock.wait_lock to NULL a trx can finish
 
-  So as long as we hold lock_sys->mutex we can access trxs.
+  So as long as we hold exclusive global lock_sys latch we can access trxs.
   */
 
   lock_wait_mutex_exit();
@@ -1209,7 +1223,6 @@ static bool lock_wait_check_candidate_cycle(
 
   lock_wait_handle_deadlock(chosen_victim, cycle_ids, infos, new_weights);
 
-  lock_mutex_exit();
   return true;
 }
 
diff --git a/storage/innobase/que/que0que.cc b/storage/innobase/que/que0que.cc
index dbf65dfba31..0c89cd1ee4b 100644
--- a/storage/innobase/que/que0que.cc
+++ b/storage/innobase/que/que0que.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1996, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1996, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -190,13 +190,11 @@ que_thr_t *que_thr_create(que_fork_t *parent, mem_heap_t *heap,
 que_thr_t *que_thr_end_lock_wait(trx_t *trx) /*!< in: transaction with que_state
                                              in QUE_THR_LOCK_WAIT */
 {
-  que_thr_t *thr;
-  ibool was_active;
+  ut_ad(locksys::owns_lock_shard(trx->lock.wait_lock));
 
-  ut_ad(lock_mutex_own());
   ut_ad(trx_mutex_own(trx));
 
-  thr = trx->lock.wait_thr;
+  que_thr_t *const thr = trx->lock.wait_thr;
 
   ut_ad(thr != nullptr);
 
@@ -204,7 +202,7 @@ que_thr_t *que_thr_end_lock_wait(trx_t *trx) /*!< in: transaction with que_state
   /* In MySQL this is the only possible state here */
   ut_a(thr->state == QUE_THR_LOCK_WAIT);
 
-  was_active = thr->is_active;
+  bool const was_active = thr->is_active;
 
   que_thr_move_to_run_state(thr);
 
@@ -215,7 +213,7 @@ que_thr_t *que_thr_end_lock_wait(trx_t *trx) /*!< in: transaction with que_state
   /* In MySQL we let the OS thread (not just the query thread) to wait
   for the lock to be released: */
 
-  return ((!was_active && thr != nullptr) ? thr : nullptr);
+  return !was_active ? thr : nullptr;
 }
 
 /** Inits a query thread for a command. */
@@ -591,16 +589,10 @@ static void que_thr_move_to_run_state(
   thr->state = QUE_THR_RUNNING;
 }
 
-/** Stops a query thread if graph or trx is in a state requiring it. The
- conditions are tested in the order (1) graph, (2) trx.
- @return true if stopped */
-ibool que_thr_stop(que_thr_t *thr) /*!< in: query thread */
-{
-  que_t *graph;
+bool que_thr_stop(que_thr_t *thr) {
+  que_t *graph = thr->graph;
   trx_t *trx = thr_get_trx(thr);
 
-  graph = thr->graph;
-
   ut_ad(trx_mutex_own(trx));
 
   if (graph->state == QUE_FORK_COMMAND_WAIT) {
@@ -620,10 +612,10 @@ ibool que_thr_stop(que_thr_t *thr) /*!< in: query thread */
   } else {
     ut_ad(graph->state == QUE_FORK_ACTIVE);
 
-    return (FALSE);
+    return false;
   }
 
-  return (TRUE);
+  return true;
 }
 
 /** Decrements the query thread reference counts in the query graph and the
diff --git a/storage/innobase/row/row0ins.cc b/storage/innobase/row/row0ins.cc
index 43dd6385099..63e4af7c9e1 100644
--- a/storage/innobase/row/row0ins.cc
+++ b/storage/innobase/row/row0ins.cc
@@ -701,11 +701,14 @@ static void row_ins_foreign_trx_print(trx_t *trx) /*!< in: transaction */
     return;
   }
 
-  lock_mutex_enter();
-  n_rec_locks = lock_number_of_rows_locked(&trx->lock);
-  n_trx_locks = UT_LIST_GET_LEN(trx->lock.trx_locks);
-  heap_size = mem_heap_get_size(trx->lock.lock_heap);
-  lock_mutex_exit();
+  {
+    /** lock_number_of_rows_locked() requires global exclusive latch, and so
+    does accessing trx_locks with trx->mutex */
+    locksys::Global_exclusive_latch_guard guard{};
+    n_rec_locks = lock_number_of_rows_locked(&trx->lock);
+    n_trx_locks = UT_LIST_GET_LEN(trx->lock.trx_locks);
+    heap_size = mem_heap_get_size(trx->lock.lock_heap);
+  }
 
   trx_sys_mutex_enter();
 
diff --git a/storage/innobase/row/row0mysql.cc b/storage/innobase/row/row0mysql.cc
index c275637ede6..dd7fb46f29d 100644
--- a/storage/innobase/row/row0mysql.cc
+++ b/storage/innobase/row/row0mysql.cc
@@ -76,7 +76,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "trx0rec.h"
 #include "trx0roll.h"
 #include "trx0undo.h"
-#include "ut0mpmcbq.h"
+#include "ut0cpu_cache.h"
 #include "ut0new.h"
 
 #include "current_thd.h"
@@ -1153,8 +1153,11 @@ dberr_t row_lock_table_autoinc_for_mysql(
   ibool was_lock_wait;
 
   /* If we already hold an AUTOINC lock on the table then do nothing.
-  Note: We peek at the value of the current owner without acquiring
-  the lock mutex. */
+  Note: We peek at the value of the current owner without acquiring any latch,
+  which is OK, because if the equality holds, it means we were granted the lock,
+  and the only way table->autoinc_trx can subsequently change is by releasing
+  the lock, which can not happen concurrently with the thread running the trx.*/
+  ut_ad(trx_can_be_handled_by_current_thread(trx));
   if (trx == table->autoinc_trx) {
     return (DB_SUCCESS);
   }
@@ -3990,8 +3993,8 @@ dberr_t row_drop_table_for_mysql(const char *name, trx_t *trx, bool nonatomic,
     if (!table->is_intrinsic()) {
       lock_remove_all_on_table(table, TRUE);
     }
-    ut_a(table->n_rec_locks == 0);
-  } else if (table->get_ref_count() > 0 || table->n_rec_locks > 0) {
+    ut_a(table->n_rec_locks.load() == 0);
+  } else if (table->get_ref_count() > 0 || table->n_rec_locks.load() > 0) {
     ibool added;
 
     ut_ad(0);
@@ -4405,8 +4408,7 @@ dberr_t row_mysql_parallel_select_count_star(
   Shards n_recs;
   Counter::clear(n_recs);
 
-  struct Check_interrupt {
-    byte m_pad[INNOBASE_CACHE_LINE_SIZE - (sizeof(size_t) + sizeof(void *))];
+  struct alignas(ut::INNODB_CACHE_LINE_SIZE) Check_interrupt {
     size_t m_count{};
     const buf_block_t *m_prev_block{};
   };
diff --git a/storage/innobase/row/row0vers.cc b/storage/innobase/row/row0vers.cc
index 9d7f2c34f92..511ed3c3911 100644
--- a/storage/innobase/row/row0vers.cc
+++ b/storage/innobase/row/row0vers.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1997, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1997, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -284,20 +284,23 @@ static bool row_vers_find_matching(
 
 /** Finds out if an active transaction has inserted or modified a secondary
  index record.
+ @param[in]       clust_rec     clustered index record
+ @param[in]       clust_index   the clustered index
+ @param[in]       sec_rec       secondary index record
+ @param[in]       sec_index     the secondary index
+ @param[in]       sec_offsets   rec_get_offsets(sec_rec, sec_index)
+ @param[in,out]   mtr           mini-transaction
  @return 0 if committed, else the active transaction id;
  NOTE that this function can return false positives but never false
- negatives. The caller must confirm all positive results by calling
- trx_is_active() while holding lock_sys->mutex. */
+ negatives. The caller must confirm all positive results by calling checking if
+ the trx is still active.*/
 UNIV_INLINE
-trx_t *row_vers_impl_x_locked_low(
-    const rec_t *const clust_rec,          /*!< in: clustered index record */
-    const dict_index_t *const clust_index, /*!< in: the clustered index */
-    const rec_t *const sec_rec,            /*!< in: secondary index record */
-    const dict_index_t *const sec_index,   /*!< in: the secondary index */
-    const ulint
-        *const sec_offsets, /*!< in: rec_get_offsets(sec_rec, sec_index) */
-    mtr_t *const mtr)       /*!< in/out: mini-transaction */
-{
+trx_t *row_vers_impl_x_locked_low(const rec_t *const clust_rec,
+                                  const dict_index_t *const clust_index,
+                                  const rec_t *const sec_rec,
+                                  const dict_index_t *const sec_index,
+                                  const ulint *const sec_offsets,
+                                  mtr_t *const mtr) {
   trx_id_t trx_id;
   ibool corrupt;
   ulint comp;
@@ -530,23 +533,14 @@ trx_t *row_vers_impl_x_locked_low(
   return trx;
 }
 
-/** Finds out if an active transaction has inserted or modified a secondary
- index record.
- @return 0 if committed, else the active transaction id;
- NOTE that this function can return false positives but never false
- negatives. The caller must confirm all positive results by calling
- trx_is_active() while holding lock_sys->mutex. */
-trx_t *row_vers_impl_x_locked(
-    const rec_t *rec,          /*!< in: record in a secondary index */
-    const dict_index_t *index, /*!< in: the secondary index */
-    const ulint *offsets)      /*!< in: rec_get_offsets(rec, index) */
-{
+trx_t *row_vers_impl_x_locked(const rec_t *rec, const dict_index_t *index,
+                              const ulint *offsets) {
   mtr_t mtr;
   trx_t *trx;
   const rec_t *clust_rec;
   dict_index_t *clust_index;
 
-  ut_ad(!lock_mutex_own());
+  ut_ad(!locksys::owns_exclusive_global_latch());
   ut_ad(!trx_sys_mutex_own());
 
   mtr_start(&mtr);
diff --git a/storage/innobase/srv/srv0srv.cc b/storage/innobase/srv/srv0srv.cc
index efb6e472be0..d2d87cbb37a 100644
--- a/storage/innobase/srv/srv0srv.cc
+++ b/storage/innobase/srv/srv0srv.cc
@@ -1256,18 +1256,30 @@ static void srv_refresh_innodb_monitor_stats(void) {
   mutex_exit(&srv_innodb_monitor_mutex);
 }
 
-/** Outputs to a file the output of the InnoDB Monitor.
- @return false if not all information printed
- due to failure to obtain necessary mutex */
-ibool srv_printf_innodb_monitor(
-    FILE *file,           /*!< in: output stream */
-    ibool nowait,         /*!< in: whether to wait for the
-                          lock_sys_t:: mutex */
-    ulint *trx_start_pos, /*!< out: file position of the start of
-                          the list of active transactions */
-    ulint *trx_end)       /*!< out: file position of the end of
-                          the list of active transactions */
-{
+/**
+Prints info summary and info about all transactions to the file, recording the
+position where the part about transactions starts.
+@param[in]    file            output stream
+@param[out]   trx_start_pos   file position of the start of the list of active
+                              transactions
+*/
+static void srv_printf_locks_and_transactions(FILE *file,
+                                              ulint *trx_start_pos) {
+  ut_ad(locksys::owns_exclusive_global_latch());
+  lock_print_info_summary(file);
+  if (trx_start_pos) {
+    long t = ftell(file);
+    if (t < 0) {
+      *trx_start_pos = ULINT_UNDEFINED;
+    } else {
+      *trx_start_pos = (ulint)t;
+    }
+  }
+  lock_print_info_all_transactions(file);
+}
+
+bool srv_printf_innodb_monitor(FILE *file, bool nowait, ulint *trx_start_pos,
+                               ulint *trx_end) {
   ulint n_reserved;
   ibool ret;
 
@@ -1325,27 +1337,22 @@ ibool srv_printf_innodb_monitor(
 
   mutex_exit(&dict_foreign_err_mutex);
 
-  /* Only if lock_print_info_summary proceeds correctly,
-  before we call the lock_print_info_all_transactions
-  to print all the lock information. IMPORTANT NOTE: This
-  function acquires the lock mutex on success. */
-  ret = lock_print_info_summary(file, nowait);
-
-  if (ret) {
-    if (trx_start_pos) {
-      long t = ftell(file);
-      if (t < 0) {
-        *trx_start_pos = ULINT_UNDEFINED;
-      } else {
-        *trx_start_pos = (ulint)t;
-      }
+  ret = true;
+  if (nowait) {
+    locksys::Global_exclusive_try_latch guard{};
+    if (guard.owns_lock()) {
+      srv_printf_locks_and_transactions(file, trx_start_pos);
+    } else {
+      fputs("FAIL TO OBTAIN LOCK MUTEX, SKIP LOCK INFO PRINTING\n", file);
+      ret = false;
     }
+  } else {
+    locksys::Global_exclusive_latch_guard guard{};
+    srv_printf_locks_and_transactions(file, trx_start_pos);
+  }
 
-    /* NOTE: If we get here then we have the lock mutex. This
-    function will release the lock mutex that we acquired when
-    we called the lock_print_info_summary() function earlier. */
-
-    lock_print_info_all_transactions(file);
+  if (ret) {
+    ut_ad(lock_validate());
 
     if (trx_end) {
       long t = ftell(file);
@@ -1687,7 +1694,7 @@ void srv_monitor_thread() {
   ib_time_monotonic_t current_time;
   ib_time_monotonic_t time_elapsed;
   ulint mutex_skipped;
-  ibool last_srv_print_monitor;
+  bool last_srv_print_monitor = srv_print_innodb_monitor;
 
   ut_ad(!srv_read_only_mode);
 
@@ -1695,10 +1702,9 @@ void srv_monitor_thread() {
   srv_last_monitor_time = last_monitor_time;
 
   mutex_skipped = 0;
-  last_srv_print_monitor = srv_print_innodb_monitor;
 loop:
   /* Wake up every 5 seconds to see if we need to print
-  monitor information or if signalled at shutdown. */
+  monitor information or if signaled at shutdown. */
 
   sig_count = os_event_reset(srv_monitor_event);
 
@@ -1712,14 +1718,13 @@ loop:
     last_monitor_time = ut_time_monotonic();
 
     if (srv_print_innodb_monitor) {
-      /* Reset mutex_skipped counter everytime
-      srv_print_innodb_monitor changes. This is to
-      ensure we will not be blocked by lock_sys->mutex
-      for short duration information printing,
-      such as requested by sync_array_print_long_waits() */
+      /* Reset mutex_skipped counter every time srv_print_innodb_monitor
+      changes. This is to ensure we will not be blocked by lock_sys global latch
+      for short duration information printing, such as requested by
+      sync_array_print_long_waits() */
       if (!last_srv_print_monitor) {
         mutex_skipped = 0;
-        last_srv_print_monitor = TRUE;
+        last_srv_print_monitor = true;
       }
 
       if (!srv_printf_innodb_monitor(stderr, MUTEX_NOWAIT(mutex_skipped),
@@ -1730,7 +1735,7 @@ loop:
         mutex_skipped = 0;
       }
     } else {
-      last_srv_print_monitor = FALSE;
+      last_srv_print_monitor = false;
     }
 
     /* We don't create the temp files or associated
diff --git a/storage/innobase/sync/sync0debug.cc b/storage/innobase/sync/sync0debug.cc
index d4fd4f09cf3..5c243345d9a 100644
--- a/storage/innobase/sync/sync0debug.cc
+++ b/storage/innobase/sync/sync0debug.cc
@@ -138,7 +138,7 @@ struct LatchDebug {
   @return	pointer to a thread's acquired latches. */
   Latches *thread_latches(bool add = false) UNIV_NOTHROW;
 
-  /** Check that all the latches already owned by a thread have a lower
+  /** Check that all the latches already owned by a thread have a higher
   level than limit.
   @param[in]	latches		the thread's existing (acquired) latches
   @param[in]	limit		to check against
@@ -441,11 +441,11 @@ LatchDebug::LatchDebug() {
   LEVEL_MAP_INSERT(SYNC_PAGE_CLEANER);
   LEVEL_MAP_INSERT(SYNC_PURGE_QUEUE);
   LEVEL_MAP_INSERT(SYNC_TRX_SYS_HEADER);
-  LEVEL_MAP_INSERT(SYNC_REC_LOCK);
   LEVEL_MAP_INSERT(SYNC_THREADS);
   LEVEL_MAP_INSERT(SYNC_TRX);
   LEVEL_MAP_INSERT(SYNC_TRX_SYS);
-  LEVEL_MAP_INSERT(SYNC_LOCK_SYS);
+  LEVEL_MAP_INSERT(SYNC_LOCK_SYS_GLOBAL);
+  LEVEL_MAP_INSERT(SYNC_LOCK_SYS_SHARDED);
   LEVEL_MAP_INSERT(SYNC_LOCK_WAIT_SYS);
   LEVEL_MAP_INSERT(SYNC_INDEX_ONLINE_LOG);
   LEVEL_MAP_INSERT(SYNC_IBUF_BITMAP);
@@ -546,11 +546,6 @@ void LatchDebug::crash(const Latches *latches, const Latched *latched,
   ut_error;
 }
 
-/** Check that all the latches already owned by a thread have a lower
-level than limit.
-@param[in]	latches		the thread's existing (acquired) latches
-@param[in]	limit		to check against
-@return latched info if there is one with a level <= limit . */
 const Latched *LatchDebug::less(const Latches *latches,
                                 latch_level_t limit) const UNIV_NOTHROW {
   Latches::const_iterator end = latches->end();
@@ -565,6 +560,7 @@ const Latched *LatchDebug::less(const Latches *latches,
 }
 
 /** Do a basic ordering check.
+Asserts that all the existing latches have a level higher than the in_level.
 @param[in]	latches		thread's existing latches
 @param[in]	requested_level	Level requested by latch
 @param[in]	in_level	declared ulint so that we can do level - 1.
@@ -702,7 +698,7 @@ Latches *LatchDebug::check_order(const latch_t *latch,
     case SYNC_PAGE_ARCH_CLIENT:
     case SYNC_SEARCH_SYS:
     case SYNC_THREADS:
-    case SYNC_LOCK_SYS:
+    case SYNC_LOCK_SYS_GLOBAL:
     case SYNC_LOCK_WAIT_SYS:
     case SYNC_TRX_SYS:
     case SYNC_IBUF_BITMAP_MUTEX:
@@ -755,12 +751,13 @@ Latches *LatchDebug::check_order(const latch_t *latch,
 
     case SYNC_TRX:
 
-      /* Either the thread must own the lock_sys->mutex, or
-      it is allowed to own only ONE trx_t::mutex. */
+      /* Either the thread must own the lock_sys global latch, or
+      it is allowed to own only ONE trx_t::mutex. There are additional rules
+      for holding more than one trx_t::mutex @see trx_before_mutex_enter(). */
 
       if (less(latches, level) != nullptr) {
         basic_check(latches, level, level - 1);
-        ut_a(find(latches, SYNC_LOCK_SYS) != nullptr);
+        ut_a(find(latches, SYNC_LOCK_SYS_GLOBAL) != nullptr);
       }
       break;
 
@@ -774,6 +771,9 @@ Latches *LatchDebug::check_order(const latch_t *latch,
     case SYNC_BUF_ZIP_HASH:
     case SYNC_BUF_FLUSH_STATE:
     case SYNC_RSEG_ARRAY_HEADER:
+    case SYNC_LOCK_SYS_SHARDED:
+    case SYNC_BUF_PAGE_HASH:
+    case SYNC_BUF_BLOCK:
 
       /* We can have multiple mutexes of this type therefore we
       can only check whether the greater than condition holds. */
@@ -781,24 +781,6 @@ Latches *LatchDebug::check_order(const latch_t *latch,
       basic_check(latches, level, level - 1);
       break;
 
-    case SYNC_BUF_PAGE_HASH:
-      /* Fall through */
-    case SYNC_BUF_BLOCK:
-
-      if (less(latches, level) != nullptr) {
-        basic_check(latches, level, level - 1);
-      }
-      break;
-
-    case SYNC_REC_LOCK:
-
-      if (find(latches, SYNC_LOCK_SYS) != nullptr) {
-        basic_check(latches, level, SYNC_REC_LOCK - 1);
-      } else {
-        basic_check(latches, level, SYNC_REC_LOCK);
-      }
-      break;
-
     case SYNC_IBUF_BITMAP:
 
       /* Either the thread must own the master mutex to all
@@ -1391,7 +1373,11 @@ static void sync_latch_meta_init() UNIV_NOTHROW {
 
   LATCH_ADD_MUTEX(TRX, SYNC_TRX, trx_mutex_key);
 
-  LATCH_ADD_MUTEX(LOCK_SYS, SYNC_LOCK_SYS, lock_mutex_key);
+  LATCH_ADD_MUTEX(LOCK_SYS_PAGE, SYNC_LOCK_SYS_SHARDED,
+                  lock_sys_page_mutex_key);
+
+  LATCH_ADD_MUTEX(LOCK_SYS_TABLE, SYNC_LOCK_SYS_SHARDED,
+                  lock_sys_table_mutex_key);
 
   LATCH_ADD_MUTEX(LOCK_SYS_WAIT, SYNC_LOCK_WAIT_SYS, lock_wait_mutex_key);
 
@@ -1455,6 +1441,9 @@ static void sync_latch_meta_init() UNIV_NOTHROW {
 
   LATCH_ADD_RWLOCK(RSEGS, SYNC_RSEGS, rsegs_lock_key);
 
+  LATCH_ADD_RWLOCK(LOCK_SYS_GLOBAL, SYNC_LOCK_SYS_GLOBAL,
+                   lock_sys_global_rw_lock_key);
+
   LATCH_ADD_RWLOCK(UNDO_SPACES, SYNC_UNDO_SPACES, undo_spaces_lock_key);
 
   LATCH_ADD_MUTEX(UNDO_DDL, SYNC_UNDO_DDL, PFS_NOT_INSTRUMENTED);
diff --git a/storage/innobase/sync/sync0sync.cc b/storage/innobase/sync/sync0sync.cc
index c640a0dbf68..a3f853b6621 100644
--- a/storage/innobase/sync/sync0sync.cc
+++ b/storage/innobase/sync/sync0sync.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1995, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1995, 2020, Oracle and/or its affiliates. All Rights Reserved.
 Copyright (c) 2008, Google Inc.
 
 Portions of this file contain modifications contributed and copyrighted by
@@ -122,7 +122,8 @@ mysql_pfs_key_t trx_mutex_key;
 mysql_pfs_key_t trx_pool_mutex_key;
 mysql_pfs_key_t trx_pool_manager_mutex_key;
 mysql_pfs_key_t temp_pool_manager_mutex_key;
-mysql_pfs_key_t lock_mutex_key;
+mysql_pfs_key_t lock_sys_table_mutex_key;
+mysql_pfs_key_t lock_sys_page_mutex_key;
 mysql_pfs_key_t lock_wait_mutex_key;
 mysql_pfs_key_t trx_sys_mutex_key;
 mysql_pfs_key_t srv_sys_mutex_key;
@@ -154,6 +155,7 @@ mysql_pfs_key_t buf_block_debug_latch_key;
 #endif /* UNIV_DEBUG */
 mysql_pfs_key_t undo_spaces_lock_key;
 mysql_pfs_key_t rsegs_lock_key;
+mysql_pfs_key_t lock_sys_global_rw_lock_key;
 mysql_pfs_key_t dict_operation_lock_key;
 mysql_pfs_key_t dict_table_stats_key;
 mysql_pfs_key_t hash_table_locks_key;
diff --git a/storage/innobase/trx/trx0i_s.cc b/storage/innobase/trx/trx0i_s.cc
index 0fc9d3d2fbf..9013822a5a9 100644
--- a/storage/innobase/trx/trx0i_s.cc
+++ b/storage/innobase/trx/trx0i_s.cc
@@ -187,15 +187,16 @@ struct trx_i_s_cache_t {
 #define CACHE_STORAGE_INITIAL_SIZE 1024
 /** Number of hash cells in the cache storage */
 #define CACHE_STORAGE_HASH_CELLS 2048
-  ha_storage_t *storage; /*!< storage for external volatile
-                         data that may become unavailable
-                         when we release
-                         lock_sys->mutex or trx_sys->mutex */
-  ulint mem_allocd;      /*!< the amount of memory
-                         allocated with mem_alloc*() */
-  ibool is_truncated;    /*!< this is TRUE if the memory
-                         limit was hit and thus the data
-                         in the cache is truncated */
+  /** storage for external volatile data that may become unavailable when we
+  release exclusive global locksys latch or trx_sys->mutex */
+  ha_storage_t *storage;
+
+  /** the amount of memory allocated with mem_alloc*() */
+  ulint mem_allocd;
+
+  /** this is TRUE if the memory limit was hit and thus the data in the cache is
+  truncated */
+  bool is_truncated;
 };
 
 /** This is the intermediate buffer where data needed to fill the
@@ -435,7 +436,11 @@ static ibool fill_trx_row(
 
   /* We are going to read various trx->lock fields protected by trx->mutex */
   ut_ad(trx_mutex_own(trx));
-  ut_ad(lock_mutex_own());
+  /* We are going to read TRX_WEIGHT, lock_number_of_rows_locked() and
+  lock_number_of_tables_locked() which requires latching the lock_sys.
+  Also, we need it to avoid reading temporary NULL value set to wait_lock by a
+  B-tree page reorganization. */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   row->trx_id = trx_get_id_for_print(trx);
   row->trx_started = (ib_time_t)trx->start_time;
@@ -645,12 +650,10 @@ void p_s_fill_lock_data(const char **lock_data, const lock_t *lock,
   const rec_t *rec;
   const dict_index_t *index;
   ulint n_fields;
-  mem_heap_t *heap;
-  ulint offsets_onstack[REC_OFFS_NORMAL_SIZE];
-  ulint *offsets;
   char buf[TRX_I_S_LOCK_DATA_MAX_LEN];
   ulint buf_used;
   ulint i;
+  Rec_offsets rec_offsets;
 
   mtr_start(&mtr);
 
@@ -667,9 +670,6 @@ void p_s_fill_lock_data(const char **lock_data, const lock_t *lock,
 
   page = reinterpret_cast<const page_t *>(buf_block_get_frame(block));
 
-  rec_offs_init(offsets_onstack);
-  offsets = offsets_onstack;
-
   rec = page_find_rec_with_heap_no(page, heap_no);
 
   index = lock_rec_get_index(lock);
@@ -678,8 +678,7 @@ void p_s_fill_lock_data(const char **lock_data, const lock_t *lock,
 
   ut_a(n_fields > 0);
 
-  heap = nullptr;
-  offsets = rec_get_offsets(rec, index, offsets, n_fields, &heap);
+  const ulint *offsets = rec_offsets.compute(rec, index);
 
   /* format and store the data */
 
@@ -692,14 +691,6 @@ void p_s_fill_lock_data(const char **lock_data, const lock_t *lock,
 
   *lock_data = container->cache_string(buf);
 
-  if (heap != nullptr) {
-    /* this means that rec_get_offsets() has created a new
-    heap and has stored offsets in it; check that this is
-    really the case and free the heap */
-    ut_a(offsets != offsets_onstack);
-    mem_heap_free(heap);
-  }
-
   mtr_commit(&mtr);
 }
 
@@ -772,7 +763,11 @@ static ibool add_trx_relevant_locks_to_cache(
                                requested lock row, or NULL or
                                undefined */
 {
-  ut_ad(lock_mutex_own());
+  /* We are about to iterate over locks for various tables/rows so we can not
+  narrow the required latch to any specific shard, and thus require exclusive
+  access to lock_sys. This is also needed to avoid observing NULL temporarily
+  set to wait_lock during B-tree page reorganization. */
+  ut_ad(locksys::owns_exclusive_global_latch());
 
   /* If transaction is waiting we add the wait lock and all locks
   from another transactions that are blocking the wait lock. */
@@ -872,6 +867,9 @@ static void fetch_data_into_cache_low(
                              transactions */
     trx_ut_list_t *trx_list) /*!< in: trx list */
 {
+  /* We are going to iterate over many different shards of lock_sys so we need
+  exclusive access */
+  ut_ad(locksys::owns_exclusive_global_latch());
   trx_t *trx;
   bool rw_trx_list = trx_list == &trx_sys->rw_trx_list;
 
@@ -903,7 +901,7 @@ static void fetch_data_into_cache_low(
     ut_ad(trx->in_rw_trx_list == rw_trx_list);
 
     if (!add_trx_relevant_locks_to_cache(cache, trx, &requested_lock_row)) {
-      cache->is_truncated = TRUE;
+      cache->is_truncated = true;
       trx_mutex_exit(trx);
       return;
     }
@@ -913,7 +911,7 @@ static void fetch_data_into_cache_low(
 
     /* memory could not be allocated */
     if (trx_row == nullptr) {
-      cache->is_truncated = TRUE;
+      cache->is_truncated = true;
       trx_mutex_exit(trx);
       return;
     }
@@ -921,7 +919,7 @@ static void fetch_data_into_cache_low(
     if (!fill_trx_row(trx_row, trx, requested_lock_row, cache)) {
       /* memory could not be allocated */
       --cache->innodb_trx.rows_used;
-      cache->is_truncated = TRUE;
+      cache->is_truncated = true;
       trx_mutex_exit(trx);
       return;
     }
@@ -934,7 +932,9 @@ static void fetch_data_into_cache_low(
  table cache buffer. Cache must be locked for write. */
 static void fetch_data_into_cache(trx_i_s_cache_t *cache) /*!< in/out: cache */
 {
-  ut_ad(lock_mutex_own());
+  /* We are going to iterate over many different shards of lock_sys so we need
+  exclusive access */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   trx_i_s_cache_clear(cache);
@@ -946,7 +946,7 @@ static void fetch_data_into_cache(trx_i_s_cache_t *cache) /*!< in/out: cache */
   /* Capture the state of the read-only active transactions */
   fetch_data_into_cache_low(cache, false, &trx_sys->mysql_trx_list);
 
-  cache->is_truncated = FALSE;
+  cache->is_truncated = false;
 }
 
 /** Update the transactions cache if it has not been read for some time.
@@ -959,26 +959,21 @@ int trx_i_s_possibly_fetch_data_into_cache(
     return (1);
   }
 
-  /* We need to read trx_sys and record/table lock queues */
+  {
+    /* We need to read trx_sys and record/table lock queues */
+    locksys::Global_exclusive_latch_guard guard{};
 
-  lock_mutex_enter();
+    trx_sys_mutex_enter();
 
-  trx_sys_mutex_enter();
+    fetch_data_into_cache(cache);
 
-  fetch_data_into_cache(cache);
-
-  trx_sys_mutex_exit();
-
-  lock_mutex_exit();
+    trx_sys_mutex_exit();
+  }
 
   return (0);
 }
 
-/** Returns TRUE if the data in the cache is truncated due to the memory
- limit posed by TRX_I_S_MEM_LIMIT.
- @return true if truncated */
-ibool trx_i_s_cache_is_truncated(trx_i_s_cache_t *cache) /*!< in: cache */
-{
+bool trx_i_s_cache_is_truncated(trx_i_s_cache_t *cache) {
   return (cache->is_truncated);
 }
 
@@ -987,8 +982,10 @@ void trx_i_s_cache_init(trx_i_s_cache_t *cache) /*!< out: cache to init */
 {
   /* The latching is done in the following order:
   acquire trx_i_s_cache_t::rw_lock, X
-  acquire lock mutex
-  release lock mutex
+  acquire locksys exclusive global latch
+  acquire trx_sys mutex
+  release trx_sys mutex
+  release locksys exclusive global latch
   release trx_i_s_cache_t::rw_lock
   acquire trx_i_s_cache_t::rw_lock, S
   acquire trx_i_s_cache_t::last_read_mutex
@@ -1014,7 +1011,7 @@ void trx_i_s_cache_init(trx_i_s_cache_t *cache) /*!< out: cache to init */
 
   cache->mem_allocd = 0;
 
-  cache->is_truncated = FALSE;
+  cache->is_truncated = false;
 }
 
 /** Free the INFORMATION SCHEMA trx related cache. */
diff --git a/storage/innobase/trx/trx0roll.cc b/storage/innobase/trx/trx0roll.cc
index 648282a6b69..bd9f49869f9 100644
--- a/storage/innobase/trx/trx0roll.cc
+++ b/storage/innobase/trx/trx0roll.cc
@@ -1,6 +1,6 @@
 /*****************************************************************************
 
-Copyright (c) 1996, 2019, Oracle and/or its affiliates. All Rights Reserved.
+Copyright (c) 1996, 2020, Oracle and/or its affiliates. All Rights Reserved.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License, version 2.0, as published by the
@@ -637,8 +637,7 @@ static ibool trx_rollback_resurrected(
   ut_ad(trx_sys_mutex_own());
 
   /* The trx->is_recovered flag and trx->state are set
-  atomically under the protection of the trx->mutex (and
-  lock_sys->mutex) in lock_trx_release_locks(). We do not want
+  atomically under the protection of the trx->mutex . We do not want
   to accidentally clean up a non-recovered transaction here. */
 
   trx_mutex_enter(trx);
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index eb7bf180293..832b901fe00 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -557,8 +557,7 @@ static void trx_validate_state_before_free(trx_t *trx) {
 
   if (trx->n_mysql_tables_in_use != 0 || trx->mysql_n_tables_locked != 0) {
     ib::error(ER_IB_MSG_1203)
-        << "MySQL is freeing a thd though"
-           " trx->n_mysql_tables_in_use is "
+        << "MySQL is freeing a thd though trx->n_mysql_tables_in_use is "
         << trx->n_mysql_tables_in_use << " and trx->mysql_n_tables_locked is "
         << trx->mysql_n_tables_locked << ".";
 
@@ -1820,7 +1819,7 @@ written */
     ut_ad(trx->rsegs.m_redo.rseg == nullptr);
     ut_ad(!trx->in_rw_trx_list);
 
-    /* Note: We are asserting without holding the lock mutex. But
+    /* Note: We are asserting without holding the locksys latch. But
     that is OK because this transaction is not waiting and cannot
     be rolled back and no new locks can (or should not) be added
     because it is flagged as a non-locking read-only transaction. */
@@ -2488,16 +2487,10 @@ state_ok:
   }
 }
 
-/** Prints info about a transaction.
- The caller must hold lock_sys->mutex and trx_sys->mutex.
- When possible, use trx_print() instead. */
-void trx_print_latched(
-    FILE *f,             /*!< in: output stream */
-    const trx_t *trx,    /*!< in: transaction */
-    ulint max_query_len) /*!< in: max query length to print,
-                         or 0 to use the default max length */
-{
-  ut_ad(lock_mutex_own());
+void trx_print_latched(FILE *f, const trx_t *trx, ulint max_query_len) {
+  /* We need exclusive access to lock_sys for lock_number_of_rows_locked(),
+  and accessing trx->lock fields without trx->mutex.*/
+  ut_ad(locksys::owns_exclusive_global_latch());
   ut_ad(trx_sys_mutex_own());
 
   trx_print_low(f, trx, max_query_len, lock_number_of_rows_locked(&trx->lock),
@@ -2505,27 +2498,11 @@ void trx_print_latched(
                 mem_heap_get_size(trx->lock.lock_heap));
 }
 
-/** Prints info about a transaction.
- Acquires and releases lock_sys->mutex and trx_sys->mutex. */
-void trx_print(FILE *f,             /*!< in: output stream */
-               const trx_t *trx,    /*!< in: transaction */
-               ulint max_query_len) /*!< in: max query length to print,
-                                    or 0 to use the default max length */
-{
-  ulint n_rec_locks;
-  ulint n_trx_locks;
-  ulint heap_size;
-
-  lock_mutex_enter();
-  n_rec_locks = lock_number_of_rows_locked(&trx->lock);
-  n_trx_locks = UT_LIST_GET_LEN(trx->lock.trx_locks);
-  heap_size = mem_heap_get_size(trx->lock.lock_heap);
-  lock_mutex_exit();
-
+void trx_print(FILE *f, const trx_t *trx, ulint max_query_len) {
+  /* trx_print_latched() requires exclusive global latch */
+  locksys::Global_exclusive_latch_guard guard{};
   mutex_enter(&trx_sys->mutex);
-
-  trx_print_low(f, trx, max_query_len, n_rec_locks, n_trx_locks, heap_size);
-
+  trx_print_latched(f, trx, max_query_len);
   mutex_exit(&trx_sys->mutex);
 }
 
@@ -2547,8 +2524,7 @@ ibool trx_assert_started(const trx_t *trx) /*!< in: transaction */
 
   /* trx->state can change from or to NOT_STARTED while we are holding
   trx_sys->mutex for non-locking autocommit selects but not for other
-  types of transactions. It may change from ACTIVE to PREPARED. Unless
-  we are holding lock_sys->mutex, it may also change to COMMITTED. */
+  types of transactions. It may change from ACTIVE to PREPARED. */
 
   switch (trx->state) {
     case TRX_STATE_PREPARED:
@@ -2565,6 +2541,145 @@ ibool trx_assert_started(const trx_t *trx) /*!< in: transaction */
 
   ut_error;
 }
+
+/*
+Interaction between Lock-sys and trx->mutex-es is rather complicated.
+In particular we allow a thread performing Lock-sys operations to request
+another trx->mutex even though it already holds one for a different trx.
+Therefore one has to prove that it is impossible to form a deadlock cycle in the
+imaginary wait-for-graph in which edges go from thread trying to obtain
+trx->mutex to a thread which holds it at the moment.
+
+In the past it was simple, because Lock-sys was protected by a global mutex,
+which meant that there was at most one thread which could try to posses more
+than one trx->mutex - one can not form a cycle in a graph in which only
+one node has both incoming and outgoing edges.
+
+Today it is much harder to prove, because we have sharded the Lock-sys mutex,
+and now multiple threads can perform Lock-sys operations in parallel, as long
+as they happen in different shards.
+
+Here's my attempt at the proof.
+
+Assumption 1.
+  If a thread attempts to acquire more then one trx->mutex, then it either has
+  exclusive global latch, or it attempts to acquire exactly two of them, and at
+  just before calling mutex_enter for the second time it saw
+  trx1->lock.wait_lock==nullptr, trx2->lock.wait_lock!=nullptr, and it held the
+  latch for the shard containing trx2->lock.wait_lock.
+
+@see asserts in trx_before_mutex_enter
+
+Assumption 2.
+  The Lock-sys latches are taken before any trx->mutex.
+
+@see asserts in sync0debug.cc
+
+Assumption 3.
+  Changing trx->lock.wait_lock from NULL to non-NULL requires latching
+  trx->mutex and the shard containing new wait_lock value.
+
+@see asserts in lock_set_lock_and_trx_wait()
+
+Assumption 4.
+  Changing trx->lock.wait_lock from non-NULL to NULL requires latching the shard
+  containing old wait_lock value.
+
+@see asserts in lock_reset_lock_and_trx_wait()
+
+Assumption 5.
+  If a thread is latching two Lock-sys shards then it's acquiring and releasing
+  both shards together (that is, without interleaving it with trx->mutex
+  operations).
+
+@see Shard_latches_guard
+
+Theorem 1.
+  If the Assumptions 1-5 hold, then it's impossible for trx_mutex_enter() call
+  to deadlock.
+
+By proving the theorem, and observing that the assertions hold for multiple runs
+of test suite on debug build, we gain more and more confidence that
+trx_mutex_enter() calls can not deadlock.
+
+The intuitive, albeit imprecise, version of the proof is that by Assumption 1
+each edge of the deadlock cycle leads from a trx with NULL trx->lock.wait_lock
+to one with non-NULL wait_lock, which means it has only one edge.
+
+The difficulty lays in that wait_lock is a field which can be modified over time
+from several threads, so care must be taken to clarify at which moment in time
+we make our observations and from whose perspective.
+
+We will now formally prove Theorem 1.
+Assume otherwise, that is that we are in a thread which have just started a call
+to mutex_enter(trx_a->mutex) and caused a deadlock.
+
+Fact 0. There is no thread which possesses exclusive Lock-sys latch, since to
+        form a deadlock one needs at least two threads inside Lock-sys
+Fact 1. Each thread participating in the deadlock holds one trx mutex and waits
+        for the second one it tried to acquire
+Fact 2. Thus each thread participating in the deadlock had gone through "else"
+        branch inside trx_before_mutex_enter(), so it verifies Assumption 1.
+Fact 3.	Our thread owns_lock_shard(trx_a->lock.wait_lock)
+Fact 4. Another thread has latched trx_a->mutex as the first of its two latches
+
+Consider the situation from the point of view of this other thread, which is now
+in the deadlock waiting for mutex_enter(trx_b->mutex) for some trx_b!=trx_a.
+By Fact 2 and assumption 1, it had to take the "else" branch on the way there,
+and thus it has saw: trx_a->lock.wait_lock == nullptr at some moment in time.
+This observation was either before or after our observation that
+trx_a->lock.wait_lock != nullptr (again Fact 2 and Assumption 1).
+
+If our thread observed non-NULL value first, then it means a change from
+non-NULL to NULL has happened, which by Assumption 4 requires a shard latch,
+which only our thread posses - and we couldn't manipulate the wait_lock as we
+are in a deadlock.
+
+If the other thread observed NULL first, then it means that the value has
+changed to non-NULL, which requires trx_a->mutex according to Assumption 3, yet
+this mutex was held entire time by the other thread, since it observed the NULL
+just before it deadlock, so it could not change it, either.
+
+So, there is no way the value of wait_lock has changed from NULL to non-NULL or
+vice-versa, yet one thread sees NULL and the other non-NULL - contradiction ends
+the proof.
+*/
+
+static thread_local const trx_t *trx_first_latched_trx = nullptr;
+static thread_local int32_t trx_latched_count = 0;
+static thread_local bool trx_allowed_two_latches = false;
+
+void trx_before_mutex_enter(const trx_t *trx, bool first_of_two) {
+  if (0 == trx_latched_count++) {
+    ut_a(trx_first_latched_trx == nullptr);
+    trx_first_latched_trx = trx;
+    if (first_of_two) {
+      trx_allowed_two_latches = true;
+    }
+  } else {
+    ut_a(!first_of_two);
+    if (!locksys::owns_exclusive_global_latch()) {
+      ut_a(trx_allowed_two_latches);
+      ut_a(trx_latched_count == 2);
+      ut_a(trx_first_latched_trx->lock.wait_lock == nullptr);
+      ut_a(trx_first_latched_trx != trx);
+      /* This is not very safe, because to read trx->lock.wait_lock we
+      should already either latch trx->mutex (which we don't) or shard with
+      trx->lock.wait_lock. But our claim is precisely that we have latched
+      this shard, and we want to check that here. */
+      ut_a(trx->lock.wait_lock != nullptr);
+      ut_a(locksys::owns_lock_shard(trx->lock.wait_lock));
+    }
+  }
+}
+void trx_before_mutex_exit(const trx_t *trx) {
+  ut_a(0 < trx_latched_count);
+  if (0 == --trx_latched_count) {
+    ut_a(trx_first_latched_trx == trx);
+    trx_first_latched_trx = nullptr;
+    trx_allowed_two_latches = false;
+  }
+}
 #endif /* UNIV_DEBUG */
 
 /** Compares the "weight" (or size) of two transactions. Transactions that
@@ -2574,6 +2689,8 @@ ibool trx_assert_started(const trx_t *trx) /*!< in: transaction */
 bool trx_weight_ge(const trx_t *a, /*!< in: transaction to be compared */
                    const trx_t *b) /*!< in: transaction to be compared */
 {
+  /* To read TRX_WEIGHT we need a exclusive global lock_sys latch */
+  ut_ad(locksys::owns_exclusive_global_latch());
   ibool a_notrans_edit;
   ibool b_notrans_edit;
 
@@ -2861,7 +2978,7 @@ int trx_recover_for_mysql(
     /* The state of a read-write transaction cannot change
     from or to NOT_STARTED while we are holding the
     trx_sys->mutex. It may change to PREPARED, but not if
-    trx->is_recovered. It may also change to COMMITTED. */
+    trx->is_recovered. */
     if (trx_state_eq(trx, TRX_STATE_PREPARED)) {
       if (get_info_about_prepared_transaction(&txn_list[count], trx, mem_root))
         break;
@@ -2899,8 +3016,7 @@ int trx_recover_for_mysql(
 /** This function is used to find one X/Open XA distributed transaction
  which is in the prepared state
  @return trx on match, the trx->xid will be invalidated;
- note that the trx may have been committed, unless the caller is
- holding lock_sys->mutex */
+ */
 static MY_ATTRIBUTE((warn_unused_result)) trx_t *trx_get_trx_by_xid_low(
     const XID *xid) /*!< in: X/Open XA transaction
                     identifier */
@@ -2930,14 +3046,7 @@ static MY_ATTRIBUTE((warn_unused_result)) trx_t *trx_get_trx_by_xid_low(
   return (trx);
 }
 
-/** This function is used to find one X/Open XA distributed transaction
- which is in the prepared state
- @return trx or NULL; on match, the trx->xid will be invalidated;
- note that the trx may have been committed, unless the caller is
- holding lock_sys->mutex */
-trx_t *trx_get_trx_by_xid(
-    const XID *xid) /*!< in: X/Open XA transaction identifier */
-{
+trx_t *trx_get_trx_by_xid(const XID *xid) {
   trx_t *trx;
 
   if (xid == nullptr) {
-- 
2.28.0.windows.1


From ba7dea3303471b453e62924250cfbd543cee2538 Mon Sep 17 00:00:00 2001
From: Winston-leon <1871056255@qq.com>
Date: Wed, 18 Nov 2020 16:38:16 +0800
Subject: [PATCH 02/19] Repair Lock-sys releated MTR

---
 mysql-test/suite/innodb/r/cats-autoinc.result | 38 ++++++++
 .../suite/innodb/r/lock_rec_unlock.result     | 22 +++++
 ...ck_trx_release_read_locks_in_x_mode.result | 68 ++++++++++++++
 mysql-test/suite/innodb/r/rec_offsets.result  | 20 ++++
 mysql-test/suite/innodb/t/cats-autoinc.test   | 94 +++++++++++++++++++
 .../suite/innodb/t/lock_rec_unlock.test       | 45 +++++++++
 ...lock_trx_release_read_locks_in_x_mode.test | 84 +++++++++++++++++
 mysql-test/suite/innodb/t/rec_offsets.test    | 31 ++++++
 .../suite/perfschema/r/sxlock_func.result     |  1 +
 9 files changed, 403 insertions(+)
 create mode 100644 mysql-test/suite/innodb/r/lock_trx_release_read_locks_in_x_mode.result
 create mode 100644 mysql-test/suite/innodb/r/rec_offsets.result
 create mode 100644 mysql-test/suite/innodb/t/lock_trx_release_read_locks_in_x_mode.test
 create mode 100644 mysql-test/suite/innodb/t/rec_offsets.test

diff --git a/mysql-test/suite/innodb/r/cats-autoinc.result b/mysql-test/suite/innodb/r/cats-autoinc.result
index d1f79d312c4..16332890746 100644
--- a/mysql-test/suite/innodb/r/cats-autoinc.result
+++ b/mysql-test/suite/innodb/r/cats-autoinc.result
@@ -51,3 +51,41 @@ SET @@global.innodb_lock_wait_timeout = @innodb_lock_wait_timeout_saved;
 # End of Bug #27944920 #
 #                      #
 ########################
+CREATE TABLE t1 (
+id INT PRIMARY KEY AUTO_INCREMENT,
+val INT
+) Engine=InnoDB;
+CREATE TABLE t2 (
+id INT PRIMARY KEY
+) Engine=InnoDB;
+CREATE TABLE t3 (
+id INT PRIMARY KEY,
+val INT
+) Engine=InnoDB;
+INSERT INTO t1 (id, val) VALUES (1,1);
+INSERT INTO t2 (id) VALUES (1),(2),(3);
+INSERT INTO t3 (id, val) VALUES (1,1),(2,2),(3,3),(4,4),(5,5),(6,6);
+SET @innodb_lock_wait_timeout_saved = @@global.innodb_lock_wait_timeout;
+SET @@global.innodb_lock_wait_timeout = 100000;
+BEGIN;
+SELECT * FROM t2 WHERE id=2 FOR UPDATE;
+id
+2
+BEGIN;
+SELECT * FROM t2 WHERE id=3 FOR UPDATE;
+id
+3
+BEGIN;
+UPDATE t3 SET val = 13;
+SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL C2_will_wait';
+INSERT INTO t1 (val) SELECT id FROM t2;
+SET DEBUG_SYNC = 'now WAIT_FOR C2_will_wait';
+SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL C3_will_wait';
+INSERT INTO t1 (val) VALUES (13);;
+SET DEBUG_SYNC = 'now WAIT_FOR C3_will_wait';
+ROLLBACK;
+ERROR 40001: Deadlock found when trying to get lock; try restarting transaction
+ROLLBACK;
+ROLLBACK;
+DROP TABLES t1,t2,t3;
+SET @@global.innodb_lock_wait_timeout = @innodb_lock_wait_timeout_saved;
diff --git a/mysql-test/suite/innodb/r/lock_rec_unlock.result b/mysql-test/suite/innodb/r/lock_rec_unlock.result
index 886a54782fe..6d8ec5da9ea 100644
--- a/mysql-test/suite/innodb/r/lock_rec_unlock.result
+++ b/mysql-test/suite/innodb/r/lock_rec_unlock.result
@@ -59,3 +59,25 @@ SET DEBUG_SYNC = 'RESET';
 # End of Bug #27898384 #
 #                      #
 ########################
+# Bug #31046834    ASSERTION FAILURE: TRX0TRX.CC:2663:TRX_ALLOWED_TWO_LATCHES THREAD 14024410520550
+# Bug #31047326    ASSERTION FAILURE: TRX0TRX.CC:2663:TRX_ALLOWED_2_LATCHES THREAD 139840853837568
+CREATE TABLE t1 (
+id INT PRIMARY KEY,
+val INT
+) Engine=InnoDB;
+INSERT INTO t1 (id, val) VALUES (1,1);
+SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
+BEGIN;
+SET DEBUG_SYNC = 'after_lock_clust_rec_read_check_and_lock SIGNAL con1_created_lock WAIT_FOR con2_will_wait';
+SELECT * FROM t1 WHERE val=13 FOR UPDATE;
+SET DEBUG_SYNC = 'now WAIT_FOR con1_created_lock';
+BEGIN;
+SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL con2_will_wait';
+SELECT * FROM t1 WHERE id=1 FOR UPDATE;
+id	val
+COMMIT;
+id	val
+1	1
+COMMIT;
+DROP TABLE t1;
+SET DEBUG_SYNC = 'RESET';
diff --git a/mysql-test/suite/innodb/r/lock_trx_release_read_locks_in_x_mode.result b/mysql-test/suite/innodb/r/lock_trx_release_read_locks_in_x_mode.result
new file mode 100644
index 00000000000..676aaa3c80d
--- /dev/null
+++ b/mysql-test/suite/innodb/r/lock_trx_release_read_locks_in_x_mode.result
@@ -0,0 +1,68 @@
+CREATE TABLE t0 (id INT PRIMARY KEY) ENGINE=InnoDB;
+CREATE TABLE t1 (id INT PRIMARY KEY) ENGINE=InnoDB;
+CREATE TABLE t2 (id INT PRIMARY KEY) ENGINE=InnoDB;
+CREATE TABLE t3 (id INT PRIMARY KEY) ENGINE=InnoDB;
+CREATE TABLE t4 (id INT PRIMARY KEY) ENGINE=InnoDB;
+CREATE TABLE t5 (id INT PRIMARY KEY) ENGINE=InnoDB;
+INSERT INTO t0 (id) VALUES (1);
+SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
+XA START 'x';
+INSERT INTO t1 (id) VALUES (1);;
+INSERT INTO t2 (id) VALUES (1);;
+INSERT INTO t3 (id) VALUES (1);;
+INSERT INTO t4 (id) VALUES (1);;
+INSERT INTO t5 (id) VALUES (1);;
+SELECT * FROM t0 WHERE id=1 FOR UPDATE;
+id
+1
+XA END 'x';
+SET DEBUG_SYNC='lock_trx_release_read_locks_in_x_mode_will_release
+    SIGNAL c0_releases_in_xmode';
+SET DEBUG_SYNC='try_relatch_trx_and_shard_and_do_noted_expected_version
+    SIGNAL c0_noted_expected_version
+    WAIT_FOR c0_can_go
+    EXECUTE 5';
+XA PREPARE 'x';
+BEGIN;
+SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go';
+SELECT * FROM t1 FOR SHARE;
+BEGIN;
+SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go';
+SELECT * FROM t2 FOR SHARE;
+BEGIN;
+SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go';
+SELECT * FROM t3 FOR SHARE;
+BEGIN;
+SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go';
+SELECT * FROM t4 FOR SHARE;
+BEGIN;
+SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go';
+SELECT * FROM t5 FOR SHARE;
+SET DEBUG_SYNC='now WAIT_FOR c0_releases_in_xmode';
+XA COMMIT 'x';
+id
+1
+COMMIT;
+id
+1
+COMMIT;
+id
+1
+COMMIT;
+id
+1
+COMMIT;
+id
+1
+COMMIT;
+DROP TABLE t0;
+DROP TABLE t1;
+DROP TABLE t2;
+DROP TABLE t3;
+DROP TABLE t4;
+DROP TABLE t5;
diff --git a/mysql-test/suite/innodb/r/rec_offsets.result b/mysql-test/suite/innodb/r/rec_offsets.result
new file mode 100644
index 00000000000..54ce07efc12
--- /dev/null
+++ b/mysql-test/suite/innodb/r/rec_offsets.result
@@ -0,0 +1,20 @@
+CREATE TABLE t (
+id INT PRIMARY KEY,
+c0 INT,   c1 INT,   c2 INT,   c3 INT,   c4 INT,   c5 INT,   c6 INT,   c7 INT,   c8 INT,   c9 INT,
+c10 INT,  c11 INT,  c12 INT,  c13 INT,  c14 INT,  c15 INT,  c16 INT,  c17 INT,  c18 INT,  c19 INT,
+c20 INT,  c21 INT,  c22 INT,  c23 INT,  c24 INT,  c25 INT,  c26 INT,  c27 INT,  c28 INT,  c29 INT,
+c30 INT,  c31 INT,  c32 INT,  c33 INT,  c34 INT,  c35 INT,  c36 INT,  c37 INT,  c38 INT,  c39 INT,
+c40 INT,  c41 INT,  c42 INT,  c43 INT,  c44 INT,  c45 INT,  c46 INT,  c47 INT,  c48 INT,  c49 INT,
+c50 INT,  c51 INT,  c52 INT,  c53 INT,  c54 INT,  c55 INT,  c56 INT,  c57 INT,  c58 INT,  c59 INT,
+c60 INT,  c61 INT,  c62 INT,  c63 INT,  c64 INT,  c65 INT,  c66 INT,  c67 INT,  c68 INT,  c69 INT,
+c70 INT,  c71 INT,  c72 INT,  c73 INT,  c74 INT,  c75 INT,  c76 INT,  c77 INT,  c78 INT,  c79 INT,
+c80 INT,  c81 INT,  c82 INT,  c83 INT,  c84 INT,  c85 INT,  c86 INT,  c87 INT,  c88 INT,  c89 INT,
+c90 INT,  c91 INT,  c92 INT,  c93 INT,  c94 INT,  c95 INT,  c96 INT,  c97 INT,  c98 INT,  c99 INT,
+c100 INT UNIQUE KEY
+) ENGINE=InnoDB;
+BEGIN;
+INSERT INTO t (id,c100) VALUES (1,1);
+INSERT INTO t (id,c100) VALUES (2,1);
+ERROR 23000: Duplicate entry '1' for key 't.c100'
+COMMIT;
+DROP TABLE t;
diff --git a/mysql-test/suite/innodb/t/cats-autoinc.test b/mysql-test/suite/innodb/t/cats-autoinc.test
index dd2a2182843..1df63fce0b6 100644
--- a/mysql-test/suite/innodb/t/cats-autoinc.test
+++ b/mysql-test/suite/innodb/t/cats-autoinc.test
@@ -136,3 +136,97 @@
 --echo # End of Bug #27944920 #
 --echo #                      #
 --echo ########################
+
+# Following scenario is intended to cover the rare case of trx being
+# killed while waiting for a table lock, which excersises the table
+# lock case in lock_cancel_waiting_and_release function.
+#
+# To generate a situation when trx is waiting for a table lock inside
+# InnoDB we use following scenario:
+# C1 locks t2.id = 2
+# C3 locks t2.id = 3
+# C2 obtains t1.AUTO_INC and waits for C1 t2.id=2 row lock
+# C3 tries to insert to t1, and has to wait for C2's autoinc lock
+# C1 rolls back, which unlocks t2.id=2, and C2 proceeds to lock t2.id=3,
+# and now is blocked by C3, but C3 is already blocked by C2, so we have
+# a deadlock cycle.
+# We make C2 heavy to make sure that C3 is chosen as victim, by modyfing
+# many rows in t3.
+
+  CREATE TABLE t1 (
+    id INT PRIMARY KEY AUTO_INCREMENT,
+    val INT
+  ) Engine=InnoDB;
+
+  CREATE TABLE t2 (
+    id INT PRIMARY KEY
+  ) Engine=InnoDB;
+
+  CREATE TABLE t3 (
+    id INT PRIMARY KEY,
+    val INT
+  ) Engine=InnoDB;
+
+  INSERT INTO t1 (id, val) VALUES (1,1);
+  INSERT INTO t2 (id) VALUES (1),(2),(3);
+  INSERT INTO t3 (id, val) VALUES (1,1),(2,2),(3,3),(4,4),(5,5),(6,6);
+
+  # Save the original settings, to be restored at the end of test
+    SET @innodb_lock_wait_timeout_saved = @@global.innodb_lock_wait_timeout;
+
+  # Make sure that transactions will not finish prematurely
+    SET @@global.innodb_lock_wait_timeout = 100000;
+
+
+  --connect (C1, localhost, root,,)
+  --connect (C2, localhost, root,,)
+  --connect (C3, localhost, root,,)
+
+  --connection C1
+    BEGIN;
+    SELECT * FROM t2 WHERE id=2 FOR UPDATE;
+
+  --connection C3
+    BEGIN;
+    SELECT * FROM t2 WHERE id=3 FOR UPDATE;
+
+  --connection C2
+    BEGIN;
+    UPDATE t3 SET val = 13;
+    SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL C2_will_wait';
+    --send INSERT INTO t1 (val) SELECT id FROM t2
+    # C2 --waits-for[t2.id=2]--> C1
+
+  --connection C3
+    SET DEBUG_SYNC = 'now WAIT_FOR C2_will_wait';
+    SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL C3_will_wait';
+    --send INSERT INTO t1 (val) VALUES (13);
+    # C3 --waits-for[t1.autoinc]--> C2 --waits-for[t2.id=2]--> C1
+
+  --connection C1
+    SET DEBUG_SYNC = 'now WAIT_FOR C3_will_wait';
+    ROLLBACK;
+    # C3 --waits-for[t1.autoinc]--> C2 --waits-for[t2.id=3]--> C3
+    # this is a deadlock.
+
+  --connection C3
+    --error ER_LOCK_DEADLOCK
+    --reap
+    ROLLBACK;
+
+  --connection C2
+    --reap
+    ROLLBACK;
+
+  --connection default
+  --disconnect C1
+  --disconnect C2
+  --disconnect C3
+
+
+  DROP TABLES t1,t2,t3;
+
+  # Restore saved state
+
+  SET @@global.innodb_lock_wait_timeout = @innodb_lock_wait_timeout_saved;
+
diff --git a/mysql-test/suite/innodb/t/lock_rec_unlock.test b/mysql-test/suite/innodb/t/lock_rec_unlock.test
index cb2000c02c2..ae641cf66ae 100644
--- a/mysql-test/suite/innodb/t/lock_rec_unlock.test
+++ b/mysql-test/suite/innodb/t/lock_rec_unlock.test
@@ -1,4 +1,5 @@
 --source include/have_debug_sync.inc
+--source include/count_sessions.inc
 
 --echo #################################################################
 --echo #                                                               #
@@ -139,3 +140,47 @@
 --echo # End of Bug #27898384 #
 --echo #                      #
 --echo ########################
+
+
+--echo # Bug #31046834    ASSERTION FAILURE: TRX0TRX.CC:2663:TRX_ALLOWED_TWO_LATCHES THREAD 14024410520550
+--echo # Bug #31047326    ASSERTION FAILURE: TRX0TRX.CC:2663:TRX_ALLOWED_2_LATCHES THREAD 139840853837568
+
+    CREATE TABLE t1 (
+        id INT PRIMARY KEY,
+        val INT
+    ) Engine=InnoDB;
+    INSERT INTO t1 (id, val) VALUES (1,1);
+
+    --connect (con1, localhost, root,,)
+    --connect (con2, localhost, root,,)
+
+    --connection con1
+        SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
+        BEGIN;
+        SET DEBUG_SYNC = 'after_lock_clust_rec_read_check_and_lock SIGNAL con1_created_lock WAIT_FOR con2_will_wait';
+        --send SELECT * FROM t1 WHERE val=13 FOR UPDATE
+
+    --connection con2
+        SET DEBUG_SYNC = 'now WAIT_FOR con1_created_lock';
+        BEGIN;
+        SET DEBUG_SYNC = 'lock_wait_will_wait SIGNAL con2_will_wait';
+        --send SELECT * FROM t1 WHERE id=1 FOR UPDATE
+
+    --connection con1
+        --reap
+        COMMIT;
+
+    --connection con2
+        --reap
+        COMMIT;
+
+
+
+    # Clean up:
+        --connection default
+        --disconnect con1
+        --disconnect con2
+        DROP TABLE t1;
+        SET DEBUG_SYNC = 'RESET';
+
+--source include/wait_until_count_sessions.inc
diff --git a/mysql-test/suite/innodb/t/lock_trx_release_read_locks_in_x_mode.test b/mysql-test/suite/innodb/t/lock_trx_release_read_locks_in_x_mode.test
new file mode 100644
index 00000000000..24dfc31882d
--- /dev/null
+++ b/mysql-test/suite/innodb/t/lock_trx_release_read_locks_in_x_mode.test
@@ -0,0 +1,84 @@
+--source include/have_debug_sync.inc
+--source include/count_sessions.inc
+
+# This test scenario exercises a rare case in which READ COMMITTED
+# (or UNCOMMITTED) transaction tries to release read locks during PREPARE stage
+# (as is typical for XA or in group replication) and is interupted MAX_FAILURES
+# times by other transactions when trying to iterate over its own list of locks.
+# The other transactions are converting implicit locks of the transaction to
+# explicit, adding the explicit locks to the list the transaction is iterating
+# over, so it has to restart. Finally the transaction gives up with s-latching
+# and attempts to x-latch the whole lock-sys to get job done.
+
+# keep in sync with MAX_FAILURES defined in lock_trx_release_read_locks()
+--let MAX_FAILURES=5
+# We create one extra table
+--let i=0
+while($i<=$MAX_FAILURES)
+{
+  --eval CREATE TABLE t$i (id INT PRIMARY KEY) ENGINE=InnoDB
+  --inc $i
+}
+# We will need this row to create explicit lock on it from c0
+INSERT INTO t0 (id) VALUES (1);
+
+--connect (c0, localhost, root,,)
+  SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
+  XA START 'x';
+  # create at least MAX_FAILURES implicit locks
+  --let i=1
+  while($i<=$MAX_FAILURES)
+  {
+    --eval INSERT INTO t$i (id) VALUES (1);
+    --inc $i
+  }
+  # create at least 1 explicit lock
+  SELECT * FROM t0 WHERE id=1 FOR UPDATE;
+  XA END 'x';
+  SET DEBUG_SYNC='lock_trx_release_read_locks_in_x_mode_will_release
+    SIGNAL c0_releases_in_xmode';
+  SET DEBUG_SYNC='try_relatch_trx_and_shard_and_do_noted_expected_version
+    SIGNAL c0_noted_expected_version
+    WAIT_FOR c0_can_go
+    EXECUTE 5';
+  --send XA PREPARE 'x'
+
+--let i=1
+while($i<=$MAX_FAILURES)
+{
+  --connect (c$i, localhost, root,,)
+    BEGIN;
+    SET DEBUG_SYNC = 'now WAIT_FOR c0_noted_expected_version';
+    --eval SET DEBUG_SYNC='lock_wait_will_wait SIGNAL c0_can_go'
+    --send_eval SELECT * FROM t$i FOR SHARE
+
+  --inc $i
+}
+
+--connection default
+SET DEBUG_SYNC='now WAIT_FOR c0_releases_in_xmode';
+
+--connection c0
+  --reap
+  XA COMMIT 'x';
+
+--disconnect c0
+--let i=1
+while($i<=$MAX_FAILURES)
+{
+  --connection c$i
+    --reap
+    COMMIT;
+  --connection default
+  --disconnect c$i
+  --inc $i
+}
+
+--let i=0
+while($i<=$MAX_FAILURES)
+{
+  --eval DROP TABLE t$i
+  --inc $i
+}
+
+--source include/wait_until_count_sessions.inc
diff --git a/mysql-test/suite/innodb/t/rec_offsets.test b/mysql-test/suite/innodb/t/rec_offsets.test
new file mode 100644
index 00000000000..a02419902c5
--- /dev/null
+++ b/mysql-test/suite/innodb/t/rec_offsets.test
@@ -0,0 +1,31 @@
+# More than 100 columns for sure will overflow REC_OFFS_NORMAL_SIZE.
+
+CREATE TABLE t (
+  id INT PRIMARY KEY,
+   c0 INT,   c1 INT,   c2 INT,   c3 INT,   c4 INT,   c5 INT,   c6 INT,   c7 INT,   c8 INT,   c9 INT,
+  c10 INT,  c11 INT,  c12 INT,  c13 INT,  c14 INT,  c15 INT,  c16 INT,  c17 INT,  c18 INT,  c19 INT,
+  c20 INT,  c21 INT,  c22 INT,  c23 INT,  c24 INT,  c25 INT,  c26 INT,  c27 INT,  c28 INT,  c29 INT,
+  c30 INT,  c31 INT,  c32 INT,  c33 INT,  c34 INT,  c35 INT,  c36 INT,  c37 INT,  c38 INT,  c39 INT,
+  c40 INT,  c41 INT,  c42 INT,  c43 INT,  c44 INT,  c45 INT,  c46 INT,  c47 INT,  c48 INT,  c49 INT,
+  c50 INT,  c51 INT,  c52 INT,  c53 INT,  c54 INT,  c55 INT,  c56 INT,  c57 INT,  c58 INT,  c59 INT,
+  c60 INT,  c61 INT,  c62 INT,  c63 INT,  c64 INT,  c65 INT,  c66 INT,  c67 INT,  c68 INT,  c69 INT,
+  c70 INT,  c71 INT,  c72 INT,  c73 INT,  c74 INT,  c75 INT,  c76 INT,  c77 INT,  c78 INT,  c79 INT,
+  c80 INT,  c81 INT,  c82 INT,  c83 INT,  c84 INT,  c85 INT,  c86 INT,  c87 INT,  c88 INT,  c89 INT,
+  c90 INT,  c91 INT,  c92 INT,  c93 INT,  c94 INT,  c95 INT,  c96 INT,  c97 INT,  c98 INT,  c99 INT,
+  c100 INT UNIQUE KEY
+) ENGINE=InnoDB;
+
+# In this test we exercise the nontrivial case in lock_rec_convert_impl_to_expl_for_trx
+# being called from row_convert_impl_to_expl_if_needed without precomputed offsets and
+# requiring more than REC_OFFS_NORMAL_SIZE to be allocated.
+# For that we need to cause secondary unique index conflict, so that the trx has to
+# rollback, but only to save_point (that is: only rollback single query, not whole trx).
+
+BEGIN;
+INSERT INTO t (id,c100) VALUES (1,1);
+--error ER_DUP_ENTRY
+INSERT INTO t (id,c100) VALUES (2,1);
+COMMIT;
+
+DROP TABLE t;
+
diff --git a/mysql-test/suite/perfschema/r/sxlock_func.result b/mysql-test/suite/perfschema/r/sxlock_func.result
index d38b80a06ae..4c04ded433d 100644
--- a/mysql-test/suite/perfschema/r/sxlock_func.result
+++ b/mysql-test/suite/perfschema/r/sxlock_func.result
@@ -20,6 +20,7 @@ wait/synch/sxlock/innodb/fts_cache_rw_lock
 wait/synch/sxlock/innodb/hash_table_locks
 wait/synch/sxlock/innodb/index_online_log
 wait/synch/sxlock/innodb/index_tree_rw_lock
+wait/synch/sxlock/innodb/lock_sys_global_rw_lock
 wait/synch/sxlock/innodb/log_sn_lock
 wait/synch/sxlock/innodb/rsegs_lock
 wait/synch/sxlock/innodb/trx_i_s_cache_lock
-- 
2.28.0.windows.1


From cbd3a078f0cb2872702697c023561397be3e48df Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Thu, 19 Nov 2020 17:44:09 +0800
Subject: [PATCH 03/19] A lock-free hash is used to optimize a key field named
 rw_trx_ids, which frees the readview management from the latch competition.

---
 include/lf.h                              |   3 +
 mysys/lf_hash.cc                          | 121 ++++-
 storage/innobase/api/api0api.cc           |   4 +-
 storage/innobase/buf/buf0buf.cc           |   4 +-
 storage/innobase/clone/clone0copy.cc      |   4 +-
 storage/innobase/clone/clone0repl.cc      |   2 +-
 storage/innobase/dict/dict0mem.cc         |   4 +-
 storage/innobase/handler/ha_innodb.cc     |  38 +-
 storage/innobase/handler/ha_innopart.cc   |   4 +-
 storage/innobase/handler/handler0alter.cc |   6 +-
 storage/innobase/handler/p_s.cc           |  98 +---
 storage/innobase/include/clone0repl.h     |   2 -
 storage/innobase/include/read0read.h      |  95 ----
 storage/innobase/include/read0types.h     | 214 +++-----
 storage/innobase/include/row0vers.h       |  12 +-
 storage/innobase/include/sync0sync.h      |   1 +
 storage/innobase/include/sync0types.h     |   2 +
 storage/innobase/include/trx0roll.h       |   8 +-
 storage/innobase/include/trx0sys.h        | 612 ++++++++++++++++-----
 storage/innobase/include/trx0sys.ic       | 195 -------
 storage/innobase/include/trx0trx.h        | 154 +++---
 storage/innobase/include/trx0trx.ic       |  46 +-
 storage/innobase/include/trx0types.h      |  34 --
 storage/innobase/include/trx0undo.h       |   5 +-
 storage/innobase/include/ut0cpu_cache.h   |   2 +-
 storage/innobase/include/ut0new.h         |   1 -
 storage/innobase/lock/lock0lock.cc        | 592 +++++++++------------
 storage/innobase/page/page0page.cc        |   2 +-
 storage/innobase/read/read0read.cc        | 614 ++++------------------
 storage/innobase/row/row0ins.cc           |   4 -
 storage/innobase/row/row0merge.cc         |   8 +-
 storage/innobase/row/row0mysql.cc         |   2 +-
 storage/innobase/row/row0pread.cc         |   6 +-
 storage/innobase/row/row0row.cc           |  11 +-
 storage/innobase/row/row0sel.cc           |  27 +-
 storage/innobase/row/row0upd.cc           |   3 +-
 storage/innobase/row/row0vers.cc          |  67 ++-
 storage/innobase/srv/srv0srv.cc           |   8 +-
 storage/innobase/srv/srv0start.cc         |  15 +-
 storage/innobase/sync/sync0debug.cc       |   5 +
 storage/innobase/sync/sync0sync.cc        |   1 +
 storage/innobase/trx/trx0i_s.cc           |  90 +---
 storage/innobase/trx/trx0purge.cc         |  12 +-
 storage/innobase/trx/trx0roll.cc          | 165 +++---
 storage/innobase/trx/trx0sys.cc           | 603 +++++++++++++++------
 storage/innobase/trx/trx0trx.cc           | 587 +++++++--------------
 storage/innobase/trx/trx0undo.cc          |  41 +-
 storage/innobase/ut/ut0new.cc             |   3 -
 unittest/gunit/mysys_lf-t.cc              |  11 +-
 49 files changed, 1958 insertions(+), 2590 deletions(-)

diff --git a/include/lf.h b/include/lf.h
index 53898f428d0..549875dca30 100644
--- a/include/lf.h
+++ b/include/lf.h
@@ -159,6 +159,7 @@ struct LF_HASH;
 
 typedef uint lf_hash_func(const LF_HASH *, const uchar *, size_t);
 typedef void lf_hash_init_func(uchar *dst, const uchar *src);
+typedef bool lf_hash_walk_func(void *, void *);
 
 #define LF_HASH_UNIQUE 1
 #define MY_LF_ERRPTR ((void *)(intptr)1)
@@ -214,6 +215,8 @@ int lf_hash_insert(LF_HASH *hash, LF_PINS *pins, const void *data);
 void *lf_hash_search(LF_HASH *hash, LF_PINS *pins, const void *key,
                      uint keylen);
 int lf_hash_delete(LF_HASH *hash, LF_PINS *pins, const void *key, uint keylen);
+int lf_hash_iterate(LF_HASH *hash, LF_PINS *pins, const lf_hash_walk_func *callback,
+                    const void *argument);
 
 static inline LF_PINS *lf_hash_get_pins(LF_HASH *hash) {
   return lf_pinbox_get_pins(&hash->alloc.pinbox);
diff --git a/mysys/lf_hash.cc b/mysys/lf_hash.cc
index e447432521c..2a4b709cb29 100644
--- a/mysys/lf_hash.cc
+++ b/mysys/lf_hash.cc
@@ -101,27 +101,47 @@ static inline T *SET_DELETED(T *ptr) {
   return reinterpret_cast<T *>(i);
 }
 
-/*
-  DESCRIPTION
-    Search for hashnr/key/keylen in the list starting from 'head' and
-    position the cursor. The list is ORDER BY hashnr, key
+/**
+  Walk the list, searching for an element or invoking a callback.
 
-  RETURN
-    0 - not found
-    1 - found
+  Search for hashnr/key/keylen in the list starting from 'head' and position the
+  cursor. The list is ORDER by hashnr, key
 
-  NOTE
+  @param head         start walking the list from this node
+  @param cs           charset for comparing keys, nullptr if callback is used
+  @param hashnr       hash number to searching for
+  @param key          key to search for OR data for the callback
+  @param keylen       length of the key to compare, 0 if callback is used
+  @param cursor       for returning the found element
+  @param pins         see lf_alloc-pin.cc
+  @param callback     callback action, invoked for every element
+
+  @note
     cursor is positioned in either case
-    pins[0..2] are used, they are NOT removed on return
+    pins[0..2] are used, they are not removed on return
+    callback might see some elements twice (because of retries)
+
+  @return
+    if find: 0 - not found
+             1 - found
+    if callback:
+             0 - ok
+             1 - error (callback returned true)
 */
 static int my_lfind(std::atomic<LF_SLIST *> *head, CHARSET_INFO *cs,
                     uint32 hashnr, const uchar *key, size_t keylen,
-                    CURSOR *cursor, LF_PINS *pins) {
+                    CURSOR *cursor, LF_PINS *pins,
+                    lf_hash_walk_func *callback) {
   uint32 cur_hashnr;
   const uchar *cur_key;
   size_t cur_keylen;
   LF_SLIST *link;
 
+  /* should not be set both */
+  DBUG_ASSERT((cs == nullptr) || (callback == nullptr));
+  /* should not be set both */
+  DBUG_ASSERT((keylen == 0) || (callback == nullptr));
+
 retry:
   cursor->prev = head;
   do /* PTR() isn't necessary below, head is a dummy node */
@@ -133,21 +153,23 @@ retry:
     if (unlikely(!cursor->curr)) {
       return 0; /* end of the list */
     }
+    cur_hashnr = cursor->curr->hashnr;
+    cur_keylen = cursor->curr->keylen;
+    cur_key = cursor->curr->key;
     do {
-      /* QQ: XXX or goto retry ? */
-      link = cursor->curr->link.load();
+      link = cursor->curr->link;
       cursor->next = PTR(link);
       lf_pin(pins, 0, cursor->next);
     } while (link != cursor->curr->link && LF_BACKOFF);
-    cur_hashnr = cursor->curr->hashnr;
-    cur_key = cursor->curr->key;
-    cur_keylen = cursor->curr->keylen;
-    if (*cursor->prev != cursor->curr) {
-      (void)LF_BACKOFF;
-      goto retry;
-    }
+
     if (!DELETED(link)) {
-      if (cur_hashnr >= hashnr) {
+      if (likely(callback != nullptr)) {
+        if ((cur_hashnr & 1) > 0 &&
+            callback(cursor->curr + 1,
+                     const_cast<void *>(static_cast<const void *>(key)))) {
+          return 1;
+        }
+      } else if (cur_hashnr >= hashnr) {
         int r = 1;
         if (cur_hashnr > hashnr ||
             (r = my_strnncoll(cs, cur_key, cur_keylen, key, keylen)) >= 0) {
@@ -155,6 +177,8 @@ retry:
         }
       }
       cursor->prev = &(cursor->curr->link);
+      if (!(cur_hashnr & 1)) /* dummy node */
+        head = cursor->prev;
       lf_pin(pins, 2, cursor->curr);
     } else {
       /*
@@ -174,6 +198,25 @@ retry:
   }
 }
 
+/*
+  DESCRIPTION
+    Search for hashnr/key/keylen in the list starting from 'head' and
+    position the cursor. The list is ORDER BY hashnr, key
+
+  RETURN
+    0 - not found
+    1 - found
+
+  NOTE
+    cursor is positioned in either case
+    pins[0..2] are used, they are NOT removed on return
+*/
+static int my_lfind(std::atomic<LF_SLIST *> *head, CHARSET_INFO *cs,
+                    uint32 hashnr, const uchar *key, size_t keylen,
+                    CURSOR *cursor, LF_PINS *pins) {
+  return my_lfind(head, cs, hashnr, key, keylen, cursor, pins, nullptr);
+}
+
 /**
   Search for list element satisfying condition specified by match
   function and position cursor on it.
@@ -570,6 +613,44 @@ int lf_hash_delete(LF_HASH *hash, LF_PINS *pins, const void *key, uint keylen) {
   return 0;
 }
 
+/**
+  Iterate over all elements in hash and call function with the element.
+
+  @note
+  If one of 'callback' invocations returns true the iteration aborts.
+  'action' might see some elements twice!
+
+  @return 0 if ok, or 1 if error (action returned true)
+*/
+int lf_hash_iterate(LF_HASH *hash, LF_PINS *pins, const lf_hash_walk_func *callback,
+                    const void *argument) {
+  CURSOR cursor;
+  uint bucket = 0;
+  int res;
+  std::atomic<LF_SLIST *> *el;
+
+  el = static_cast<std::atomic<LF_SLIST *> *>(
+      lf_dynarray_lvalue(&hash->array, bucket));
+  if (unlikely(el == nullptr)) {
+    /* if there's no bucket==0, the hash is empty */
+    return 0;
+  }
+
+  if (*el == nullptr && unlikely(initialize_bucket(hash, el, bucket, pins))) {
+    /* if there's no bucket==0, the hash is empty */
+    return 0;
+  }
+
+  res = my_lfind(el, nullptr, 0, static_cast<const uchar *>(argument), 0, &cursor,
+                 pins, callback);
+
+  lf_unpin(pins, 2);
+  lf_unpin(pins, 1);
+  lf_unpin(pins, 0);
+
+  return res;
+}
+
 /**
   Find hash element corresponding to the key.
 
diff --git a/storage/innobase/api/api0api.cc b/storage/innobase/api/api0api.cc
index 6d805ebb5bb..42c857e2b29 100644
--- a/storage/innobase/api/api0api.cc
+++ b/storage/innobase/api/api0api.cc
@@ -745,7 +745,7 @@ static ib_err_t ib_create_cursor(ib_crsr_t *ib_crsr,  /*!< out: InnoDB cursor */
       /* Assign a read view if the transaction does
       not have it yet */
 
-      trx_assign_read_view(prebuilt->trx);
+      prebuilt->trx->read_view.open(prebuilt->trx);
     }
 
     *ib_crsr = (ib_crsr_t)cursor;
@@ -936,7 +936,7 @@ ib_err_t ib_cursor_new_trx(ib_crsr_t ib_crsr, /*!< in/out: InnoDB cursor */
 
   cursor->valid_trx = TRUE;
 
-  trx_assign_read_view(prebuilt->trx);
+  prebuilt->trx->read_view.open(prebuilt->trx);
 
   ib_qry_proc_free(&cursor->q_proc);
 
diff --git a/storage/innobase/buf/buf0buf.cc b/storage/innobase/buf/buf0buf.cc
index 0edbc98dda1..bf9c6ac1723 100644
--- a/storage/innobase/buf/buf0buf.cc
+++ b/storage/innobase/buf/buf0buf.cc
@@ -2145,8 +2145,8 @@ withdraw_retry:
       locksys::Global_exclusive_latch_guard guard{};
       trx_sys_mutex_enter();
       bool found = false;
-      for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->mysql_trx_list);
-           trx != nullptr; trx = UT_LIST_GET_NEXT(mysql_trx_list, trx)) {
+      for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->trx_list);
+           trx != nullptr; trx = UT_LIST_GET_NEXT(trx_list, trx)) {
         if (trx->state != TRX_STATE_NOT_STARTED && trx->mysql_thd != nullptr &&
             ut_difftime(withdraw_started, trx->start_time) > 0) {
           if (!found) {
diff --git a/storage/innobase/clone/clone0copy.cc b/storage/innobase/clone/clone0copy.cc
index c7d488f1dbd..8e4b2c190ea 100644
--- a/storage/innobase/clone/clone0copy.cc
+++ b/storage/innobase/clone/clone0copy.cc
@@ -327,14 +327,14 @@ int Clone_Snapshot::update_binlog_position() {
 }
 
 int Clone_Snapshot::wait_trx_end(THD *thd, trx_id_t trx_id) {
-  auto trx = trx_rw_is_active(trx_id, nullptr, false);
+  auto trx = trx_sys->find(current_trx(), trx_id, false);
   if (trx == nullptr) {
     return (0);
   }
 
   auto wait_cond = [&](bool alert, bool &result) {
     /* Check if transaction is still active. */
-    auto trx = trx_rw_is_active(trx_id, nullptr, false);
+    auto trx = trx_sys->find(current_trx(), trx_id, false);
     if (trx == nullptr) {
       result = false;
       return (0);
diff --git a/storage/innobase/clone/clone0repl.cc b/storage/innobase/clone/clone0repl.cc
index ad4e314e8a8..1cb4bde4be4 100644
--- a/storage/innobase/clone/clone0repl.cc
+++ b/storage/innobase/clone/clone0repl.cc
@@ -49,7 +49,7 @@ void Clone_persist_gtid::add(const Gtid_desc &gtid_desc) {
   if (!is_active() || gtid_table_persistor == nullptr) {
     return;
   }
-  ut_ad(trx_sys_mutex_own());
+
   /* Get active GTID list */
   auto &current_gtids = get_active_list();
 
diff --git a/storage/innobase/dict/dict0mem.cc b/storage/innobase/dict/dict0mem.cc
index 4c2991f8cf8..a72049502eb 100644
--- a/storage/innobase/dict/dict0mem.cc
+++ b/storage/innobase/dict/dict0mem.cc
@@ -623,8 +623,8 @@ bool dict_index_t::is_usable(const trx_t *trx) const {
 
   /* Check if the specified transaction can see this index. */
   return (table->is_temporary() || trx_id == 0 ||
-          !MVCC::is_view_active(trx->read_view) ||
-          trx->read_view->changes_visible(trx_id, table->name));
+          !trx->read_view.is_open() ||
+          trx->read_view.changes_visible(trx_id, table->name));
 }
 #endif /* !UNIV_HOTBACKUP */
 
diff --git a/storage/innobase/handler/ha_innodb.cc b/storage/innobase/handler/ha_innodb.cc
index fc194d2e493..7fba03c4676 100644
--- a/storage/innobase/handler/ha_innodb.cc
+++ b/storage/innobase/handler/ha_innodb.cc
@@ -676,6 +676,7 @@ static PSI_mutex_info all_innodb_mutexes[] = {
     PSI_MUTEX_KEY(rtr_match_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(rtr_path_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(rtr_ssn_mutex, 0, 0, PSI_DOCUMENT_ME),
+    PSI_MUTEX_KEY(rw_trx_hash_element_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(trx_sys_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(zip_pad_mutex, 0, 0, PSI_DOCUMENT_ME),
     PSI_MUTEX_KEY(master_key_id_mutex, 0, 0, PSI_DOCUMENT_ME),
@@ -2853,6 +2854,17 @@ ibool trx_is_strict(trx_t *trx) /*!< in: transaction */
           (!trx->in_truncate));
 }
 
+/** Gets current trx.
+ This function may be called during InnoDB initialisation, when
+ innodb_hton_ptr->slot is not yet set to meaningful value. */
+trx_t *current_trx() {
+  THD *thd = current_thd;
+  if (UNIV_LIKELY(thd != nullptr) && innodb_hton_ptr->slot != HA_SLOT_UNDEF) {
+    return thd_to_trx(thd);
+  }
+  return nullptr;
+}
+
 /** Resets some fields of a m_prebuilt struct. The template is used in fast
  retrieval of just those column values MySQL needs in its processing. */
 void ha_innobase::reset_template(void) {
@@ -2906,7 +2918,7 @@ void ha_innobase::init_table_handle_for_HANDLER(void) {
 
   /* Assign a read view if the transaction does not have it yet */
 
-  trx_assign_read_view(m_prebuilt->trx);
+  m_prebuilt->trx->read_view.open(m_prebuilt->trx);
 
   innobase_register_trx(ht, m_user_thd, m_prebuilt->trx);
 
@@ -4952,7 +4964,7 @@ static int innobase_init_files(dict_init_mode_t dict_init_mode,
       return innodb_init_abort();
     }
 
-    if (trx_sys->found_prepared_trx) {
+    if (trx_sys->found_prepared_trx > 0) {
       ib::error(ER_DD_UPGRADE_FOUND_PREPARED_XA_TRANSACTION);
       return innodb_init_abort();
     }
@@ -5198,7 +5210,7 @@ static int innobase_start_trx_and_assign_read_view(
       innobase_map_isolation_level(thd_get_trx_isolation(thd));
 
   if (trx->isolation_level == TRX_ISO_REPEATABLE_READ) {
-    trx_assign_read_view(trx);
+    trx->read_view.open(trx);
   } else {
     push_warning_printf(thd, Sql_condition::SL_WARNING, HA_ERR_UNSUPPORTED,
                         "InnoDB: WITH CONSISTENT SNAPSHOT"
@@ -10096,7 +10108,7 @@ int ha_innobase::sample_init(void *&scan_ctx, double sampling_percentage,
   auto trx = m_prebuilt->trx;
   innobase_register_trx(ht, ha_thd(), trx);
   trx_start_if_not_started_xa(trx, false);
-  trx_assign_read_view(trx);
+  trx->read_view.open(trx);
 
   /* Parallel read is not currently supported for sampling. */
   size_t n_threads = 1;
@@ -17820,13 +17832,8 @@ int ha_innobase::external_lock(THD *thd, /*!< in: handle to the user thread */
         ut_d(trx->is_dd_trx = false);
       }
 
-    } else if (trx->isolation_level <= TRX_ISO_READ_COMMITTED &&
-               MVCC::is_view_active(trx->read_view)) {
-      mutex_enter(&trx_sys->mutex);
-
-      trx_sys->mvcc->view_close(trx->read_view, true);
-
-      mutex_exit(&trx_sys->mutex);
+    } else if (trx->isolation_level <= TRX_ISO_READ_COMMITTED) {
+      trx->read_view.close();
     }
   }
 
@@ -18422,16 +18429,11 @@ THR_LOCK_DATA **ha_innobase::store_lock(
     trx->isolation_level =
         innobase_map_isolation_level((enum_tx_isolation)thd_tx_isolation(thd));
 
-    if (trx->isolation_level <= TRX_ISO_READ_COMMITTED &&
-        MVCC::is_view_active(trx->read_view)) {
+    if (trx->isolation_level <= TRX_ISO_READ_COMMITTED) {
       /* At low transaction isolation levels we let
       each consistent read set its own snapshot */
 
-      mutex_enter(&trx_sys->mutex);
-
-      trx_sys->mvcc->view_close(trx->read_view, true);
-
-      mutex_exit(&trx_sys->mutex);
+      trx->read_view.close();
     }
   }
 
diff --git a/storage/innobase/handler/ha_innopart.cc b/storage/innobase/handler/ha_innopart.cc
index 9f2203e7909..1f7b5438b2a 100644
--- a/storage/innobase/handler/ha_innopart.cc
+++ b/storage/innobase/handler/ha_innopart.cc
@@ -2032,7 +2032,7 @@ int ha_innopart::sample_init(void *&scan_ctx, double sampling_percentage,
   auto trx = m_prebuilt->trx;
   innobase_register_trx(ht, ha_thd(), trx);
   trx_start_if_not_started_xa(trx, false);
-  trx_assign_read_view(trx);
+  trx->read_view.open(trx);
 
   /* Parallel read is not currently supported for sampling. */
   size_t n_threads = 1;
@@ -3180,7 +3180,7 @@ int ha_innopart::records(ha_rows *num_rows) {
       trx->mysql_n_tables_locked == 0 && !m_prebuilt->ins_sel_stmt &&
       n_threads > 1) {
     trx_start_if_not_started_xa(trx, false);
-    trx_assign_read_view(trx);
+    trx->read_view.open(trx);
 
     const auto first_used_partition = m_part_info->get_first_used_partition();
 
diff --git a/storage/innobase/handler/handler0alter.cc b/storage/innobase/handler/handler0alter.cc
index d278d036856..920553e1058 100644
--- a/storage/innobase/handler/handler0alter.cc
+++ b/storage/innobase/handler/handler0alter.cc
@@ -1173,7 +1173,7 @@ int ha_innobase::parallel_scan_init(void *&scan_ctx, size_t &num_threads) {
 
   trx_start_if_not_started_xa(trx, false);
 
-  trx_assign_read_view(trx);
+  trx->read_view.open(trx);
 
   size_t n_threads = thd_parallel_read_threads(m_prebuilt->trx->mysql_thd);
 
@@ -4790,7 +4790,7 @@ static MY_ATTRIBUTE((warn_unused_result)) bool prepare_inplace_alter_table_dict(
   if (ctx->online) {
     /* Assign a consistent read view for
     row_merge_read_clustered_index(). */
-    trx_assign_read_view(ctx->prebuilt->trx);
+    ctx->prebuilt->trx->read_view.open(ctx->prebuilt->trx);
   }
 
   if (fts_index) {
@@ -9764,7 +9764,7 @@ int ha_innopart::parallel_scan_init(void *&scan_ctx, size_t &num_threads) {
 
   trx_start_if_not_started_xa(trx, false);
 
-  trx_assign_read_view(trx);
+  trx->read_view.open(trx);
 
   auto dd_client = ha_thd()->dd_client();
   dd::cache::Dictionary_client::Auto_releaser releaser(dd_client);
diff --git a/storage/innobase/handler/p_s.cc b/storage/innobase/handler/p_s.cc
index 4775fb71f54..d3e8d821d55 100644
--- a/storage/innobase/handler/p_s.cc
+++ b/storage/innobase/handler/p_s.cc
@@ -143,25 +143,17 @@ this program; if not, write to the Free Software Foundation, Inc.,
 static const char *g_engine = "INNODB";
 static const size_t g_engine_length = 6;
 
-inline trx_t *get_next_trx(const trx_t *trx, bool read_write) {
-  if (read_write) {
-    return (UT_LIST_GET_NEXT(trx_list, trx));
-  } else {
-    return (UT_LIST_GET_NEXT(mysql_trx_list, trx));
-  }
+inline trx_t *get_next_trx(const trx_t *trx) {
+  return (UT_LIST_GET_NEXT(trx_list, trx));
 }
 
 /** Pass of a given scan. */
 enum scan_pass {
   INIT_SCANNING,
-  /** Scan the RW trx list.
-  @sa trx_sys_t::rw_trx_list
-  */
-  SCANNING_RW_TRX_LIST,
-  /** Scan the MySQL trx list.
-  @sa trx_t::mysql_trx_list
+  /** Scan the trx list.
+  @sa trx_sys_t::trx_list
   */
-  SCANNING_MYSQL_TRX_LIST,
+  SCANNING_TRX_LIST,
   DONE_SCANNING
 };
 
@@ -203,18 +195,12 @@ class Innodb_trx_scan_state {
     } else {
       switch (m_scan_pass) {
         case INIT_SCANNING:
-          m_scan_pass = SCANNING_RW_TRX_LIST;
-          m_start_trx_id_range = 0;
-          m_end_trx_id_range = SCAN_RANGE;
-          m_next_trx_id_range = TRX_ID_MAX;
-          break;
-        case SCANNING_RW_TRX_LIST:
-          m_scan_pass = SCANNING_MYSQL_TRX_LIST;
+          m_scan_pass = SCANNING_TRX_LIST;
           m_start_trx_id_range = 0;
           m_end_trx_id_range = SCAN_RANGE;
           m_next_trx_id_range = TRX_ID_MAX;
           break;
-        case SCANNING_MYSQL_TRX_LIST:
+        case SCANNING_TRX_LIST:
           m_scan_pass = DONE_SCANNING;
           break;
         case DONE_SCANNING:
@@ -351,34 +337,19 @@ class Innodb_data_lock_wait_iterator
 };
 
 /** Check if a transaction should be discarded.
-Transactions present in any TRX LIST that have not started yet
+Transactions present in the trx list that have not started yet
 are discarded, when inspecting data locks.
-Transactions present in the MySQL TRX LIST,
-that are writing data and have an id, are also discarded.
 @param[in] trx Transaction to evaluate
-@param[in] read_write True if trx is in the RW TRX list
 @returns True if the trx should be discarded
 */
-bool discard_trx(const trx_t *trx, bool read_write) {
-  if (!trx_is_started(trx)) {
-    return true;
-  }
-
-  if ((!read_write && trx->id != 0 && !trx->read_only)) {
-    return true;
-  }
-
-  return false;
-}
+bool discard_trx(const trx_t *trx) { return !trx_is_started(trx); }
 
 /** Find a transaction in a TRX LIST.
-@param[in] filter_trx_immutable_id  The transaction immutable id
-@param[in] read_write	            True for the RW TRX LIST
-@param[in] trx_list	            The transaction list
+@param[in] trx_id	The transaction id
+@param[in] trx_list	The transaction list
 @returns The transaction when found, or NULL
 */
 static const trx_t *fetch_trx_in_trx_list(uint64_t filter_trx_immutable_id,
-                                          bool read_write,
                                           trx_ut_list_t *trx_list) {
   const trx_t *trx;
 
@@ -388,8 +359,8 @@ static const trx_t *fetch_trx_in_trx_list(uint64_t filter_trx_immutable_id,
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = get_next_trx(trx, read_write)) {
-    if (discard_trx(trx, read_write)) {
+       trx = get_next_trx(trx)) {
+    if (discard_trx(trx)) {
       continue;
     }
 
@@ -592,15 +563,8 @@ bool Innodb_data_lock_iterator::scan(PSI_server_data_lock_container *container,
 
   size_t found = 0;
 
-  while ((m_scan_state.get_pass() == SCANNING_RW_TRX_LIST) && (found == 0)) {
-    found =
-        scan_trx_list(container, with_lock_data, true, &trx_sys->rw_trx_list);
-    m_scan_state.prepare_next_scan();
-  }
-
-  while ((m_scan_state.get_pass() == SCANNING_MYSQL_TRX_LIST) && (found == 0)) {
-    found = scan_trx_list(container, with_lock_data, false,
-                          &trx_sys->mysql_trx_list);
+  while ((m_scan_state.get_pass() == SCANNING_TRX_LIST) && (found == 0)) {
+    found = scan_trx_list(container, with_lock_data, true, &trx_sys->trx_list);
     m_scan_state.prepare_next_scan();
   }
 
@@ -635,12 +599,7 @@ bool Innodb_data_lock_iterator::fetch(PSI_server_data_lock_container *container,
 
   trx_sys_mutex_enter();
 
-  trx = fetch_trx_in_trx_list(trx_immutable_id, true, &trx_sys->rw_trx_list);
-
-  if (trx == nullptr) {
-    trx = fetch_trx_in_trx_list(trx_immutable_id, false,
-                                &trx_sys->mysql_trx_list);
-  }
+  trx = fetch_trx_in_trx_list(trx_immutable_id, &trx_sys->trx_list);
 
   if (trx != nullptr) {
     scan_trx(container, with_lock_data, trx, true, lock_immutable_id, heap_id);
@@ -672,8 +631,8 @@ size_t Innodb_data_lock_iterator::scan_trx_list(
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = get_next_trx(trx, read_write)) {
-    if (discard_trx(trx, read_write)) {
+       trx = get_next_trx(trx)) {
+    if (discard_trx(trx)) {
       continue;
     }
 
@@ -865,13 +824,8 @@ bool Innodb_data_lock_wait_iterator::scan(
 
   size_t found = 0;
 
-  while ((m_scan_state.get_pass() == SCANNING_RW_TRX_LIST) && (found == 0)) {
-    found = scan_trx_list(container, true, &trx_sys->rw_trx_list);
-    m_scan_state.prepare_next_scan();
-  }
-
-  while ((m_scan_state.get_pass() == SCANNING_MYSQL_TRX_LIST) && (found == 0)) {
-    found = scan_trx_list(container, false, &trx_sys->mysql_trx_list);
+  while ((m_scan_state.get_pass() == SCANNING_TRX_LIST) && (found == 0)) {
+    found = scan_trx_list(container, true, &trx_sys->trx_list);
     m_scan_state.prepare_next_scan();
   }
 
@@ -921,13 +875,7 @@ bool Innodb_data_lock_wait_iterator::fetch(
 
   trx_sys_mutex_enter();
 
-  trx = fetch_trx_in_trx_list(requesting_trx_immutable_id, true,
-                              &trx_sys->rw_trx_list);
-
-  if (trx == nullptr) {
-    trx = fetch_trx_in_trx_list(requesting_trx_immutable_id, false,
-                                &trx_sys->mysql_trx_list);
-  }
+  trx = fetch_trx_in_trx_list(requesting_trx_immutable_id, &trx_sys->trx_list);
 
   if (trx != nullptr) {
     scan_trx(container, trx, true, requesting_lock_immutable_id,
@@ -958,8 +906,8 @@ size_t Innodb_data_lock_wait_iterator::scan_trx_list(
   ut_ad(trx_sys_mutex_own());
 
   for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = get_next_trx(trx, read_write)) {
-    if (discard_trx(trx, read_write)) {
+       trx = get_next_trx(trx)) {
+    if (discard_trx(trx)) {
       continue;
     }
 
diff --git a/storage/innobase/include/clone0repl.h b/storage/innobase/include/clone0repl.h
index fa7deb5d7d4..7015925eaea 100644
--- a/storage/innobase/include/clone0repl.h
+++ b/storage/innobase/include/clone0repl.h
@@ -235,7 +235,6 @@ class Clone_persist_gtid {
 
   /** @return current active GTID list */
   Gitd_info_list &get_active_list() {
-    ut_ad(trx_sys_mutex_own());
     return (get_list(m_active_number));
   }
 
@@ -292,7 +291,6 @@ class Clone_persist_gtid {
   /** Switch active GTID list. */
   uint64_t switch_active_list() {
     /* Switch active list under transaction system mutex. */
-    ut_ad(trx_sys_mutex_own());
     uint64_t flush_number = m_active_number;
     ++m_active_number;
     m_compression_gtid_counter += m_num_gtid_mem;
diff --git a/storage/innobase/include/read0read.h b/storage/innobase/include/read0read.h
index 9345bfb70c8..de1b377445f 100644
--- a/storage/innobase/include/read0read.h
+++ b/storage/innobase/include/read0read.h
@@ -33,99 +33,4 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #ifndef read0read_h
 #define read0read_h
 
-#include <stddef.h>
-#include <algorithm>
-
-#include "read0types.h"
-#include "univ.i"
-
-/** The MVCC read view manager */
-class MVCC {
- public:
-  /** Constructor
-  @param size		Number of views to pre-allocate */
-  explicit MVCC(ulint size);
-
-  /** Destructor.
-  Free all the views in the m_free list */
-  ~MVCC();
-
-  /**
-  Allocate and create a view.
-  @param view		view owned by this class created for the
-                          caller. Must be freed by calling close()
-  @param trx		transaction creating the view */
-  void view_open(ReadView *&view, trx_t *trx);
-
-  /**
-  Close a view created by the above function.
-  @param view		view allocated by trx_open.
-  @param own_mutex	true if caller owns trx_sys_t::mutex */
-  void view_close(ReadView *&view, bool own_mutex);
-
-  /**
-  Release a view that is inactive but not closed. Caller must own
-  the trx_sys_t::mutex.
-  @param view		View to release */
-  void view_release(ReadView *&view);
-
-  /** Clones the oldest view and stores it in view. No need to
-  call view_close(). The caller owns the view that is passed in.
-  It will also move the closed views from the m_views list to the
-  m_free list. This function is called by Purge to create it view.
-  @param view		Preallocated view, owned by the caller */
-  void clone_oldest_view(ReadView *view);
-
-  /**
-  @return the number of active views */
-  ulint size() const;
-
-  /**
-  @return true if the view is active and valid */
-  static bool is_view_active(ReadView *view) {
-    ut_a(view != reinterpret_cast<ReadView *>(0x1));
-
-    return (view != nullptr && !(intptr_t(view) & 0x1));
-  }
-
-  /**
-  Set the view creator transaction id. Note: This shouldbe set only
-  for views created by RW transactions. */
-  static void set_view_creator_trx_id(ReadView *view, trx_id_t id);
-
- private:
-  /**
-  Validates a read view list. */
-  bool validate() const;
-
-  /**
-  Find a free view from the active list, if none found then allocate
-  a new view. This function will also attempt to move delete marked
-  views from the active list to the freed list.
-  @return a view to use */
-  inline ReadView *get_view();
-
-  /**
-  Get the oldest view in the system. It will also move the delete
-  marked read views from the views list to the freed list.
-  @return oldest view if found or NULL */
-  inline ReadView *get_oldest_view() const;
-  ReadView *get_view_created_by_trx_id(trx_id_t trx_id) const;
-
- private:
-  // Prevent copying
-  MVCC(const MVCC &);
-  MVCC &operator=(const MVCC &);
-
- private:
-  typedef UT_LIST_BASE_NODE_T(ReadView) view_list_t;
-
-  /** Free views ready for reuse. */
-  view_list_t m_free;
-
-  /** Active and closed views, the closed views will have the
-  creator trx id set to TRX_ID_MAX */
-  view_list_t m_views;
-};
-
 #endif /* read0read_h */
diff --git a/storage/innobase/include/read0types.h b/storage/innobase/include/read0types.h
index 973f1018192..d80b5ada138 100644
--- a/storage/innobase/include/read0types.h
+++ b/storage/innobase/include/read0types.h
@@ -38,117 +38,19 @@ this program; if not, write to the Free Software Foundation, Inc.,
 
 #include "trx0types.h"
 
-// Friend declaration
-class MVCC;
+/** View is not visible to purge thread. */
+constexpr int32_t READ_VIEW_STATE_CLOSED = 0;
+
+/** View is being opened, purge thread must wait for state change. */
+constexpr int32_t READ_VIEW_STATE_SNAPSHOT = 1;
+
+/** View is visible to purge thread. */
+constexpr int32_t READ_VIEW_STATE_OPEN = 2;
 
 /** Read view lists the trx ids of those transactions for which a consistent
 read should not see the modifications to the database. */
 
 class ReadView {
-  /** This is similar to a std::vector but it is not a drop
-  in replacement. It is specific to ReadView. */
-  class ids_t {
-    typedef trx_ids_t::value_type value_type;
-
-    /**
-    Constructor */
-    ids_t() : m_ptr(), m_size(), m_reserved() {}
-
-    /**
-    Destructor */
-    ~ids_t() { UT_DELETE_ARRAY(m_ptr); }
-
-    /**
-    Try and increase the size of the array. Old elements are
-    copied across. It is a no-op if n is < current size.
-
-    @param n 		Make space for n elements */
-    void reserve(ulint n);
-
-    /**
-    Resize the array, sets the current element count.
-    @param n		new size of the array, in elements */
-    void resize(ulint n) {
-      ut_ad(n <= capacity());
-
-      m_size = n;
-    }
-
-    /**
-    Reset the size to 0 */
-    void clear() { resize(0); }
-
-    /**
-    @return the capacity of the array in elements */
-    ulint capacity() const { return (m_reserved); }
-
-    /**
-    Copy and overwrite the current array contents
-
-    @param start		Source array
-    @param end		Pointer to end of array */
-    void assign(const value_type *start, const value_type *end);
-
-    /**
-    Insert the value in the correct slot, preserving the order.
-    Doesn't check for duplicates. */
-    void insert(value_type value);
-
-    /**
-    @return the value of the first element in the array */
-    value_type front() const {
-      ut_ad(!empty());
-
-      return (m_ptr[0]);
-    }
-
-    /**
-    @return the value of the last element in the array */
-    value_type back() const {
-      ut_ad(!empty());
-
-      return (m_ptr[m_size - 1]);
-    }
-
-    /**
-    Append a value to the array.
-    @param value		the value to append */
-    void push_back(value_type value);
-
-    /**
-    @return a pointer to the start of the array */
-    trx_id_t *data() { return (m_ptr); }
-
-    /**
-    @return a const pointer to the start of the array */
-    const trx_id_t *data() const { return (m_ptr); }
-
-    /**
-    @return the number of elements in the array */
-    ulint size() const { return (m_size); }
-
-    /**
-    @return true if size() == 0 */
-    bool empty() const { return (size() == 0); }
-
-   private:
-    // Prevent copying
-    ids_t(const ids_t &);
-    ids_t &operator=(const ids_t &);
-
-   private:
-    /** Memory for the array */
-    value_type *m_ptr;
-
-    /** Number of active elements in the array */
-    ulint m_size;
-
-    /** Size of m_ptr in elements */
-    ulint m_reserved;
-
-    friend class ReadView;
-  };
-
  public:
   ReadView();
   ~ReadView();
@@ -178,9 +80,7 @@ class ReadView {
       return (true);
     }
 
-    const ids_t::value_type *p = m_ids.data();
-
-    return (!std::binary_search(p, p + m_ids.size(), id));
+    return (!std::binary_search(m_ids.begin(), m_ids.end(), id));
   }
 
   /**
@@ -188,16 +88,50 @@ class ReadView {
   @return true if view sees transaction id */
   bool sees(trx_id_t id) const { return (id < m_up_limit_id); }
 
+  /** Creates a snapshot where exactly the transaction serialized before this
+  point in time are seen in the view.
+
+  @param[in, out] trx transaction */
+  void snapshot(trx_t *trx);
+
+  /** Open a read view where exactly the transaction serialized before this
+  point in time are seen in the view.
+
+  View become visible to purge thread.
+
+  @param[in,out] trx transaction */
+  void open(trx_t *trx);
+
   /**
   Mark the view as closed */
   void close() {
-    ut_ad(m_creator_trx_id != TRX_ID_MAX);
-    m_creator_trx_id = TRX_ID_MAX;
+    int32_t state = m_state.load(std::memory_order_relaxed);
+    ut_ad(state == READ_VIEW_STATE_CLOSED || state == READ_VIEW_STATE_OPEN);
+    if (state == READ_VIEW_STATE_OPEN) {
+      m_state.store(READ_VIEW_STATE_CLOSED, std::memory_order_relaxed);
+    }
   }
 
-  /**
-  @return true if the view is closed */
-  bool is_closed() const { return (m_closed); }
+  /** m_state getter fir trx_sys::clone_oldest_view() & trx_sys::size(). */
+  int32_t get_state() const { return m_state.load(std::memory_order_acquire); }
+
+  /** Returns ture if view is open.
+
+  Only used by view owner thread, thus we can omit atomic operations. */
+  bool is_open() const {
+    int32_t state = m_state.load(std::memory_order_relaxed);
+    ut_ad(state == READ_VIEW_STATE_OPEN || state == READ_VIEW_STATE_CLOSED);
+    return (state == READ_VIEW_STATE_OPEN);
+  }
+
+  /** Set the creator transaction id.
+
+  This should be set only for views created by RW transactions. */
+  void set_creator_trx_id(trx_id_t id) {
+    ut_ad(id > 0);
+    ut_ad(m_creator_trx_id == 0);
+    m_creator_trx_id = id;
+  }
 
   /**
   Write the limits to the file.
@@ -244,35 +178,8 @@ class ReadView {
     return (m_low_limit_no <= rhs->m_low_limit_no);
   }
 #endif /* UNIV_DEBUG */
- private:
-  /**
-  Copy the transaction ids from the source vector */
-  inline void copy_trx_ids(const trx_ids_t &trx_ids);
-
-  /**
-  Opens a read view where exactly the transactions serialized before this
-  point in time are seen in the view.
-  @param id		Creator transaction id */
-  inline void prepare(trx_id_t id);
-
-  /**
-  Copy state from another view. Must call copy_complete() to finish.
-  @param other		view to copy from */
-  inline void copy_prepare(const ReadView &other);
-
-  /**
-  Complete the copy, insert the creator transaction id into the
-  m_trx_ids too and adjust the m_up_limit_id *, if required */
-  inline void copy_complete();
 
-  /**
-  Set the creator transaction id, existing id must be 0 */
-  void creator_trx_id(trx_id_t id) {
-    ut_ad(m_creator_trx_id == 0);
-    m_creator_trx_id = id;
-  }
-
-  friend class MVCC;
+  void copy(const ReadView &other);
 
  private:
   // Disable copying
@@ -295,7 +202,7 @@ class ReadView {
 
   /** Set of RW transactions that was active when this snapshot
   was taken */
-  ids_t m_ids;
+  trx_ids_t m_ids;
 
   /** The view does not need to see the undo logs for transactions
   whose transaction number is strictly smaller (<) than this value:
@@ -310,14 +217,23 @@ class ReadView {
   trx_id_t m_view_low_limit_no;
 #endif /* UNIV_DEBUG */
 
-  /** AC-NL-RO transaction view that has been "closed". */
-  bool m_closed;
+  /** View state.
+
+  It is not defined as enum as it has to be updated using atomic operations.
+  Possible values are READ_VIEW_STATE_CLOSED, READ_VIEW_STATE_SNAPSHOT and
+  READ_VIEW_STATE_OPEN.
+
+  Possible state transfers...
+
+  Start view open:
+  READ_VIEW_STATE_CLOSED -> READ_VIEW_STATE_SNAPSHOT
 
-  typedef UT_LIST_NODE_T(ReadView) node_t;
+  Complete view open:
+  READ_VIEW_STATE_SNAPSHOT -> READ_VIEW_STATE_OPEN
 
-  /** List of read views in trx_sys */
-  byte pad1[64 - sizeof(node_t)];
-  node_t m_view_list;
+  Close view:
+  READ_VIEW_STATE_OPEN -> READ_VIEW_STATE_CLOSED */
+  std::atomic<int32_t> m_state;
 };
 
 #endif
diff --git a/storage/innobase/include/row0vers.h b/storage/innobase/include/row0vers.h
index b6cc006b126..21936045a47 100644
--- a/storage/innobase/include/row0vers.h
+++ b/storage/innobase/include/row0vers.h
@@ -48,16 +48,17 @@ class ReadView;
 
 /** Finds out if an active transaction has inserted or modified a secondary
  index record.
- @param[in]   rec       record in a secondary index
- @param[in]   index     the secondary index
- @param[in]   offsets   rec_get_offsets(rec, index)
+ @param[in/out]  caller_trx  trx of current thread
+ @param[in]      rec         record in a secondary index
+ @param[in]      index       the secondary index
+ @param[in]      offsets     rec_get_offsets(rec, index)
  @return 0 if committed, else the active transaction id;
  NOTE that this function can return false positives but never false
  negatives. The caller must confirm all positive results by checking if the trx
  is still active.
 */
-trx_t *row_vers_impl_x_locked(const rec_t *rec, const dict_index_t *index,
-                              const ulint *offsets);
+trx_t *row_vers_impl_x_locked(trx_t *caller_trx, const rec_t *rec,
+                              const dict_index_t *index, const ulint *offsets);
 
 /** Finds out if we must preserve a delete marked earlier version of a clustered
  index record, because it is >= the purge view.
@@ -119,6 +120,7 @@ dberr_t row_vers_build_for_consistent_read(
 /** Constructs the last committed version of a clustered index record,
  which should be seen by a semi-consistent read. */
 void row_vers_build_for_semi_consistent_read(
+    trx_t *caller_trx,        /*!< in/out: trx of current thread */
     const rec_t *rec,         /*!< in: record in a clustered index; the
                               caller must have a latch on the page; this
                               latch locks the top of the stack of versions
diff --git a/storage/innobase/include/sync0sync.h b/storage/innobase/include/sync0sync.h
index cab905f8114..c0d38bb786f 100644
--- a/storage/innobase/include/sync0sync.h
+++ b/storage/innobase/include/sync0sync.h
@@ -158,6 +158,7 @@ extern mysql_pfs_key_t temp_pool_manager_mutex_key;
 extern mysql_pfs_key_t lock_sys_page_mutex_key;
 extern mysql_pfs_key_t lock_sys_table_mutex_key;
 extern mysql_pfs_key_t lock_wait_mutex_key;
+extern mysql_pfs_key_t rw_trx_hash_element_mutex_key;
 extern mysql_pfs_key_t trx_sys_mutex_key;
 extern mysql_pfs_key_t srv_sys_mutex_key;
 extern mysql_pfs_key_t srv_threads_mutex_key;
diff --git a/storage/innobase/include/sync0types.h b/storage/innobase/include/sync0types.h
index 31a42c5c98c..a90365fad96 100644
--- a/storage/innobase/include/sync0types.h
+++ b/storage/innobase/include/sync0types.h
@@ -274,6 +274,7 @@ enum latch_level_t {
   SYNC_TRX_SYS_HEADER,
   SYNC_THREADS,
   SYNC_TRX,
+  SYNC_RW_TRX_HASH_ELEMENT,
   SYNC_POOL,
   SYNC_POOL_MANAGER,
   SYNC_TRX_SYS,
@@ -462,6 +463,7 @@ enum latch_id_t {
   LATCH_ID_CLONE_SYS,
   LATCH_ID_CLONE_TASK,
   LATCH_ID_CLONE_SNAPSHOT,
+  LATCH_ID_RW_TRX_HASH_ELEMENT,
   LATCH_ID_PARALLEL_READ,
   LATCH_ID_DBLR,
   LATCH_ID_REDO_LOG_ARCHIVE_ADMIN_MUTEX,
diff --git a/storage/innobase/include/trx0roll.h b/storage/innobase/include/trx0roll.h
index 83c13a9dbf3..26f7760864d 100644
--- a/storage/innobase/include/trx0roll.h
+++ b/storage/innobase/include/trx0roll.h
@@ -64,10 +64,10 @@ trx_undo_rec_t *trx_roll_pop_top_rec_of_trx(
 /** Rollback or clean up any incomplete transactions which were
  encountered in crash recovery.  If the transaction already was
  committed, then we clean up a possible insert undo log. If the
- transaction was not yet committed, then we roll it back. */
-void trx_rollback_or_clean_recovered(
-    ibool all); /*!< in: FALSE=roll back dictionary transactions;
-                TRUE=roll back all non-PREPARED transactions */
+ transaction was not yet committed, then we roll it back.
+ @param all true=roll back all recovered active transactions;
+ false=roll back any incomplete dictionary transaction */
+void trx_rollback_recovered(bool all);
 
 /** Rollback or clean up any incomplete transactions which were
 encountered in crash recovery.  If the transaction already was
diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index 49aed07851c..6bc643b4950 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -35,6 +35,8 @@ this program; if not, write to the Free Software Foundation, Inc.,
 
 #include "univ.i"
 
+#include "lf.h"
+
 #include "buf0buf.h"
 #include "fil0fil.h"
 #include "trx0types.h"
@@ -54,7 +56,6 @@ this program; if not, write to the Free Software Foundation, Inc.,
 typedef UT_LIST_BASE_NODE_T(trx_t) trx_ut_list_t;
 
 // Forward declaration
-class MVCC;
 class ReadView;
 
 /** The transaction system */
@@ -130,16 +131,6 @@ UNIV_INLINE
 void trx_sysf_rseg_set_page_no(trx_sysf_t *sys_header, ulint i,
                                page_no_t page_no, mtr_t *mtr);
 
-/** Allocates a new transaction id.
- @return new, allocated trx id */
-UNIV_INLINE
-trx_id_t trx_sys_get_new_trx_id();
-/** Determines the maximum transaction id.
- @return maximum currently allocated trx id; will be stale after the
- next call to trx_sys_get_new_trx_id() */
-UNIV_INLINE
-trx_id_t trx_sys_get_max_trx_id(void);
-
 #ifdef UNIV_DEBUG
 /* Flag to control TRX_RSEG_N_SLOTS behavior debugging. */
 extern uint trx_rseg_n_slots_debug;
@@ -162,54 +153,6 @@ UNIV_INLINE
 trx_id_t trx_read_trx_id(
     const byte *ptr); /*!< in: pointer to memory from where to read */
 
-/** Looks for the trx handle with the given id in rw trxs list.
- The caller must be holding trx_sys->mutex.
- @param[in]   trx_id   trx id to search for
- @return the trx handle or NULL if not found */
-UNIV_INLINE
-trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id);
-
-/** Returns the minimum trx id in rw trx list. This is the smallest id for which
- the trx can possibly be active. (But, you must look at the trx->state to
- find out if the minimum trx id transaction itself is active, or already
- committed.)
- @return the minimum trx id, or trx_sys->max_trx_id if the trx list is empty */
-UNIV_INLINE
-trx_id_t trx_rw_min_trx_id(void);
-
-/** Checks if a rw transaction with the given id is active.
-@param[in]	trx_id		trx id of the transaction
-@param[in]	corrupt		NULL or pointer to a flag that will be set if
-                                corrupt
-@return transaction instance if active, or NULL */
-UNIV_INLINE
-trx_t *trx_rw_is_active_low(trx_id_t trx_id, ibool *corrupt);
-
-/** Checks if a rw transaction with the given id is active.
-Please note, that positive result means only that the trx was active
-at some moment during the call, but it might have already become
-TRX_STATE_COMMITTED_IN_MEMORY before the call returns to the caller, as this
-transition is protected by trx->mutex and trx_sys->mutex, but it is impossible
-for the caller to hold any of these mutexes when calling this function as the
-function itself internally acquires trx_sys->mutex which would cause recurrent
-mutex acquisition if caller already had trx_sys->mutex, or latching order
-violation in case of holding trx->mutex.
-@param[in]	trx_id		trx id of the transaction
-@param[in]	corrupt		NULL or pointer to a flag that will be set if
-                                corrupt
-@param[in]	do_ref_count	if true then increment the trx_t::n_ref_count
-@return transaction instance if active, or NULL; */
-UNIV_INLINE
-trx_t *trx_rw_is_active(trx_id_t trx_id, ibool *corrupt, bool do_ref_count);
-
-#if defined UNIV_DEBUG || defined UNIV_BLOB_LIGHT_DEBUG
-/** Assert that a transaction has been recovered.
- @return true */
-UNIV_INLINE
-ibool trx_assert_recovered(trx_id_t trx_id) /*!< in: transaction identifier */
-    MY_ATTRIBUTE((warn_unused_result));
-#endif /* UNIV_DEBUG || UNIV_BLOB_LIGHT_DEBUG */
-
 /** Persist transaction number limit below which all transaction GTIDs
 are persisted to disk table.
 @param[in]	gtid_trx_no	transaction number */
@@ -248,28 +191,7 @@ void trx_sys_update_mysql_binlog_offset(trx_t *trx, mtr_t *mtr);
 
 /** Shutdown/Close the transaction system. */
 void trx_sys_close(void);
-
-/** Determine if there are incomplete transactions in the system.
-@return whether incomplete transactions need rollback */
-UNIV_INLINE
-bool trx_sys_need_rollback();
-
-/*********************************************************************
-Check if there are any active (non-prepared) transactions.
-@return total number of active transactions or 0 if none */
-ulint trx_sys_any_active_transactions(void);
 #endif /* !UNIV_HOTBACKUP */
-/**
-Add the transaction to the RW transaction set
-@param trx		transaction instance to add */
-UNIV_INLINE
-void trx_sys_rw_trx_add(trx_t *trx);
-
-#ifdef UNIV_DEBUG
-/** Validate the trx_sys_t::rw_trx_list.
- @return true if the list is valid */
-bool trx_sys_validate_trx_list();
-#endif /* UNIV_DEBUG */
 
 /** Initialize trx_sys_undo_spaces, called once during srv_start(). */
 void trx_sys_undo_spaces_init();
@@ -423,58 +345,235 @@ class Space_Ids : public std::vector<space_id_t, ut_allocator<space_id_t>> {
 };
 
 #ifndef UNIV_HOTBACKUP
+trx_t *current_trx();
+
+struct rw_trx_hash_element_t {
+  rw_trx_hash_element_t() : trx(nullptr) {
+    mutex_create(LATCH_ID_RW_TRX_HASH_ELEMENT, &mutex);
+  }
+
+  ~rw_trx_hash_element_t() { mutex_free(&mutex); }
+
+  /* lf_hash_init() relies on this to be first in the struct. */
+  trx_id_t id = 0;
+
+  std::atomic<trx_id_t> no;
+
+  trx_t *trx;
+
+  ib_mutex_t mutex;
+};
+
+/** Wrapper around LF_HASH to store set of in-memory read-write transactions. */
+class rw_trx_hash_t {
+  LF_HASH hash;
+
+  /** Constructor callback for lock-free allocator.
+
+  Object is just allocated and is not yet accessible via rw_trx_hash by
+  concurrent threads. Object can be reused multiple times before it is freed.
+  Every time object is being reused initialize() callback is called. */
+  static void rw_trx_hash_constructor(uchar *arg);
+
+  /** Destructor callback for lock-free allocator.
+
+  Object is about to be freed and is not accessible via rw_trx_hash by
+  concurrent threads. */
+  static void rw_trx_hash_destructor(uchar *arg);
+
+  /** Destructor callback for lock-free allocator.
+
+  This destructor is used at shutdown. It frees remaining transaction objects.
+
+  XA PREPARED transactions may remain if they haven't been committed or rolled
+  back. ACTIVE transactions may remain if startup was interrupted or server is
+  running in read-only mode or for certain srv_force_recovery levels. */
+  static void rw_trx_hash_shutdown_destructor(uchar *arg);
+
+  /** Initializer callback for lock-free hash.
+
+  Object is not yet accessible via rw_trx_hash by concurrent threads, but is
+  about to become such. Object id can be changed only by this callback and
+  remains the same until all pins to this object are released.
+
+  Object trx can be changed to 0 by erase() under object mutex protection,
+  which indicates it is about to be removed from lock-free hash and become not
+  accessible by concurrent threads. */
+  static void rw_trx_hash_initialize(rw_trx_hash_element_t *element,
+                                     trx_t *trx);
+
+  /** Gets LF_HASH pins.
+
+  Pins are used to protect object from being destroyed or reused. They are
+  normally stored in trx object for quick access. If caller doesn't have trx
+  available, we try to get it using current_trx(). If caller doesn't have trx at
+  all, temporary pins are allocated. */
+  LF_PINS *get_pins(trx_t *trx);
+
+  struct eliminate_duplicates_arg {
+    trx_ids_t ids;
+    lf_hash_walk_func *action;
+    void *argument;
+
+    eliminate_duplicates_arg(size_t size, lf_hash_walk_func *act, void *arg)
+        : action(act), argument(arg) {
+      ids.reserve(size);
+    }
+  };
+
+  static bool eliminate_duplicates(rw_trx_hash_element_t *element,
+                                   eliminate_duplicates_arg *arg);
+
+#ifdef UNIV_DEBUG
+  static void validate_element(trx_t *trx);
+
+  struct debug_iterator_arg {
+    lf_hash_walk_func *action;
+    void *argument;
+  };
+
+  static bool debug_iterator(rw_trx_hash_element_t *element,
+                             debug_iterator_arg *arg);
+#endif /* UNIV_DEBUG */
+
+ public:
+  void init();
+
+  void destroy();
+
+  /** Releases LF_HASH pins.
+
+  Must be called by thread that owns trx_t object when the later is being
+  "detached" from thread (e.g. released to the pool by trx_free()). Can be
+  called earlier if thread is expected not to use rw_trx_hash.
+
+  Since pins are not allowed to be transferred to another thread,
+  initialisation thread calls this for recovered transactions. */
+  void put_pins(trx_t *trx);
+
+  /** Finds trx object in lock-free hash with given id.
+
+  Only ACTIVE or PREPARED trx objects may participate in hash. Nevertheless the
+  transaction may get committed before this method returns.
+
+  With do_ref_count == false the caller may dereference returned trx pointer
+  only if lock_sys.mutex was acquired before calling find().
+
+  With do_ref_count == true caller dereferemce trx even if it is not holding
+  lock_sys.mutex. Caller is responsible for calling trx->release_reference()
+  when it is done playing with trx.
+
+  Ideally this method should get caller rw_trx_hash_pins along with trx object
+  as a parameter, similar to insert() and erase(). However most callers lose trx
+  early in their call chains and it is not that easy to pass them through.
+
+  So we take more expensive approach: get trx through current_thd()->ha_data.
+  Some threads don't have trx attached to THD, and at least server
+  initialisation thread, fts_optimize_thread, srv_master_thread,
+  dict_stats_thread, srv_monitor_thread, btr_defragment_thread don't even have
+  THD at all. For such cases we allocate pins only for duration of search and
+  free them immediately.
+
+  This has negative performance impact and should be fixed eventually (by
+  passing caller_trx as a parameter). Still stream of DML is more or less Ok.
+
+  @return pointer to trx or nullptr if not found */
+  trx_t *find(trx_t *caller_trx, trx_id_t trx_id, bool do_ref_count);
+
+  /** Inserts trx to lock-free hash.
+
+  Object becomes accessible via rw_trx_hash. */
+  void insert(trx_t *trx);
+
+  /** Removes trx from lock-free hash.
+
+  Object becomes not accessible via rw_trx_hash. But it still can be pinned by
+  concurrent find(), which is supposed to release it immediately after it sees
+  object trx is nullptr. */
+  void erase(trx_t *trx);
+
+  /** Returns the number of elements in the hash.
+
+  The number is exact only if hash is protected against concurrent modifications
+  (e.g., single threaded startup or hash is protected by some mutex). Otherwise
+  the number maybe used as a hint only, because it may change even before this
+  method returns. */
+  uint32_t size();
+
+  /** Iterates the hash.
+
+  @param caller_trx used to get/set pins
+  @param action     called for every element in hash
+  @param argument   opque argument passed to action
+
+  May return the same element multiple times if hash is under contention. If
+  caller doesn't like to see the same transaction multiple times, it has to call
+  iterate_no_dups() instead.
+
+  May return element with committed transaction. If caller doesn't like to see
+  committed transactions, it has to skip those under element mutex:
+
+    mutex_enter(&element->mutex);
+    trx_t *trx = element->trx;
+    if (trx != nullptr) {
+      // trx is protected against commit in this branch
+    }
+    mutex_exit(&element->mutex);
+
+  May miss concurrently inserted transactions.
+
+  @return 0 if iteration completed successfuly, or 1 if iteration was
+  interrupted (action returned true) */
+  int iterate(trx_t *caller_trx, const lf_hash_walk_func *action, const void *argument);
+
+  int iterate(const lf_hash_walk_func *action, const void *argument);
+
+  /** Iterates the hash and eliminates duplicate elements.
+
+  @sa iterate() */
+  int iterate_no_dups(trx_t *caller_trx, lf_hash_walk_func *action,
+                      void *argument);
+
+  int iterate_no_dups(lf_hash_walk_func *action, void *argument);
+};
+
 /** The transaction system central memory data structure. */
 struct trx_sys_t {
-  TrxSysMutex mutex; /*!< mutex protecting most fields in
-                     this structure except when noted
-                     otherwise */
-
-  MVCC *mvcc;                   /*!< Multi version concurrency control
-                                manager */
-  volatile trx_id_t max_trx_id; /*!< The smallest number not yet
-                                assigned as a transaction id or
-                                transaction number. This is declared
-                                volatile because it can be accessed
-                                without holding any mutex during
-                                AC-NL-RO view creation. */
-  std::atomic<trx_id_t> min_active_id;
-  /*!< Minimal transaction id which is
-  still in active state. */
-  trx_ut_list_t serialisation_list;
-  /*!< Ordered on trx_t::no of all the
-  currenrtly active RW transactions */
-#ifdef UNIV_DEBUG
-  trx_id_t rw_max_trx_no; /*!< Max trx number of read-write
-                          transactions added for purge. */
-#endif                    /* UNIV_DEBUG */
-
-  char pad1[64];             /*!< To avoid false sharing */
-  trx_ut_list_t rw_trx_list; /*!< List of active and committed in
-                             memory read-write transactions, sorted
-                             on trx id, biggest first. Recovered
-                             transactions are always on this list. */
-
-  char pad2[64];                /*!< To avoid false sharing */
-  trx_ut_list_t mysql_trx_list; /*!< List of transactions created
-                                for MySQL. All user transactions are
-                                on mysql_trx_list. The rw_trx_list
-                                can contain system transactions and
-                                recovered transactions that will not
-                                be in the mysql_trx_list.
-                                mysql_trx_list may additionally contain
-                                transactions that have not yet been
-                                started in InnoDB. */
-
-  trx_ids_t rw_trx_ids; /*!< Array of Read write transaction IDs
-                        for MVCC snapshot. A ReadView would take
-                        a snapshot of these transactions whose
-                        changes are not visible to it. We should
-                        remove transactions from the list before
-                        committing in memory and releasing locks
-                        to ensure right order of removal and
-                        consistent snapshot. */
-
-  char pad3[64]; /*!< To avoid false sharing */
+ private:
+  /** To avoid false sharing */
+  char pad1[ut::INNODB_CACHE_LINE_SIZE];
+  /** The smallest number not yet assigned as a transaction id or transaction
+  number. Accessed and updated with atomic operations. */
+  std::atomic<trx_id_t> max_trx_id;
+
+  /** To avoid false sharing */
+  char pad2[ut::INNODB_CACHE_LINE_SIZE];
+  /** Solves race conditions between register_rw() and snapshot_ids() as well as
+  race condition between assign_new_trx_no() and snapshot_ids().
+
+  @sa register_rw()
+  @sa assign_new_trx_no()
+  @sa snapshot_ids() */
+  std::atomic<trx_id_t> rw_trx_hash_version;
+
+ public:
+  /** To avoid false sharing */
+  char pad3[ut::INNODB_CACHE_LINE_SIZE];
+  /** Mutex protecting trx list. */
+  mutable TrxSysMutex mutex;
+
+  /** To avoid false sharing */
+  char pad4[ut::INNODB_CACHE_LINE_SIZE];
+  /** List of all transactions. */
+  trx_ut_list_t trx_list;
+
+  /** To avoid false sharing */
+  char pad5[ut::INNODB_CACHE_LINE_SIZE];
+  /** Lock-free hash of in-memory read-write transactions. Works faster when
+  it's on it's own cache line (tested). */
+  rw_trx_hash_t rw_trx_hash;
+
+  char pad6[ut::INNODB_CACHE_LINE_SIZE]; /*!< To avoid false sharing */
 
   Rsegs rsegs; /*!< Vector of pointers to rollback
                segments. These rsegs are iterated
@@ -492,20 +591,259 @@ struct trx_sys_t {
                    read-only during multi-threaded
                    operation. */
 
-  ulint rseg_history_len;
+  std::atomic<ulint> rseg_history_len;
   /*!< Length of the TRX_RSEG_HISTORY
   list (update undo logs for committed
   transactions), protected by
   rseg->mutex */
 
-  TrxIdSet rw_trx_set; /*!< Mapping from transaction id
-                       to transaction instance */
+  /** To avoid false sharing */
+  char pad7[ut::INNODB_CACHE_LINE_SIZE];
+#ifdef UNIV_DEBUG
+  std::atomic<trx_id_t> rw_max_trx_no; /*!< Max trx number of read-write
+                          transactions added for purge. */
+#endif                                 /* UNIV_DEBUG */
+
+  /** Returns the minimum trx id in rw trx list.
+
+  This is the smallest id for which the trx can possibly be active. (But, you
+  must look at trx->state to find out if the minimum trx id transaction itself
+  is active, or already committed.
+
+  @return the minimum trx id, or max_trx_id if the trx list is empty */
+  trx_id_t get_min_trx_id() {
+    trx_id_t id = get_max_trx_id();
+    rw_trx_hash.iterate(
+        reinterpret_cast<lf_hash_walk_func *>(get_min_trx_id_callback), &id);
+    return id;
+  }
+
+  /** Determines the maximum transaction id.
+
+  @return maximum currently allocated trx id; will be stale after the next call
+  to trx_sys.assign_new_trx_no */
+  trx_id_t get_max_trx_id() {
+    return max_trx_id.load(std::memory_order_relaxed);
+  }
+
+  /** Allocates and assigns new transaction serialisation number.
+
+  There's a gap between max_trx_id increment and transaction serialisation
+  number becoming visible through rw_trx_hash. While we're in this gap
+  concurrent thread may come and do MVCC snapshot without seeing allocated but
+  not yet assigned serialisation number. Then at some point purge thread may
+  clone this view. As a result it won't see newly allocated serialisation number
+  and may remove "unnecessary" history data of this transaction from rollback
+  segments.
+
+  rw_trx_hash_version is intended to solve this problem. MVCC snapshot has to
+  wait until max_trx_id == rw_trx_hash_version, which effectively means that all
+  transaction serialisation numbers up to max_trx_id are available through
+  rw_trx_hash.
+
+  We rely on refresh_rw_trx_hash_version() to issue RELEASE memory barrier so
+  that rw_trx_hash_version increment happens after trx->rw_trx_hash_element->no
+  becomes available visible through rw_trx_hash.
+
+  @param trx transaction */
+  void assign_new_trx_no(trx_t *trx) {
+    trx->no = get_new_trx_id_no_refresh();
+    trx->rw_trx_hash_element->no.store(trx->no, std::memory_order_relaxed);
+    refresh_rw_trx_hash_version();
+  }
+
+  /** Takes MVCC snapshot.
+
+  To reduce malloc probability we reserve rw_trx_hash.size() + 32 elements in
+  ids.
+
+  For details about get_rw_trx_hash_version() != get_max_trx_id() spin
+  @sa register_rw() and @sa assign_new_trx_no().
+
+  We rely on get_rw_trx_hash_version() to issue ACQUIRE memory barrier so that
+  loading of rw_trx_hash_version happens before accessing rw_trx_hash.
+
+  To optimise snapshot creation rw_trx_hash.iterate is being used instead of
+  rw_trx_hash.iterate_no_dups(). It means that some transaction identifiers may
+  appear multiple times in ids.
+
+  @param[in,out] caller_trx used to get access to rw_trx_hash_pins
+  @param[out]    ids        array to store registered transaction identifiers
+  @param[out]    max_trx_id variable to store max_trx_id value
+  @param[out]    mix_trx_no variable to store min(trx->no) value */
+  void snapshot_ids(trx_t *caller_trx, trx_ids_t *ids, trx_id_t *max_trx_id,
+                    trx_id_t *min_trx_no) {
+    ut_ad(!mutex_own(&mutex));
+    snapshot_ids_arg arg(ids);
+
+    while ((arg.id = get_rw_trx_hash_version()) != get_max_trx_id()) {
+      ut_delay(1);
+    }
+    arg.no = arg.id;
+
+    uint32_t hashSize = rw_trx_hash.size();
+    ids->clear();
+
+    if (hashSize != 0) {
+        ids->reserve(hashSize + 32);
+        rw_trx_hash.iterate(
+            caller_trx, reinterpret_cast<lf_hash_walk_func *>(copy_one_id), &arg);
+    }
+
+    *max_trx_id = arg.id;
+    *min_trx_no = arg.no;
+  }
+
+  /** Initialiser for max_trx_id and rw_trx_hash_version. */
+  void init_max_trx_id(trx_id_t value) {
+    max_trx_id = rw_trx_hash_version = value;
+  }
+
+  /** @return total number of active (non-prepared) transactions */
+  ulint any_active_transactions();
+
+  /** Registers read-write transaction.
+
+  Transaction becomes visible to MVCC.
+
+  There's a gap between max_trx_id increment and transaction becoming visible
+  through rw_trx_hash. While we're in this gap concurrent thread may come and do
+  MVCC snapshot. As a result concurrent readview will be able to observe records
+  owned by this transaction even before it is committed.
+
+  rw_trx_hash_version is intendded to solve this problem. MVCC snapshot has to
+  wait until max_trx_id == rw_trx_hash_version, which effectively means that all
+  transactions up to max_trx_id are available through rw_trx_hash.
+
+  We rely on refresh_rw_trx_hash_version() to issue RELEASE memory barrier so
+  that rw_trx_hash_version increment happens after transaction becomes visible
+  through rw_trx_hash. */
+  void register_rw(trx_t *trx) {
+    trx->id = get_new_trx_id_no_refresh();
+    rw_trx_hash.insert(trx);
+    refresh_rw_trx_hash_version();
+  }
+
+  /** For replica only, registers a faked read-write transaction. */
+  void register_rw_replica(trx_t *trx) {
+    /* trx->id and trx_sys->max_trx_id were already set. */
+    rw_trx_hash.insert(trx);
+    rw_trx_hash_version.store(get_max_trx_id(), std::memory_order_release);
+  }
+
+  /** Deregisters read-write transaction.
+
+  Transaction is removed from rw_trx_hash, which releases all implicit locks.
+  MVCC snapshot won't see this transaction anymore. */
+  void deregister_rw(trx_t *trx) { rw_trx_hash.erase(trx); }
+
+  bool is_registered(trx_t *caller_trx, trx_id_t id) {
+    return (id > 0) && (find(caller_trx, id, false) != nullptr);
+  }
+
+  trx_t *find(trx_t *caller_trx, trx_id_t id, bool do_ref_count = true) {
+    return rw_trx_hash.find(caller_trx, id, do_ref_count);
+  }
+
+  /** Registers transaction in trx_sys.
+
+  @param trx transaction */
+  void register_trx(trx_t *trx) {
+    mutex_enter(&mutex);
+    UT_LIST_ADD_FIRST(trx_list, trx);
+    mutex_exit(&mutex);
+  }
+
+  /** Deregisters transaction in trx_sys.
+
+  @param trx transaction */
+  void deregister_trx(trx_t *trx) {
+    mutex_enter(&mutex);
+    UT_LIST_REMOVE(trx_list, trx);
+    mutex_exit(&mutex);
+  }
+
+  /** Clones the oldest view and stores it in view.
+
+  No need to call ReadView::close(). The caller owns the view that is passed in.
+  This function is called by purge thread to determine whether it should purge
+  the delete marked record or not. */
+  void clone_oldest_view(ReadView *view = NULL);
+
+  /** @return the number of active views. */
+  size_t view_count() const {
+    size_t count = 0;
+
+    mutex_enter(&mutex);
+    for (const trx_t *trx = UT_LIST_GET_FIRST(trx_list); trx != nullptr;
+         trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+      if (trx->read_view.get_state() == READ_VIEW_STATE_OPEN) {
+        ++count;
+      }
+    }
+    mutex_exit(&mutex);
+    return count;
+  }
+
+  ulint n_prepared_trx(); /*!< Return number of transactions currently
+                          in the XA PREPARED state. */
+  bool found_prepared_trx; /*!< True if XA PREPARED transactions are found. */
+
+ private:
+  static bool get_min_trx_id_callback(rw_trx_hash_element_t *element,
+                                      trx_id_t *id) {
+    if (element->id < *id) {
+      mutex_enter(&element->mutex);
+      /* We don't care about read-only transactions here. */
+      if (element->trx != nullptr &&
+          element->trx->rsegs.m_redo.rseg != nullptr) {
+        *id = element->id;
+      }
+      mutex_exit(&element->mutex);
+    }
+    return false;
+  }
+
+  struct snapshot_ids_arg {
+    snapshot_ids_arg(trx_ids_t *_ids) : ids(_ids), id(0), no(0) {}
+
+    trx_ids_t *ids;
+    trx_id_t id;
+    trx_id_t no;
+  };
+
+  static bool copy_one_id(rw_trx_hash_element_t *element,
+                          snapshot_ids_arg *arg) {
+    if (element->id < arg->id) {
+      trx_id_t no = element->no.load(std::memory_order_relaxed);
+      arg->ids->push_back(element->id);
+      if (no < arg->no) {
+        arg->no = no;
+      }
+    }
+    return false;
+  }
+
+  /** Get for rw_trx_hash_version, must issue ACQUIRE memory barrier. */
+  trx_id_t get_rw_trx_hash_version() {
+    return rw_trx_hash_version.load(std::memory_order_acquire);
+  }
+
+  /** Increments rw_trx_hash_version, must issue RELEASE memory barrier. */
+  void refresh_rw_trx_hash_version() {
+    rw_trx_hash_version.fetch_add(1, std::memory_order_release);
+  }
+
+  /** Allocates new transaction id without refreshing rw_trx_hash_version.
+
+  This method is extracted for exclusive use by register_rw() and
+  assign_new_trx_no() where new id must be allocated atomically with payload
+  of these methods from MVCC snapshot point of view.
 
-  ulint n_prepared_trx; /*!< Number of transactions currently
-                        in the XA PREPARED state */
+  @sa assign_new_trx_no()
 
-  bool found_prepared_trx; /*!< True if XA PREPARED trxs are
-                           found. */
+  @return new transaction id */
+  trx_id_t get_new_trx_id_no_refresh();
 };
 
 #endif /* !UNIV_HOTBACKUP */
diff --git a/storage/innobase/include/trx0sys.ic b/storage/innobase/include/trx0sys.ic
index 3dab5984b1c..1977cac3907 100644
--- a/storage/innobase/include/trx0sys.ic
+++ b/storage/innobase/include/trx0sys.ic
@@ -179,199 +179,4 @@ trx_id_t trx_read_trx_id(
   return (mach_read_from_6(ptr));
 }
 
-UNIV_INLINE
-trx_t *trx_get_rw_trx_by_id(trx_id_t trx_id) {
-  ut_ad(trx_id > 0);
-  ut_ad(trx_sys_mutex_own());
-
-  if (trx_sys->rw_trx_set.empty()) {
-    return (nullptr);
-  }
-
-  TrxIdSet::iterator it;
-
-  it = trx_sys->rw_trx_set.find(TrxTrack(trx_id));
-
-  return (it == trx_sys->rw_trx_set.end() ? nullptr : it->m_trx);
-}
-
-/** Returns the minimum trx id in trx list. This is the smallest id for which
- the trx can possibly be active. (But, you must look at the trx->state
- to find out if the minimum trx id transaction itself is active, or already
- committed.). The caller must be holding the trx_sys_t::mutex in shared mode.
- @return the minimum trx id, or trx_sys->max_trx_id if the trx list is empty */
-UNIV_INLINE
-trx_id_t trx_rw_min_trx_id_low(void) {
-  trx_id_t id;
-
-  ut_ad(trx_sys_mutex_own());
-
-  const trx_t *trx = UT_LIST_GET_LAST(trx_sys->rw_trx_list);
-
-  if (trx == nullptr) {
-    id = trx_sys->max_trx_id;
-  } else {
-    assert_trx_in_rw_list(trx);
-    id = trx->id;
-  }
-
-  return (id);
-}
-
-#if defined UNIV_DEBUG || defined UNIV_BLOB_LIGHT_DEBUG
-/** Assert that a transaction has been recovered.
- @return true */
-UNIV_INLINE
-ibool trx_assert_recovered(trx_id_t trx_id) /*!< in: transaction identifier */
-{
-  const trx_t *trx;
-
-  trx_sys_mutex_enter();
-
-  trx = trx_get_rw_trx_by_id(trx_id);
-  ut_a(trx->is_recovered);
-
-  trx_sys_mutex_exit();
-
-  return (TRUE);
-}
-#endif /* UNIV_DEBUG || UNIV_BLOB_LIGHT_DEBUG */
-
-/** Returns the minimum trx id in rw trx list. This is the smallest id for which
- the rw trx can possibly be active. (But, you must look at the trx->state
- to find out if the minimum trx id transaction itself is active, or already
- committed.)
- @return the minimum trx id, or trx_sys->max_trx_id if rw trx list is empty */
-UNIV_INLINE
-trx_id_t trx_rw_min_trx_id(void) {
-  trx_sys_mutex_enter();
-
-  trx_id_t id = trx_rw_min_trx_id_low();
-
-  trx_sys_mutex_exit();
-
-  return (id);
-}
-
-UNIV_INLINE
-trx_t *trx_rw_is_active_low(trx_id_t trx_id, ibool *corrupt) {
-  trx_t *trx;
-
-  ut_ad(trx_sys_mutex_own());
-
-  if (trx_id < trx_rw_min_trx_id_low()) {
-    trx = nullptr;
-  } else if (trx_id >= trx_sys->max_trx_id) {
-    /* There must be corruption: we let the caller handle the
-    diagnostic prints in this case. */
-
-    trx = nullptr;
-    if (corrupt != nullptr) {
-      *corrupt = TRUE;
-    }
-  } else {
-    trx = trx_get_rw_trx_by_id(trx_id);
-    /* We remove trx from rw trxs list and change state to
-    TRX_STATE_COMMITTED_IN_MEMORY in a same critical section protected by
-    trx_sys->mutex, which we happen to hold here, so we expect the state of trx
-    to match its presence in that list */
-    ut_ad(trx == nullptr || !trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY));
-  }
-
-  return (trx);
-}
-
-UNIV_INLINE
-trx_t *trx_rw_is_active(trx_id_t trx_id, ibool *corrupt, bool do_ref_count) {
-  trx_t *trx;
-
-  /* Fast checking. If it's smaller than minimal active trx id, just
-  return NULL. */
-  if (trx_sys->min_active_id.load() > trx_id) {
-    return (nullptr);
-  }
-
-  trx_sys_mutex_enter();
-
-  trx = trx_rw_is_active_low(trx_id, corrupt);
-
-  if (trx != nullptr) {
-    trx = trx_reference(trx, do_ref_count);
-  }
-
-  trx_sys_mutex_exit();
-
-  return (trx);
-}
-
-/** Allocates a new transaction id.
- @return new, allocated trx id */
-UNIV_INLINE
-trx_id_t trx_sys_get_new_trx_id() {
-  ut_ad(trx_sys_mutex_own());
-
-  /* VERY important: after the database is started, max_trx_id value is
-  divisible by TRX_SYS_TRX_ID_WRITE_MARGIN, and the following if
-  will evaluate to TRUE when this function is first time called,
-  and the value for trx id will be written to disk-based header!
-  Thus trx id values will not overlap when the database is
-  repeatedly started! */
-
-  if (!(trx_sys->max_trx_id % TRX_SYS_TRX_ID_WRITE_MARGIN)) {
-    trx_sys_flush_max_trx_id();
-  }
-
-  return (trx_sys->max_trx_id++);
-}
-
-inline trx_id_t trx_sys_get_max_trx_id() {
-  ut_ad(!trx_sys_mutex_own());
-
-  if (UNIV_WORD_SIZE < DATA_TRX_ID_LEN) {
-    /* Avoid torn reads. */
-
-    trx_sys_mutex_enter();
-
-    trx_id_t max_trx_id = trx_sys->max_trx_id;
-
-    trx_sys_mutex_exit();
-
-    return (max_trx_id);
-  } else {
-    /* Perform a dirty read. Callers should be prepared for stale
-    values, and we know that the value fits in a machine word, so
-    that it will be read and written atomically. */
-
-    return (trx_sys->max_trx_id);
-  }
-}
-
-/** Determine if there are incomplete transactions in the system.
-@return whether incomplete transactions need rollback */
-UNIV_INLINE
-bool trx_sys_need_rollback() {
-  ulint n_trx;
-
-  trx_sys_mutex_enter();
-
-  n_trx = UT_LIST_GET_LEN(trx_sys->rw_trx_list);
-  ut_ad(n_trx >= trx_sys->n_prepared_trx);
-  n_trx -= trx_sys->n_prepared_trx;
-
-  trx_sys_mutex_exit();
-
-  return (n_trx > 0);
-}
-
-/**
-Add the transaction to the RW transaction set
-@param trx		transaction instance to add */
-UNIV_INLINE
-void trx_sys_rw_trx_add(trx_t *trx) {
-  ut_ad(trx->id != 0);
-
-  trx_sys->rw_trx_set.insert(TrxTrack(trx->id, trx));
-  ut_d(trx->in_rw_trx_list = true);
-}
-
 #endif /* !UNIV_HOTBACKUP */
diff --git a/storage/innobase/include/trx0trx.h b/storage/innobase/include/trx0trx.h
index 94d5f8ffef7..9894452d6ec 100644
--- a/storage/innobase/include/trx0trx.h
+++ b/storage/innobase/include/trx0trx.h
@@ -46,6 +46,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "log0log.h"
 #include "mem0mem.h"
 #include "que0types.h"
+#include "read0types.h"
 #include "trx0xa.h"
 #include "usr0types.h"
 #include "ut0vec.h"
@@ -58,10 +59,13 @@ this program; if not, write to the Free Software Foundation, Inc.,
 struct mtr_t;
 
 // Forward declaration
-class ReadView;
+class FlushObserver;
 
 // Forward declaration
-class FlushObserver;
+struct rw_trx_hash_element_t;
+
+// Forward declaration
+struct LF_PINS;
 
 /** Dummy session used currently in MySQL interface */
 extern sess_t *trx_dummy_sess;
@@ -95,16 +99,16 @@ trx_t *trx_allocate_for_background(void);
 /** Resurrect table locks for resurrected transactions. */
 void trx_resurrect_locks();
 
-/** Free and initialize a transaction object instantiated during recovery.
-@param[in,out]	trx	transaction object to free and initialize */
-void trx_free_resurrected(trx_t *trx);
+/** Release a trx_t instance back to the pool.
+@param[in,out]  trx the instance to release */
+void trx_free(trx_t *&trx);
 
 /** Free a transaction that was allocated by background or user threads.
 @param[in,out]	trx	transaction object to free */
 void trx_free_for_background(trx_t *trx);
 
-/** At shutdown, frees a transaction object that is in the PREPARED state. */
-void trx_free_prepared(trx_t *trx); /*!< in, own: trx object */
+/** At shutdown, frees a transaction object. */
+void trx_free_at_shutdown(trx_t *trx);
 
 /** Free a transaction object for MySQL.
 @param[in,out]	trx	transaction */
@@ -188,10 +192,6 @@ void trx_commit_low(
     trx_t *trx,  /*!< in/out: transaction */
     mtr_t *mtr); /*!< in/out: mini-transaction (will be committed),
                  or NULL if trx made no modifications */
-/** Cleans up a transaction at database startup. The cleanup is needed if
- the transaction already got to the middle of a commit when the database
- crashed, and we cannot roll it back. */
-void trx_cleanup_at_db_startup(trx_t *trx); /*!< in: transaction */
 /** Does the transaction commit for MySQL.
  @return DB_SUCCESS or error number */
 dberr_t trx_commit_for_mysql(trx_t *trx); /*!< in/out: transaction */
@@ -222,10 +222,6 @@ trx_t *trx_get_trx_by_xid(const XID *xid);
 void trx_commit_complete_for_mysql(trx_t *trx); /*!< in/out: transaction */
 /** Marks the latest SQL statement ended. */
 void trx_mark_sql_stat_end(trx_t *trx); /*!< in: trx handle */
-/** Assigns a read view for a consistent read query. All the consistent reads
- within the same transaction will get the same read view, which is created
- when this function is first called for a new started transaction. */
-ReadView *trx_assign_read_view(trx_t *trx); /*!< in: active transaction */
 
 /** @return the transaction's read view or NULL if one not assigned. */
 UNIV_INLINE
@@ -370,26 +366,6 @@ tagged as such.
 @param[in,out] trx	Transaction that needs to be "upgraded" to RW from RO */
 void trx_set_rw_mode(trx_t *trx);
 
-/**
-Increase the reference count. If the transaction is in state
-TRX_STATE_COMMITTED_IN_MEMORY then the transaction is considered
-committed and the reference count is not incremented.
-@param trx Transaction that is being referenced
-@param do_ref_count Increment the reference iff this is true
-@return transaction instance if it is not committed */
-UNIV_INLINE
-trx_t *trx_reference(trx_t *trx, bool do_ref_count);
-
-/**
-Release the transaction. Decrease the reference count.
-@param trx Transaction that is being released */
-UNIV_INLINE
-void trx_release_reference(trx_t *trx);
-
-/**
-Check if the transaction is being referenced. */
-#define trx_is_referenced(t) ((t)->n_ref > 0)
-
 /**
 @param[in] requestor	Transaction requesting the lock
 @param[in] holder	Transaction holding the lock
@@ -451,16 +427,6 @@ with an explicit check for the read-only status.
 #define trx_is_ac_nl_ro(t) \
   ((t)->read_only && trx_is_autocommit_non_locking((t)))
 
-/**
-Assert that the transaction is in the trx_sys_t::rw_trx_list */
-#define assert_trx_in_rw_list(t)                         \
-  do {                                                   \
-    ut_ad(!(t)->read_only);                              \
-    ut_ad((t)->in_rw_trx_list ==                         \
-          !((t)->read_only || !(t)->rsegs.m_redo.rseg)); \
-    check_trx_state(t);                                  \
-  } while (0)
-
 /**
 Check transaction state */
 #define check_trx_state(t)                      \
@@ -486,8 +452,8 @@ Check transaction state */
     ut_ad(trx_state_eq((t), TRX_STATE_NOT_STARTED) ||    \
           trx_state_eq((t), TRX_STATE_FORCED_ROLLBACK)); \
     ut_ad(!trx_is_rseg_updated(trx));                    \
-    ut_ad(!MVCC::is_view_active((t)->read_view));        \
-    ut_ad((t)->lock.wait_thr == NULL);                   \
+    ut_ad(!(t)->read_view.is_open());                    \
+    ut_ad((t)->lock.wait_thr == nullptr);                \
     ut_ad(UT_LIST_GET_LEN((t)->lock.trx_locks) == 0);    \
     ut_ad((t)->dict_operation == TRX_DICT_OP_NONE);      \
   } while (0)
@@ -503,16 +469,15 @@ transaction pool.
 
 #ifdef UNIV_DEBUG
 /** Assert that an autocommit non-locking select cannot be in the
- rw_trx_list and that it is a read-only transaction.
- The tranasction must be in the mysql_trx_list. */
+ rw_trx_hash and that it is a read-only transaction.
+ The tranasction must have mysql_thd assigned. */
 #define assert_trx_nonlocking_or_in_list(t)         \
   do {                                              \
     if (trx_is_autocommit_non_locking(t)) {         \
       trx_state_t t_state = (t)->state;             \
       ut_ad((t)->read_only);                        \
       ut_ad(!(t)->is_recovered);                    \
-      ut_ad(!(t)->in_rw_trx_list);                  \
-      ut_ad((t)->in_mysql_trx_list);                \
+      ut_ad((t)->mysql_thd != nullptr);             \
       ut_ad(t_state == TRX_STATE_NOT_STARTED ||     \
             t_state == TRX_STATE_FORCED_ROLLBACK || \
             t_state == TRX_STATE_ACTIVE);           \
@@ -521,9 +486,9 @@ transaction pool.
     }                                               \
   } while (0)
 #else /* UNIV_DEBUG */
-/** Assert that an autocommit non-locking slect cannot be in the
- rw_trx_list and that it is a read-only transaction.
- The tranasction must be in the mysql_trx_list. */
+/** Assert that an autocommit non-locking select cannot be in the
+ rw_trx_hash and that it is a read-only transaction.
+ The tranasction must have mysql_thd assigned. */
 #define assert_trx_nonlocking_or_in_list(trx) ((void)0)
 #endif /* UNIV_DEBUG */
 #endif /* !UNIV_HOTBACKUP */
@@ -824,6 +789,16 @@ enum trx_rseg_type_t {
 };
 
 struct trx_t {
+ private:
+  std::atomic<int32_t> n_ref; /*!< Count of references, protected
+              by trx_t::mutex. We can't release the
+              locks nor commit the transaction until
+              this reference is 0.  We can change
+              the state to COMMITTED_IN_MEMORY to
+              signify that it is no longer
+              "active". */
+
+ public:
   enum isolation_level_t {
 
     /** dirty read: non-locking SELECTs are performed so that we
@@ -879,7 +854,7 @@ struct trx_t {
                transaction is moved to
                COMMITTED_IN_MEMORY state.
                Protected by trx_sys_t::mutex
-               when trx->in_rw_trx_list. Initially
+               when trx is in rw_trx_hash. Initially
                set to TRX_ID_MAX. */
 
   /** State of the trx from the point of view of concurrency control
@@ -917,11 +892,11 @@ struct trx_t {
 
   XA (2PC) transactions are always treated as non-autocommit.
 
-  Transitions to ACTIVE or NOT_STARTED occur when
-  !in_rw_trx_list (no trx_sys->mutex needed).
+  Transitions to ACTIVE or NOT_STARTED occur when transaction
+  is not in rw_trx_hash (no trx_sys->mutex needed).
 
   Autocommit non-locking read-only transactions move between states
-  without holding any mutex. They are !in_rw_trx_list.
+  without holding any mutex. They are not in rw_trx_hash.
 
   All transactions, unless they are determined to be ac-nl-ro,
   explicitly tagged as read-only or read-write, will first be put
@@ -930,16 +905,17 @@ struct trx_t {
   do we remove it from the read-only list and put it on the read-write
   list. During this switch we assign it a rollback segment.
 
-  When a transaction is NOT_STARTED, it can be in_mysql_trx_list if
-  it is a user transaction. It cannot be in rw_trx_list.
+  When a transaction is NOT_STARTED, it can be in trx_list. It cannot be
+  in rw_trx_hash.
 
-  ACTIVE->PREPARED->COMMITTED is only possible when trx->in_rw_trx_list.
+  ACTIVE->PREPARED->COMMITTED is only possible when trx is in rw_trx_hash.
   The transition ACTIVE->PREPARED is protected by trx_sys->mutex.
 
   ACTIVE->COMMITTED is possible when the transaction is in
-  rw_trx_list.
+  rw_trx_hash.
 
-  Transitions to COMMITTED are protected by trx->mutex.
+  Transitions to COMMITTED are protected by both lock_sys->mutex
+  and trx->mutex.
 
   NOTE: Some of these state change constraints are an overkill,
   currently only required for a consistent view for printing stats.
@@ -954,16 +930,14 @@ struct trx_t {
   concurrent unique insert or replace operation. */
   bool skip_lock_inheritance;
 
-  ReadView *read_view; /*!< consistent read view used in the
-                       transaction, or NULL if not yet set */
+  ReadView read_view; /*!< consistent read view used in the
+                      transaction, or NULL if not yet set */
+
+  std::atomic<bool> serialised; /*!< trx is serialised if set */
 
   UT_LIST_NODE_T(trx_t)
-  trx_list; /*!< list of transactions;
+  trx_list; /*!< list of all transactions;
             protected by trx_sys->mutex. */
-  UT_LIST_NODE_T(trx_t)
-  no_list; /*!< Required during view creation
-           to check for the view limit for
-           transactions that are committing */
 
   /** Information about the transaction locks and state.
   Protected by trx->mutex or lock_sys latches or both */
@@ -972,7 +946,7 @@ struct trx_t {
   bool is_recovered; /*!< 0=normal transaction,
                      1=recovered, must be rolled back,
                      protected by trx_sys->mutex when
-                     trx->in_rw_trx_list holds */
+                     trx is in rw_trx_hash */
 
   os_thread_id_t killed_by; /*!< The thread ID that wants to
                             kill this transaction asynchronously.
@@ -1085,22 +1059,6 @@ struct trx_t {
   statement uses, except those
   in consistent read */
   /*------------------------------*/
-#ifdef UNIV_DEBUG
-  /** The following two fields are mutually exclusive. */
-  /* @{ */
-
-  bool in_rw_trx_list; /*!< true if in trx_sys->rw_trx_list */
-                       /* @} */
-#endif                 /* UNIV_DEBUG */
-  UT_LIST_NODE_T(trx_t)
-  mysql_trx_list; /*!< list of transactions created for
-                  MySQL; protected by trx_sys->mutex */
-#ifdef UNIV_DEBUG
-  bool in_mysql_trx_list;
-  /*!< true if in
-  trx_sys->mysql_trx_list */
-#endif /* UNIV_DEBUG */
-  /*------------------------------*/
   dberr_t error_state;             /*!< 0 if no error, otherwise error
                                    number; NOTE That ONLY the thread
                                    doing the transaction is allowed to
@@ -1195,14 +1153,6 @@ struct trx_t {
   const char *start_file; /*!< Filename where it was started */
 #endif                    /* UNIV_DEBUG */
 
-  lint n_ref; /*!< Count of references, protected
-              by trx_t::mutex. We can't release the
-              locks nor commit the transaction until
-              this reference is 0.  We can change
-              the state to COMMITTED_IN_MEMORY to
-              signify that it is no longer
-              "active". */
-
   /** Version of this instance. It is incremented each time the
   instance is re-used in trx_start_low(). It is used to track
   whether a transaction has been restarted since it was tagged
@@ -1230,6 +1180,8 @@ struct trx_t {
                   doing Non-locking Read-only Read
                   Committed on DD tables */
 #endif            /* UNIV_DEBUG */
+  rw_trx_hash_element_t *rw_trx_hash_element{nullptr};
+  LF_PINS *rw_trx_hash_pins{nullptr};
   ulint magic_n;
 
   bool is_read_uncommitted() const {
@@ -1250,6 +1202,18 @@ struct trx_t {
   }
 
   bool allow_semi_consistent() const { return (skip_gap_locks()); }
+
+  bool is_referenced() { return (n_ref.load(std::memory_order_relaxed) > 0); }
+
+  void reference() {
+    int32_t old_n_ref = n_ref.fetch_add(1, std::memory_order_relaxed);
+    ut_a(old_n_ref >= 0);
+  }
+
+  void release_reference() {
+    int32_t old_n_ref = n_ref.fetch_sub(1, std::memory_order_relaxed);
+    ut_a(old_n_ref > 0);
+  }
 };
 #ifndef UNIV_HOTBACKUP
 
diff --git a/storage/innobase/include/trx0trx.ic b/storage/innobase/include/trx0trx.ic
index 6171a1f5f10..f4509323027 100644
--- a/storage/innobase/include/trx0trx.ic
+++ b/storage/innobase/include/trx0trx.ic
@@ -35,7 +35,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 /** Determines if a transaction is in the given state.
  The caller must hold trx_sys->mutex, or it must be the thread
  that is serving a running transaction.
- A running RW transaction must be in trx_sys->rw_trx_list.
+ A running RW transaction must be in trx_sys->rw_trx_hash.
  @return true if trx->state == state */
 UNIV_INLINE
 bool trx_state_eq(const trx_t *trx,  /*!< in: transaction */
@@ -65,8 +65,6 @@ bool trx_state_eq(const trx_t *trx,  /*!< in: transaction */
       ut_a(state == TRX_STATE_NOT_STARTED ||
            state == TRX_STATE_FORCED_ROLLBACK);
 
-      ut_ad(!trx->in_rw_trx_list);
-
       return (true);
   }
   ut_error;
@@ -225,50 +223,12 @@ bool trx_is_rseg_assigned(const trx_t *trx) /*!< in: transaction */
           trx->rsegs.m_noredo.rseg != nullptr);
 }
 
-/**
-Increase the reference count. If the transaction is in state
-TRX_STATE_COMMITTED_IN_MEMORY then the transaction is considered
-committed and the reference count is not incremented.
-@param trx Transaction that is being referenced
-@param do_ref_count Increment the reference iff this is true
-@return transaction instance if it is not committed */
-UNIV_INLINE
-trx_t *trx_reference(trx_t *trx, bool do_ref_count) {
-  trx_mutex_enter(trx);
-
-  if (trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY)) {
-    trx_mutex_exit(trx);
-    trx = nullptr;
-  } else if (do_ref_count) {
-    ut_ad(trx->n_ref >= 0);
-    ++trx->n_ref;
-    trx_mutex_exit(trx);
-  } else {
-    trx_mutex_exit(trx);
-  }
-
-  return (trx);
-}
-
-/**
-Release the transaction. Decrease the reference count.
-@param trx Transaction that is being released */
-UNIV_INLINE
-void trx_release_reference(trx_t *trx) {
-  trx_mutex_enter(trx);
-
-  ut_ad(trx->n_ref > 0);
-  --trx->n_ref;
-
-  trx_mutex_exit(trx);
-}
-
 /**
 @param trx		Get the active view for this transaction, if one exists
 @return the transaction's read view or NULL if one not assigned. */
 UNIV_INLINE
 ReadView *trx_get_read_view(trx_t *trx) {
-  return (!MVCC::is_view_active(trx->read_view) ? nullptr : trx->read_view);
+  return (!trx->read_view.is_open() ? nullptr : &trx->read_view);
 }
 
 /**
@@ -276,7 +236,7 @@ ReadView *trx_get_read_view(trx_t *trx) {
 @return the transaction's read view or NULL if one not assigned. */
 UNIV_INLINE
 const ReadView *trx_get_read_view(const trx_t *trx) {
-  return (!MVCC::is_view_active(trx->read_view) ? nullptr : trx->read_view);
+  return (!trx->read_view.is_open() ? nullptr : &trx->read_view);
 }
 
 /**
diff --git a/storage/innobase/include/trx0types.h b/storage/innobase/include/trx0types.h
index 88988b0cf63..dca3e5b8322 100644
--- a/storage/innobase/include/trx0types.h
+++ b/storage/innobase/include/trx0types.h
@@ -585,40 +585,6 @@ typedef std::priority_queue<
 
 typedef std::vector<trx_id_t, ut_allocator<trx_id_t>> trx_ids_t;
 
-/** Mapping read-write transactions from id to transaction instance, for
-creating read views and during trx id lookup for MVCC and locking. */
-struct TrxTrack {
-  explicit TrxTrack(trx_id_t id, trx_t *trx = nullptr) : m_id(id), m_trx(trx) {
-    // Do nothing
-  }
-
-  trx_id_t m_id;
-  trx_t *m_trx;
-};
-
-struct TrxTrackHash {
-  size_t operator()(const TrxTrack &key) const { return (size_t(key.m_id)); }
-};
-
-/**
-Comparator for TrxMap */
-struct TrxTrackHashCmp {
-  bool operator()(const TrxTrack &lhs, const TrxTrack &rhs) const {
-    return (lhs.m_id == rhs.m_id);
-  }
-};
-
-/**
-Comparator for TrxMap */
-struct TrxTrackCmp {
-  bool operator()(const TrxTrack &lhs, const TrxTrack &rhs) const {
-    return (lhs.m_id < rhs.m_id);
-  }
-};
-
-// typedef std::unordered_set<TrxTrack, TrxTrackHash, TrxTrackHashCmp> TrxIdSet;
-typedef std::set<TrxTrack, TrxTrackCmp, ut_allocator<TrxTrack>> TrxIdSet;
-
 struct TrxVersion {
   TrxVersion(trx_t *trx);
 
diff --git a/storage/innobase/include/trx0undo.h b/storage/innobase/include/trx0undo.h
index 6c22b3e822d..fe622456795 100644
--- a/storage/innobase/include/trx0undo.h
+++ b/storage/innobase/include/trx0undo.h
@@ -272,9 +272,8 @@ the data can be discarded.
 @param[in]	noredo		whether the undo tablespace is redo logged */
 void trx_undo_insert_cleanup(trx_undo_ptr_t *undo_ptr, bool noredo);
 
-/** At shutdown, frees the undo logs of a PREPARED transaction. */
-void trx_undo_free_prepared(trx_t *trx) /*!< in/out: PREPARED transaction */
-    UNIV_COLD;
+/** At shutdown, frees the undo logs of a transaction. */
+void trx_undo_free_at_shutdown(trx_t *trx);
 
 /* Forward declaration. */
 namespace undo {
diff --git a/storage/innobase/include/ut0cpu_cache.h b/storage/innobase/include/ut0cpu_cache.h
index 40053e02dc3..6e5bd19a05f 100644
--- a/storage/innobase/include/ut0cpu_cache.h
+++ b/storage/innobase/include/ut0cpu_cache.h
@@ -34,7 +34,7 @@ Utilities related to CPU cache. */
 namespace ut {
 
 /** CPU cache line size */
-#ifdef __powerpc__
+#if (defined(__powerpc__)) || (defined(__aarch64__))
 constexpr size_t INNODB_CACHE_LINE_SIZE = 128;
 #else
 constexpr size_t INNODB_CACHE_LINE_SIZE = 64;
diff --git a/storage/innobase/include/ut0new.h b/storage/innobase/include/ut0new.h
index 838520b136c..da6e2aaaa70 100644
--- a/storage/innobase/include/ut0new.h
+++ b/storage/innobase/include/ut0new.h
@@ -182,7 +182,6 @@ extern PSI_memory_key mem_key_partitioning;
 extern PSI_memory_key mem_key_row_log_buf;
 extern PSI_memory_key mem_key_row_merge_sort;
 extern PSI_memory_key mem_key_std;
-extern PSI_memory_key mem_key_trx_sys_t_rw_trx_ids;
 extern PSI_memory_key mem_key_undo_spaces;
 extern PSI_memory_key mem_key_ut_lock_free_hash_t;
 /* Please obey alphabetical order in the definitions above. */
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index 779117852de..948ed914fdb 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -228,7 +228,7 @@ bool lock_check_trx_id_sanity(
 {
   ut_ad(rec_offs_validate(rec, index, offsets));
 
-  trx_id_t max_trx_id = trx_sys_get_max_trx_id();
+  trx_id_t max_trx_id = trx_sys->get_max_trx_id();
   bool is_ok = trx_id < max_trx_id;
 
   if (!is_ok) {
@@ -837,21 +837,22 @@ static const lock_t *lock_rec_other_has_conflicting(
 
 /** Checks if some transaction has an implicit x-lock on a record in a secondary
  index.
- @param[in]   rec       user record
- @param[in]   index     secondary index
- @param[in]   offsets   rec_get_offsets(rec, index)
+ @param[in/out]  caller_trx  trx of current thread
+ @param[in]      rec         user record
+ @param[in]      index       secondary index
+ @param[in]      offsets     rec_get_offsets(rec, index)
  @return transaction id of the transaction which has the x-lock, or 0;
  NOTE that this function can return false positives but never false
  negatives. The caller must confirm all positive results by checking if the trx
  is still active. */
-static trx_t *lock_sec_rec_some_has_impl(const rec_t *rec, dict_index_t *index,
+static trx_t *lock_sec_rec_some_has_impl(trx_t *caller_trx, const rec_t *rec,
+                                         dict_index_t *index,
                                          const ulint *offsets) {
   trx_t *trx;
   trx_id_t max_trx_id;
   const page_t *page = page_align(rec);
 
   ut_ad(!locksys::owns_exclusive_global_latch());
-  ut_ad(!trx_sys_mutex_own());
   ut_ad(!index->is_clustered());
   ut_ad(page_rec_is_user_rec(rec));
   ut_ad(rec_offs_validate(rec, index, offsets));
@@ -864,7 +865,7 @@ static trx_t *lock_sec_rec_some_has_impl(const rec_t *rec, dict_index_t *index,
   max trx id to the log, and therefore during recovery, this value
   for a page may be incorrect. */
 
-  if (max_trx_id < trx_rw_min_trx_id() && !recv_recovery_is_on()) {
+  if (max_trx_id < trx_sys->get_min_trx_id()) {
     trx = nullptr;
 
   } else if (!lock_check_trx_id_sanity(max_trx_id, rec, index, offsets)) {
@@ -875,15 +876,37 @@ static trx_t *lock_sec_rec_some_has_impl(const rec_t *rec, dict_index_t *index,
     x-lock. We have to look in the clustered index. */
 
   } else {
-    trx = row_vers_impl_x_locked(rec, index, offsets);
+    trx = row_vers_impl_x_locked(caller_trx, rec, index, offsets);
   }
 
   return (trx);
 }
 
 #ifdef UNIV_DEBUG
+struct lock_rec_other_trx_holds_expl_arg {
+  ulint precise_mode;
+  const buf_block_t *const block;
+  ulint heap_no;
+  const trx_t *impl_trx;
+};
+
+static bool lock_rec_other_trx_holds_expl_callback(
+    rw_trx_hash_element_t *element, lock_rec_other_trx_holds_expl_arg *arg) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    const lock_t *expl_lock = lock_rec_has_expl(arg->precise_mode, arg->block,
+                                                arg->heap_no, arg->impl_trx);
+
+    ut_ad(!expl_lock || expl_lock->trx == arg->impl_trx);
+  }
+  mutex_exit(&element->mutex);
+  return false;
+}
+
 /** Checks if some transaction, other than given trx_id, has an explicit
  lock on the given rec, in the given precise_mode.
+@param[in]   caller_trx     current transaction
 @param[in]   precise_mode   LOCK_S or LOCK_X possibly ORed to LOCK_GAP or
                             LOCK_REC_NOT_GAP.
 @param[in]   trx            the trx holding implicit lock on rec
@@ -892,41 +915,19 @@ static trx_t *lock_sec_rec_some_has_impl(const rec_t *rec, dict_index_t *index,
 @return true iff there's a transaction, whose id is not equal to trx_id,
         that has an explicit lock on the given rec, in the given
         precise_mode. */
-static bool lock_rec_other_trx_holds_expl(ulint precise_mode, const trx_t *trx,
-                                          const rec_t *rec,
+static void lock_rec_other_trx_holds_expl(trx_t *caller_trx, ulint precise_mode,
+                                          const trx_t *trx, const rec_t *rec,
                                           const buf_block_t *block) {
-  bool holds = false;
-
   /* We will inspect locks from various shards when inspecting transactions. */
   locksys::Global_exclusive_latch_guard guard{};
-  /* If trx_rw_is_active returns non-null impl_trx it only means that impl_trx
-  was active at some moment during the call, but might already be in
-  TRX_STATE_COMMITTED_IN_MEMORY when we execute the body of the if.
-  However, we hold exclusive latch on whole lock_sys, which prevents anyone
-  from creating any new explicit locks.
-  So, all explicit locks we will see must have been created at the time when
-  the transaction was not committed yet. */
-  if (trx_t *impl_trx = trx_rw_is_active(trx->id, nullptr, false)) {
-    ulint heap_no = page_rec_get_heap_no(rec);
-    mutex_enter(&trx_sys->mutex);
 
-    for (const trx_t *t = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); t != nullptr;
-         t = UT_LIST_GET_NEXT(trx_list, t)) {
-      const lock_t *expl_lock =
-          lock_rec_has_expl(precise_mode, block, heap_no, t);
-
-      if (expl_lock && expl_lock->trx != impl_trx) {
-        /* An explicit lock is held by trx other than
-        the trx holding the implicit lock. */
-        holds = true;
-        break;
-      }
-    }
-
-    mutex_exit(&trx_sys->mutex);
-  }
+  ulint heap_no = page_rec_get_heap_no(rec);
+  lock_rec_other_trx_holds_expl_arg arg = {precise_mode, block, heap_no, trx};
 
-  return (holds);
+  trx_sys->rw_trx_hash.iterate(caller_trx,
+                               reinterpret_cast<lf_hash_walk_func *>(
+                                   lock_rec_other_trx_holds_expl_callback),
+                               &arg);
 }
 #endif /* UNIV_DEBUG */
 
@@ -940,8 +941,6 @@ ulint lock_number_of_rows_locked(const trx_lock_t *trx_lock) {
 }
 
 ulint lock_number_of_tables_locked(const trx_t *trx) {
-  ut_ad(trx_mutex_own(trx));
-
   return (trx->lock.table_locks.size());
 }
 
@@ -4283,74 +4282,77 @@ static void lock_remove_all_on_table_for_trx(
   trx_mutex_exit(trx);
 }
 
-/** Remove any explicit record locks held by recovering transactions on
- the table.
- @return number of recovered transactions examined */
-static ulint lock_remove_recovered_trx_record_locks(
-    dict_table_t *table) /*!< in: check if there are any locks
-                         held on records in this table or on the
-                         table itself */
-{
-  ut_a(table != nullptr);
-  /* We need exclusive lock_sys latch, as we are about to iterate over locks
-  held by multiple transactions while they might be operating. */
-  ut_ad(locksys::owns_exclusive_global_latch());
-
-  ulint n_recovered_trx = 0;
-
-  mutex_enter(&trx_sys->mutex);
-
-  for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    assert_trx_in_rw_list(trx);
+struct lock_remove_recovered_trx_record_locks_arg {
+  dict_table_t *table;
+  ulint n_recovered_trx;
+};
 
+static bool lock_remove_recovered_trx_record_locks_callback(
+    rw_trx_hash_element_t *element,
+    lock_remove_recovered_trx_record_locks_arg *arg) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
     if (!trx->is_recovered) {
-      continue;
+      mutex_exit(&element->mutex);
+      return false;
     }
-    /* We need trx->mutex to iterate over trx->lock.trx_lock and it is needed by
-    lock_trx_table_locks_remove() and lock_table_remove_low() but we haven't
-    acquired it yet. */
-    ut_ad(!trx_mutex_own(trx));
-    trx_mutex_enter(trx);
-    /* Because we are holding the exclusive global lock_sys latch,
+
+    /* Because we are holding the lock_sys->mutex,
     implicit locks cannot be converted to explicit ones
     while we are scanning the explicit locks. */
 
-    lock_t *next_lock;
-
     for (lock_t *lock = UT_LIST_GET_FIRST(trx->lock.trx_locks); lock != nullptr;
-         lock = next_lock) {
+         lock = UT_LIST_GET_NEXT(trx_locks, lock)) {
       ut_a(lock->trx == trx);
 
       /* Recovered transactions can't wait on a lock. */
 
       ut_a(!lock_get_wait(lock));
 
-      next_lock = UT_LIST_GET_NEXT(trx_locks, lock);
-
       switch (lock_get_type_low(lock)) {
         default:
           ut_error;
         case LOCK_TABLE:
-          if (lock->tab_lock.table == table) {
+          if (lock->tab_lock.table == arg->table) {
             lock_trx_table_locks_remove(lock);
             lock_table_remove_low(lock);
           }
           break;
         case LOCK_REC:
-          if (lock->index->table == table) {
+          if (lock->index->table == arg->table) {
             lock_rec_discard(lock);
           }
       }
     }
 
-    trx_mutex_exit(trx);
-    ++n_recovered_trx;
+    ++arg->n_recovered_trx;
   }
+  mutex_exit(&element->mutex);
+  return false;
+}
 
-  mutex_exit(&trx_sys->mutex);
+/** Remove any explicit record locks held by recovering transactions on
+ the table.
+ @return number of recovered transactions examined */
+static ulint lock_remove_recovered_trx_record_locks(
+    dict_table_t *table) /*!< in: check if there are any locks
+                         held on records in this table or on the
+                         table itself */
+{
+  ut_a(table != nullptr);
+  /* We need exclusive lock_sys latch, as we are about to iterate over locks
+  held by multiple transactions while they might be operating. */
+  ut_ad(locksys::owns_exclusive_global_latch());
+
+  lock_remove_recovered_trx_record_locks_arg arg = {table, 0};
 
-  return (n_recovered_trx);
+  trx_sys->rw_trx_hash.iterate(
+      reinterpret_cast<lf_hash_walk_func *>(
+          lock_remove_recovered_trx_record_locks_callback),
+      &arg);
+
+  return (arg.n_recovered_trx);
 }
 
 /** Removes locks on a table to be dropped.
@@ -4583,7 +4585,7 @@ void lock_print_info_summary(FILE *file) {
       "------------\n",
       file);
 
-  fprintf(file, "Trx id counter " TRX_ID_FMT "\n", trx_sys_get_max_trx_id());
+  fprintf(file, "Trx id counter " TRX_ID_FMT "\n", trx_sys->get_max_trx_id());
 
   fprintf(file,
           "Purge done for trx's n:o < " TRX_ID_FMT " undo n:o < " TRX_ID_FMT
@@ -4639,7 +4641,6 @@ struct PrintNotStarted {
   void operator()(const trx_t *trx) {
     /* We require exclusive access to lock_sys */
     ut_ad(locksys::owns_exclusive_global_latch());
-    ut_ad(trx->in_mysql_trx_list);
     ut_ad(mutex_own(&trx_sys->mutex));
 
     /* See state transitions and locking rules in trx0trx.h */
@@ -4692,64 +4693,6 @@ class TrxLockIterator {
   ulint m_index;
 };
 
-/** This iterates over both the RW and RO trx_sys lists. We need to keep
-track where the iterator was up to and we do that using an ordinal value. */
-
-class TrxListIterator {
- public:
-  TrxListIterator() : m_index() {
-    /* We iterate over the RW trx list first. */
-
-    m_trx_list = &trx_sys->rw_trx_list;
-  }
-
-  /** Get the current transaction whose ordinality is m_index.
-  @return current transaction or 0 */
-
-  const trx_t *current() { return (reposition()); }
-
-  /** Advance the transaction current ordinal value and reset the
-  transaction lock ordinal value */
-
-  void next() {
-    ++m_index;
-    m_lock_iter.rewind();
-  }
-
-  TrxLockIterator &lock_iter() { return (m_lock_iter); }
-
- private:
-  /** Reposition the "cursor" on the current transaction. If it
-  is the first time then the "cursor" will be positioned on the
-  first transaction.
-
-  @return transaction instance or 0 */
-  const trx_t *reposition() const {
-    ulint i;
-    trx_t *trx;
-
-    /* Make the transaction at the ordinal value of m_index
-    the current transaction. ie. reposition/restore */
-
-    for (i = 0, trx = UT_LIST_GET_FIRST(*m_trx_list);
-         trx != nullptr && (i < m_index);
-         trx = UT_LIST_GET_NEXT(trx_list, trx), ++i) {
-      check_trx_state(trx);
-    }
-
-    return (trx);
-  }
-
-  /** Ordinal value of the transaction in the current transaction list */
-  ulint m_index;
-
-  /** Current transaction list */
-  trx_ut_list_t *m_trx_list;
-
-  /** For iterating over a transaction's locks */
-  TrxLockIterator m_lock_iter;
-};
-
 /** Prints transaction lock wait and MVCC state.
 @param[in,out]	file	file where to print
 @param[in]	trx	transaction */
@@ -4761,10 +4704,13 @@ void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx) {
 
   trx_print_latched(file, trx, 600);
 
-  const ReadView *read_view = trx_get_read_view(trx);
+  /* Note: read_view->get_state() check is race condition. But it
+  should "kind of work" because read_view is freed only at shutdown.
+  Worst thing that may happen is that it'll get transferred to
+  another thread and print wrong values. */
 
-  if (read_view != nullptr) {
-    read_view->print_limits(file);
+  if (READ_VIEW_STATE_OPEN == trx->read_view.get_state()) {
+    trx->read_view.print_limits(file);
   }
 
   if (trx->lock.que_state == TRX_QUE_LOCK_WAIT) {
@@ -4783,105 +4729,30 @@ void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx) {
   }
 }
 
-/** Reads the page containing the record protected by the given lock.
-This function will temporarily release the exclusive global latch and the
-trx_sys_t::mutex if the page was read from disk.
-@param[in]  lock  the record lock
-@return true if a page was successfully read from the tablespace */
-static bool lock_rec_fetch_page(const lock_t *lock) {
-  ut_ad(lock_get_type_low(lock) == LOCK_REC);
-
-  space_id_t space_id = lock->rec_lock.space;
-  fil_space_t *space;
-  bool found;
-  const page_size_t &page_size = fil_space_get_page_size(space_id, &found);
-  page_no_t page_no = lock->rec_lock.page_no;
-
-  /* Check if the .ibd file exists. */
-  if (found) {
-    mtr_t mtr;
-
-    locksys::Unsafe_global_latch_manipulator::exclusive_unlatch();
-
-    mutex_exit(&trx_sys->mutex);
-
-    DEBUG_SYNC_C("innodb_monitor_before_lock_page_read");
-
-    /* Check if the space is exists or not. only
-    when the space is valid, try to get the page. */
-    space = fil_space_acquire(space_id);
-    if (space) {
-      mtr_start(&mtr);
-      buf_page_get_gen(page_id_t(space_id, page_no), page_size, RW_NO_LATCH,
-                       nullptr, Page_fetch::POSSIBLY_FREED, __FILE__, __LINE__,
-                       &mtr);
-      mtr_commit(&mtr);
-      fil_space_release(space);
-    }
-
-    locksys::Unsafe_global_latch_manipulator::exclusive_latch();
-
-    mutex_enter(&trx_sys->mutex);
-
-    return (true);
-  }
-
-  return (false);
-}
-
 /** Prints info of locks for a transaction.
  @return true if all printed, false if latches were released. */
-static bool lock_trx_print_locks(
-    FILE *file,            /*!< in/out: File to write */
-    const trx_t *trx,      /*!< in: current transaction */
-    TrxLockIterator &iter, /*!< in: transaction lock iterator */
-    bool load_block)       /*!< in: if true then read block
-                           from disk */
+static void lock_trx_print_locks(
+    FILE *file,       /*!< in/out: File to write */
+    const trx_t *trx) /*!< in: current transaction */
 {
   const lock_t *lock;
-  /* We require exclusive access to lock_sys */
-  ut_ad(locksys::owns_exclusive_global_latch());
+  uint32_t i = 0;
 
   /* Iterate over the transaction's locks. */
-  while ((lock = iter.current(trx)) != nullptr) {
+  for (lock = UT_LIST_GET_FIRST(trx->lock.trx_locks); lock != nullptr;
+       lock = UT_LIST_GET_NEXT(trx_locks, lock)) {
     if (lock_get_type_low(lock) == LOCK_REC) {
-      if (load_block) {
-        /* Note: lock_rec_fetch_page() will release both the exclusive global
-        latch and the trx_sys_t::mutex if it does a read from disk. */
-
-        if (lock_rec_fetch_page(lock)) {
-          /* We need to resync the
-          current transaction. */
-          return (false);
-        }
-
-        /* It is a single table tablespace
-        and the .ibd file is missing
-        (DISCARD TABLESPACE probably stole the
-        locks): just print the lock without
-        attempting to load the page in the
-        buffer pool. */
-
-        fprintf(file,
-                "RECORD LOCKS on non-existing"
-                " space %u\n",
-                lock->rec_lock.space);
-      }
-
       /* Print all the record locks on the page from
       the record lock bitmap */
 
       lock_rec_print(file, lock);
-
-      load_block = true;
-
     } else {
       ut_ad(lock_get_type_low(lock) & LOCK_TABLE);
 
       lock_table_print(file, lock);
     }
 
-    if (iter.next() >= 10) {
+    if (++i == 10) {
       fprintf(file,
               "10 LOCKS PRINTED FOR THIS TRX:"
               " SUPPRESSING FURTHER PRINTS\n");
@@ -4889,8 +4760,31 @@ static bool lock_trx_print_locks(
       break;
     }
   }
+}
 
-  return (true);
+static bool lock_print_info_all_transactions_callback(
+    rw_trx_hash_element_t *element, FILE *file) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    check_trx_state(trx);
+
+    lock_trx_print_wait_and_mvcc_state(file, trx);
+
+    /* If we need to print the locked record contents then we
+    need to fetch the containing block from the buffer pool. */
+    if (srv_print_innodb_lock_monitor) {
+      /* Print the locks owned by the current transaction. */
+
+      trx->reference();
+      mutex_exit(&element->mutex);
+      lock_trx_print_locks(file, trx);
+      trx->release_reference();
+      return false;
+    }
+  }
+  mutex_exit(&element->mutex);
+  return false;
 }
 
 void lock_print_info_all_transactions(FILE *file) {
@@ -4899,8 +4793,6 @@ void lock_print_info_all_transactions(FILE *file) {
 
   fprintf(file, "LIST OF TRANSACTIONS FOR EACH SESSION:\n");
 
-  mutex_enter(&trx_sys->mutex);
-
   /* First print info on non-active transactions */
 
   /* NOTE: information of auto-commit non-locking read-only
@@ -4908,55 +4800,13 @@ void lock_print_info_all_transactions(FILE *file) {
   available from INFORMATION_SCHEMA.INNODB_TRX. */
 
   PrintNotStarted print_not_started(file);
-  ut_list_map(trx_sys->mysql_trx_list, print_not_started);
-
-  const trx_t *trx;
-  TrxListIterator trx_iter;
-  const trx_t *prev_trx = nullptr;
-
-  /* Control whether a block should be fetched from the buffer pool. */
-  bool load_block = true;
-  bool monitor = srv_print_innodb_lock_monitor;
-
-  while ((trx = trx_iter.current()) != nullptr) {
-    check_trx_state(trx);
-
-    if (trx != prev_trx) {
-      lock_trx_print_wait_and_mvcc_state(file, trx);
-      prev_trx = trx;
-
-      /* The transaction that read in the page is no
-      longer the one that read the page in. We need to
-      force a page read. */
-      load_block = true;
-    }
-
-    /* If we need to print the locked record contents then we
-    need to fetch the containing block from the buffer pool. */
-    if (monitor) {
-      /* Print the locks owned by the current transaction. */
-      TrxLockIterator &lock_iter = trx_iter.lock_iter();
-
-      if (!lock_trx_print_locks(file, trx, lock_iter, load_block)) {
-        /* Resync trx_iter, the trx_sys->mutex and exclusive global latch were
-        temporarily released. A page was successfully read in. We need to print
-        its contents on the next call to lock_trx_print_locks(). On the next
-        call to lock_trx_print_locks() we should simply print the contents of
-        the page just read in.*/
-        load_block = false;
-
-        continue;
-      }
-    }
-
-    load_block = true;
-
-    /* All record lock details were printed without fetching
-    a page from disk, or we didn't need to print the detail. */
-    trx_iter.next();
-  }
-
+  mutex_enter(&trx_sys->mutex);
+  ut_list_map(trx_sys->trx_list, print_not_started);
   mutex_exit(&trx_sys->mutex);
+  trx_sys->rw_trx_hash.iterate(
+      reinterpret_cast<lf_hash_walk_func *>(
+          lock_print_info_all_transactions_callback),
+      file);
 }
 
 #ifdef UNIV_DEBUG
@@ -4989,7 +4839,6 @@ static bool lock_table_queue_validate(
 
   /* We actually hold exclusive latch here, but we require just the shard */
   ut_ad(locksys::owns_table_shard(*table));
-  ut_ad(trx_sys_mutex_own());
 
   for (lock = UT_LIST_GET_FIRST(table->locks); lock != nullptr;
        lock = UT_LIST_GET_NEXT(tab_lock.locks, lock)) {
@@ -5063,14 +4912,11 @@ static void rec_queue_validate_latched(const buf_block_t *block,
 
     trx_id = lock_clust_rec_some_has_impl(rec, index, offsets);
 
-    const trx_t *impl_trx = trx_rw_is_active_low(trx_id, nullptr);
+    const trx_t *impl_trx = nullptr;
+    impl_trx =
+        (trx_id > 0) ? trx_sys->find(current_trx(), trx_id, false) : nullptr;
     if (impl_trx != nullptr) {
       ut_ad(owns_page_shard(block->get_page_id()));
-      ut_ad(trx_sys_mutex_own());
-      /* impl_trx cannot become TRX_STATE_COMMITTED_IN_MEMORY nor removed from
-      rw_trx_set until we release trx_sys->mutex, which means that currently all
-      other threads in the system consider this impl_trx active and thus should
-      respect implicit locks held by impl_trx*/
 
       const lock_t *other_lock =
           lock_rec_other_has_expl_req(LOCK_S, block, true, heap_no, impl_trx);
@@ -5215,32 +5061,35 @@ function_exit:
   return (true);
 }
 
-/** Validates the table locks.
- @return true if ok */
-static bool lock_validate_table_locks(
-    const trx_ut_list_t *trx_list) /*!< in: trx list */
-{
-  const trx_t *trx;
-
-  /* We need exclusive access to lock_sys to iterate over trxs' locks */
-  ut_ad(locksys::owns_exclusive_global_latch());
-  ut_ad(trx_sys_mutex_own());
-
-  ut_ad(trx_list == &trx_sys->rw_trx_list);
-
-  for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    const lock_t *lock;
+static bool lock_validate_table_locks_callback(rw_trx_hash_element_t *element,
+                                               void * /*unused*/) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    const lock_t *lock = nullptr;
 
     check_trx_state(trx);
 
     for (lock = UT_LIST_GET_FIRST(trx->lock.trx_locks); lock != nullptr;
          lock = UT_LIST_GET_NEXT(trx_locks, lock)) {
-      if (lock_get_type_low(lock) & LOCK_TABLE) {
+      if ((lock_get_type_low(lock) & LOCK_TABLE) != 0u) {
         lock_table_queue_validate(lock->tab_lock.table);
       }
     }
   }
+  mutex_exit(&element->mutex);
+  return false;
+}
+
+/** Validates the table locks.
+ @return true if ok */
+static bool lock_validate_table_locks() {
+  /* We need exclusive access to lock_sys to iterate over trxs' locks */
+  ut_ad(locksys::owns_exclusive_global_latch());
+
+  trx_sys->rw_trx_hash.iterate(
+      reinterpret_cast<lf_hash_walk_func *>(lock_validate_table_locks_callback),
+      nullptr);
 
   return (true);
 }
@@ -5256,7 +5105,6 @@ static MY_ATTRIBUTE((warn_unused_result)) const lock_t *lock_rec_validate(
   /* Actually we only require to latch the start-th shard, but we happen to
   hold exclusive latch here, which is easier to assert */
   ut_ad(locksys::owns_exclusive_global_latch());
-  ut_ad(trx_sys_mutex_own());
 
   for (const lock_t *lock = static_cast<const lock_t *>(
            HASH_GET_FIRST(lock_sys->rec_hash, start));
@@ -5317,9 +5165,8 @@ bool lock_validate() {
     /* lock_validate_table_locks() needs exclusive global latch, and we will
     inspect record locks from all shards */
     locksys::Global_exclusive_latch_guard guard{};
-    mutex_enter(&trx_sys->mutex);
 
-    ut_a(lock_validate_table_locks(&trx_sys->rw_trx_list));
+    ut_a(lock_validate_table_locks());
 
     /* Iterate over all the record locks and validate the locks. We
     don't want to hog the lock_sys global latch and the trx_sys_t::mutex.
@@ -5338,8 +5185,6 @@ bool lock_validate() {
         pages.insert(std::make_pair(space, page_no));
       }
     }
-
-    mutex_exit(&trx_sys->mutex);
   }
 
   for (page_addr_set::const_iterator it = pages.begin(); it != pages.end();
@@ -5469,7 +5314,7 @@ static void lock_rec_convert_impl_to_expl_for_trx(
     trx_t *trx,               /*!< in/out: active transaction */
     ulint heap_no)            /*!< in: rec heap number to lock */
 {
-  ut_ad(trx_is_referenced(trx));
+  ut_ad(trx->is_referenced());
 
   DEBUG_SYNC_C("before_lock_rec_convert_impl_to_expl_for_trx");
   {
@@ -5512,18 +5357,21 @@ static void lock_rec_convert_impl_to_expl_for_trx(
     trx_mutex_exit(trx);
   }
 
-  trx_release_reference(trx);
+  trx->release_reference();
 
   DEBUG_SYNC_C("after_lock_rec_convert_impl_to_expl_for_trx");
 }
 
 /** If a transaction has an implicit x-lock on a record, but no explicit x-lock
 set on the record, sets one for it.
-@param[in]	block		buffer block of rec
-@param[in]	rec		user record on page
-@param[in]	index		index of record
-@param[in]	offsets		rec_get_offsets(rec, index) */
-static void lock_rec_convert_impl_to_expl(const buf_block_t *block,
+@param[in,out] caller_trx    current transaction
+@param[in]        block         buffer block of rec
+@param[in]        rec                 user record on page
+@param[in]        index                     index of record
+@param[in]        offsets                 rec_get_offsets(rec, index)
+@return whether caller_trx already holds an exclusive lock on rec */
+static bool lock_rec_convert_impl_to_expl(trx_t *caller_trx,
+                                          const buf_block_t *block,
                                           const rec_t *rec, dict_index_t *index,
                                           const ulint *offsets) {
   trx_t *trx;
@@ -5540,22 +5388,34 @@ static void lock_rec_convert_impl_to_expl(const buf_block_t *block,
 
     trx_id = lock_clust_rec_some_has_impl(rec, index, offsets);
 
-    trx = trx_rw_is_active(trx_id, nullptr, true);
+   if (trx_id == 0) {
+      return false;
+    }
+    if (UNIV_UNLIKELY(trx_id == caller_trx->id)) {
+      return true;
+    }
+
+    trx = trx_sys->find(caller_trx, trx_id);
   } else {
     ut_ad(!dict_index_is_online_ddl(index));
 
-    trx = lock_sec_rec_some_has_impl(rec, index, offsets);
-    if (trx) {
+    trx = lock_sec_rec_some_has_impl(caller_trx, rec, index, offsets);
+    if (trx == caller_trx) {
+      trx->release_reference();
+      return true;
+    }
+
+    if (trx != nullptr) {
       DEBUG_SYNC_C("lock_rec_convert_impl_to_expl_will_validate");
-      ut_ad(!lock_rec_other_trx_holds_expl(LOCK_S | LOCK_REC_NOT_GAP, trx, rec,
-                                           block));
+      ut_d(lock_rec_other_trx_holds_expl(caller_trx, LOCK_S | LOCK_REC_NOT_GAP,
+                                         trx, rec, block));
     }
   }
 
   if (trx != nullptr) {
     ulint heap_no = page_rec_get_heap_no(rec);
 
-    ut_ad(trx_is_referenced(trx));
+    ut_ad(trx->is_referenced());
 
     /* If the transaction is still active and has no
     explicit x-lock set on the record, set one for it.
@@ -5564,13 +5424,15 @@ static void lock_rec_convert_impl_to_expl(const buf_block_t *block,
     lock_rec_convert_impl_to_expl_for_trx(block, rec, index, offsets, trx,
                                           heap_no);
   }
+
+  return false;
 }
 
 void lock_rec_convert_active_impl_to_expl(const buf_block_t *block,
                                           const rec_t *rec, dict_index_t *index,
                                           const ulint *offsets, trx_t *trx,
                                           ulint heap_no) {
-  trx_reference(trx, true);
+  trx->reference();
   lock_rec_convert_impl_to_expl_for_trx(block, rec, index, offsets, trx,
                                         heap_no);
 }
@@ -5610,7 +5472,11 @@ dberr_t lock_clust_rec_modify_check_and_lock(
   /* If a transaction has no explicit x-lock set on the record, set one
   for it */
 
-  lock_rec_convert_impl_to_expl(block, rec, index, offsets);
+  if (lock_rec_convert_impl_to_expl(thr_get_trx(thr), block, rec, index,
+                                    offsets)) {
+    /* We already hold an implicit exclusive lock. */
+    return DB_SUCCESS;
+  }
 
   {
     locksys::Shard_latch_guard guard{block->get_page_id()};
@@ -5716,10 +5582,14 @@ dberr_t lock_sec_rec_read_check_and_lock(
   if the max trx id for the page >= min trx id for the trx list or a
   database recovery is running. */
 
-  if ((page_get_max_trx_id(block->frame) >= trx_rw_min_trx_id() ||
+  if ((page_get_max_trx_id(block->frame) >= trx_sys->get_min_trx_id() ||
        recv_recovery_is_on()) &&
-      !page_rec_is_supremum(rec)) {
-    lock_rec_convert_impl_to_expl(block, rec, index, offsets);
+      page_rec_is_supremum(rec) == 0u) {
+    if (lock_rec_convert_impl_to_expl(thr_get_trx(thr), block, rec, index,
+                                      offsets)) {
+      /* We already hold an implicit exclusive lock. */
+      return DB_SUCCESS;
+    }
   }
   {
     locksys::Shard_latch_guard guard{block->get_page_id()};
@@ -5768,7 +5638,11 @@ dberr_t lock_clust_rec_read_check_and_lock(
   heap_no = page_rec_get_heap_no(rec);
 
   if (heap_no != PAGE_HEAP_NO_SUPREMUM) {
-    lock_rec_convert_impl_to_expl(block, rec, index, offsets);
+    if (lock_rec_convert_impl_to_expl(thr_get_trx(thr), block, rec, index,
+                                      offsets)) {
+      /* We already hold an implicit exclusive lock. */
+      return DB_SUCCESS;
+    }
   }
 
   DEBUG_SYNC_C("after_lock_clust_rec_read_check_and_lock_impl_to_expl");
@@ -6228,8 +6102,8 @@ void lock_trx_release_locks(trx_t *trx) /*!< in/out: transaction */
   check_trx_state(trx);
   ut_ad(trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY));
 
-  if (trx_is_referenced(trx)) {
-    while (trx_is_referenced(trx)) {
+  if (trx->is_referenced()) {
+    while (trx->is_referenced()) {
       trx_mutex_exit(trx);
 
       DEBUG_SYNC_C("waiting_trx_is_not_referenced");
@@ -6242,7 +6116,7 @@ void lock_trx_release_locks(trx_t *trx) /*!< in/out: transaction */
     }
   }
 
-  ut_ad(!trx_is_referenced(trx));
+  ut_ad(!trx->is_referenced());
 
   /* If the background thread trx_rollback_or_clean_recovered()
   is still active then there is a chance that the rollback
@@ -6318,28 +6192,17 @@ dberr_t lock_trx_handle_wait(trx_t *trx) /*!< in/out: trx lock state */
 }
 
 #ifdef UNIV_DEBUG
-/** Do an exhaustive check for any locks (table or rec) against the table.
- @return lock if found */
-static const lock_t *lock_table_locks_lookup(
-    const dict_table_t *table,     /*!< in: check if there are
-                                   any locks held on records in
-                                   this table or on the table
-                                   itself */
-    const trx_ut_list_t *trx_list) /*!< in: trx list to check */
-{
-  const trx_t *trx;
-
-  ut_a(table != nullptr);
-  /* We are going to iterate over multiple transactions, so even though we know
-  which table we are looking for we can not narrow required latch to just the
-  shard which contains the table, because accessing trx->lock.trx_locks would be
-  unsafe */
-  ut_ad(locksys::owns_exclusive_global_latch());
-  ut_ad(trx_sys_mutex_own());
+struct lock_table_locks_lookup_arg {
+  const dict_table_t *table;
+  const lock_t *lock;
+};
 
-  for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    const lock_t *lock;
+static bool lock_table_locks_lookup_callback(rw_trx_hash_element_t *element,
+                                             lock_table_locks_lookup_arg *arg) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    const lock_t *lock = nullptr;
 
     check_trx_state(trx);
 
@@ -6350,16 +6213,37 @@ static const lock_t *lock_table_locks_lookup(
       if (lock_get_type_low(lock) == LOCK_REC) {
         ut_ad(!dict_index_is_online_ddl(lock->index) ||
               lock->index->is_clustered());
-        if (lock->index->table == table) {
-          return (lock);
+        if (lock->index->table == arg->table) {
+          arg->lock = lock;
+          break;
         }
-      } else if (lock->tab_lock.table == table) {
-        return (lock);
+      } else if (lock->tab_lock.table == arg->table) {  // NOLINT
+        arg->lock = lock;
+        break;
       }
     }
   }
+  mutex_exit(&element->mutex);
+  return (arg->lock != nullptr);
+}
 
-  return (nullptr);
+/** Do an exhaustive check for any locks (table or rec) against the table.
+ @return lock if found */
+static const lock_t *lock_table_locks_lookup(
+    const dict_table_t *table) /*!< in: check if there are
+                               any locks held on records in
+                               this table or on the table
+                               itself */
+{
+  ut_a(table != nullptr);
+
+  lock_table_locks_lookup_arg arg = {table, nullptr};
+
+  trx_sys->rw_trx_hash.iterate(
+      reinterpret_cast<lf_hash_walk_func *>(lock_table_locks_lookup_callback),
+      &arg);
+
+  return (arg.lock);
 }
 #endif /* UNIV_DEBUG */
 
@@ -6378,11 +6262,7 @@ bool lock_table_has_locks(const dict_table_t *table) {
 
 #ifdef UNIV_DEBUG
   if (!has_locks) {
-    mutex_enter(&trx_sys->mutex);
-
-    ut_ad(!lock_table_locks_lookup(table, &trx_sys->rw_trx_list));
-
-    mutex_exit(&trx_sys->mutex);
+   ut_ad(lock_table_locks_lookup(table) == nullptr);
   }
 #endif /* UNIV_DEBUG */
 
diff --git a/storage/innobase/page/page0page.cc b/storage/innobase/page/page0page.cc
index cc15e33792c..3957821a3d2 100644
--- a/storage/innobase/page/page0page.cc
+++ b/storage/innobase/page/page0page.cc
@@ -2180,7 +2180,7 @@ ibool page_validate(
     trx_id_t max_trx_id = page_get_max_trx_id(page);
     /* This will be 0 during recv_apply_hashed_log_recs(TRUE),
     because the transaction system has not been initialized yet */
-    trx_id_t sys_max_trx_id = trx_sys_get_max_trx_id();
+    trx_id_t sys_max_trx_id = trx_sys->get_max_trx_id();
 
     if (max_trx_id == 0 ||
         (sys_max_trx_id != 0 && max_trx_id > sys_max_trx_id)) {
diff --git a/storage/innobase/read/read0read.cc b/storage/innobase/read/read0read.cc
index 08503a76175..d5539bd0682 100644
--- a/storage/innobase/read/read0read.cc
+++ b/storage/innobase/read/read0read.cc
@@ -182,143 +182,14 @@ will mark their views as closed but not actually free their views.
 /** Minimum number of elements to reserve in ReadView::ids_t */
 static const ulint MIN_TRX_IDS = 32;
 
-#ifdef UNIV_DEBUG
-/** Functor to validate the view list. */
-struct ViewCheck {
-  ViewCheck() : m_prev_view() {}
-
-  void operator()(const ReadView *view) {
-    ut_a(m_prev_view == nullptr || view->is_closed() || view->le(m_prev_view));
-
-    m_prev_view = view;
-  }
-
-  const ReadView *m_prev_view;
-};
-
-/**
-Validates a read view list. */
-
-bool MVCC::validate() const {
-  ViewCheck check;
-
-  ut_ad(mutex_own(&trx_sys->mutex));
-
-  ut_list_map(m_views, check);
-
-  return (true);
-}
-#endif /* UNIV_DEBUG */
-
-/**
-Try and increase the size of the array. Old elements are
-copied across.
-@param n 		Make space for n elements */
-
-void ReadView::ids_t::reserve(ulint n) {
-  if (n <= capacity()) {
-    return;
-  }
-
-  /** Keep a minimum threshold */
-  if (n < MIN_TRX_IDS) {
-    n = MIN_TRX_IDS;
-  }
-
-  value_type *p = m_ptr;
-
-  m_ptr = UT_NEW_ARRAY_NOKEY(value_type, n);
-
-  m_reserved = n;
-
-  ut_ad(size() < capacity());
-
-  if (p != nullptr) {
-    ::memmove(m_ptr, p, size() * sizeof(value_type));
-
-    UT_DELETE_ARRAY(p);
-  }
-}
-
-/**
-Copy and overwrite this array contents
-@param start		Source array
-@param end		Pointer to end of array */
-
-void ReadView::ids_t::assign(const value_type *start, const value_type *end) {
-  ut_ad(end >= start);
-
-  ulint n = end - start;
-
-  /* No need to copy the old contents across during reserve(). */
-  clear();
-
-  /* Create extra space if required. */
-  reserve(n);
-
-  resize(n);
-
-  ut_ad(size() == n);
-
-  ::memmove(m_ptr, start, size() * sizeof(value_type));
-}
-
-/**
-Append a value to the array.
-@param value		the value to append */
-
-void ReadView::ids_t::push_back(value_type value) {
-  if (capacity() <= size()) {
-    reserve(size() * 2);
-  }
-
-  m_ptr[m_size++] = value;
-  ut_ad(size() <= capacity());
-}
-
-/**
-Insert the value in the correct slot, preserving the order. Doesn't
-check for duplicates. */
-
-void ReadView::ids_t::insert(value_type value) {
-  ut_ad(value > 0);
-
-  reserve(size() + 1);
-
-  if (empty() || back() < value) {
-    push_back(value);
-    return;
-  }
-
-  value_type *end = data() + size();
-  value_type *ub = std::upper_bound(data(), end, value);
-
-  if (ub == end) {
-    push_back(value);
-  } else {
-    ut_ad(ub < end);
-
-    ulint n_elems = std::distance(ub, end);
-    ulint n = n_elems * sizeof(value_type);
-
-    /* Note: Copying overlapped memory locations. */
-    ::memmove(ub + 1, ub, n);
-
-    *ub = value;
-
-    resize(size() + 1);
-  }
-}
-
 /**
 ReadView constructor */
 ReadView::ReadView()
     : m_low_limit_id(),
       m_up_limit_id(),
       m_creator_trx_id(),
-      m_ids(),
-      m_low_limit_no() {
-  ut_d(::memset(&m_view_list, 0x0, sizeof(m_view_list)));
+      m_low_limit_no(),
+      m_state(READ_VIEW_STATE_CLOSED) {
   ut_d(m_view_low_limit_no = 0);
 }
 
@@ -328,420 +199,121 @@ ReadView::~ReadView() {
   // Do nothing
 }
 
-/** Constructor
-@param size		Number of views to pre-allocate */
-MVCC::MVCC(ulint size) {
-  UT_LIST_INIT(m_free, &ReadView::m_view_list);
-  UT_LIST_INIT(m_views, &ReadView::m_view_list);
-
-  for (ulint i = 0; i < size; ++i) {
-    ReadView *view = UT_NEW_NOKEY(ReadView());
-
-    UT_LIST_ADD_FIRST(m_free, view);
-  }
-}
-
-MVCC::~MVCC() {
-  for (ReadView *view = UT_LIST_GET_FIRST(m_free); view != nullptr;
-       view = UT_LIST_GET_FIRST(m_free)) {
-    UT_LIST_REMOVE(m_free, view);
-
-    UT_DELETE(view);
-  }
-
-  ut_a(UT_LIST_GET_LEN(m_views) == 0);
-}
-
-/**
-Copy the transaction ids from the source vector */
-
-void ReadView::copy_trx_ids(const trx_ids_t &trx_ids) {
-  ulint size = trx_ids.size();
-
-  if (m_creator_trx_id > 0) {
-    ut_ad(size > 0);
-    --size;
+void ReadView::copy(const ReadView &other) {
+  ut_ad(&other != this);
+  if (m_low_limit_no > other.m_low_limit_no) {
+    m_low_limit_no = other.m_low_limit_no;
   }
-
-  if (size == 0) {
-    m_ids.clear();
-    return;
+  if (m_low_limit_id > other.m_low_limit_id) {
+    m_low_limit_id = other.m_low_limit_id;
   }
 
-  m_ids.reserve(size);
-  m_ids.resize(size);
-
-  ids_t::value_type *p = m_ids.data();
-
-  /* Copy all the trx_ids except the creator trx id */
-
-  if (m_creator_trx_id > 0) {
-    /* Note: We go through all this trouble because it is
-    unclear whether std::vector::resize() will cause an
-    overhead or not. We should test this extensively and
-    if the vector to vector copy is fast enough then get
-    rid of this code and replace it with more readable
-    and obvious code. The code below does exactly one copy,
-    and filters out the creator's trx id. */
-
-    trx_ids_t::const_iterator it =
-        std::lower_bound(trx_ids.begin(), trx_ids.end(), m_creator_trx_id);
-
-    ut_ad(it != trx_ids.end() && *it == m_creator_trx_id);
-
-    ulint i = std::distance(trx_ids.begin(), it);
-    ulint n = i * sizeof(trx_ids_t::value_type);
-
-    ::memmove(p, &trx_ids[0], n);
-
-    n = (trx_ids.size() - i - 1) * sizeof(trx_ids_t::value_type);
-
-    ut_ad(i + (n / sizeof(trx_ids_t::value_type)) == m_ids.size());
+  ut_d(m_view_low_limit_no = other.m_view_low_limit_no);
 
-    if (n > 0) {
-      ::memmove(p + i, &trx_ids[i + 1], n);
+  auto dst = m_ids.begin();
+  for (auto id : other.m_ids) {
+    if (id >= m_low_limit_id) {
+      break;
     }
-  } else {
-    ulint n = size * sizeof(trx_ids_t::value_type);
 
-    ::memmove(p, &trx_ids[0], n);
+  loop:
+    if (dst == m_ids.end()) {
+      m_ids.push_back(id);
+      dst = m_ids.end();
+      continue;
+    }
+    if (*dst < id) {
+      dst++;
+      goto loop;
+    } else if (*dst > id) {
+      dst = m_ids.insert(dst, id) + 1;
+    }
   }
+  m_ids.erase(std::lower_bound(dst, m_ids.end(), m_low_limit_id), m_ids.end());
 
-  m_up_limit_id = m_ids.front();
-
-#ifdef UNIV_DEBUG
-  /* Assert that all transaction ids in list are active. */
-  for (trx_ids_t::const_iterator it = trx_ids.begin(); it != trx_ids.end();
-       ++it) {
-    trx_t *trx = trx_get_rw_trx_by_id(*it);
-    ut_ad(trx != nullptr);
-    ut_ad(trx->state == TRX_STATE_ACTIVE || trx->state == TRX_STATE_PREPARED);
-  }
-#endif /* UNIV_DEBUG */
+  m_up_limit_id = m_ids.empty() ? m_low_limit_id : m_ids.front();
+  ut_ad(m_up_limit_id <= m_low_limit_id);
 }
 
-/**
-Opens a read view where exactly the transactions serialized before this
+/** Creates a snapshot where exactly the transaction serialized before this
 point in time are seen in the view.
-@param id		Creator transaction id */
-
-void ReadView::prepare(trx_id_t id) {
-  ut_ad(mutex_own(&trx_sys->mutex));
-
-  m_creator_trx_id = id;
-
-  m_low_limit_no = m_low_limit_id = m_up_limit_id = trx_sys->max_trx_id;
-
-  if (!trx_sys->rw_trx_ids.empty()) {
-    copy_trx_ids(trx_sys->rw_trx_ids);
-  } else {
-    m_ids.clear();
-  }
 
+@param[in,out] trx transaction */
+void ReadView::snapshot(trx_t *trx) {
+  trx_sys->snapshot_ids(trx, &m_ids, &m_low_limit_id, &m_low_limit_no);
+  std::sort(m_ids.begin(), m_ids.end());
+  m_up_limit_id = m_ids.empty() ? m_low_limit_id : m_ids.front();
   ut_ad(m_up_limit_id <= m_low_limit_id);
-
-  if (UT_LIST_GET_LEN(trx_sys->serialisation_list) > 0) {
-    const trx_t *trx;
-
-    trx = UT_LIST_GET_FIRST(trx_sys->serialisation_list);
-
-    if (trx->no < m_low_limit_no) {
-      m_low_limit_no = trx->no;
-    }
-  }
-
   ut_d(m_view_low_limit_no = m_low_limit_no);
-  m_closed = false;
-}
-
-/**
-Find a free view from the active list, if none found then allocate
-a new view.
-@return a view to use */
-
-ReadView *MVCC::get_view() {
-  ut_ad(mutex_own(&trx_sys->mutex));
-
-  ReadView *view;
-
-  if (UT_LIST_GET_LEN(m_free) > 0) {
-    view = UT_LIST_GET_FIRST(m_free);
-    UT_LIST_REMOVE(m_free, view);
-  } else {
-    view = UT_NEW_NOKEY(ReadView());
-
-    if (view == nullptr) {
-      ib::error(ER_IB_MSG_918) << "Failed to allocate MVCC view";
-    }
-  }
-
-  return (view);
-}
-
-/**
-Release a view that is inactive but not closed. Caller must own
-the trx_sys_t::mutex.
-@param view		View to release */
-void MVCC::view_release(ReadView *&view) {
-  ut_ad(!srv_read_only_mode);
-  ut_ad(trx_sys_mutex_own());
-
-  uintptr_t p = reinterpret_cast<uintptr_t>(view);
-
-  ut_a(p & 0x1);
-
-  view = reinterpret_cast<ReadView *>(p & ~1);
-
-  ut_ad(view->m_closed);
-
-  /** RW transactions should not free their views here. Their views
-  should freed using view_close_view() */
-
-  ut_ad(view->m_creator_trx_id == 0);
-
-  UT_LIST_REMOVE(m_views, view);
-
-  UT_LIST_ADD_LAST(m_free, view);
-
-  view = nullptr;
 }
 
-/**
-Allocate and create a view.
-@param view		view owned by this class created for the
-                        caller. Must be freed by calling view_close()
-@param trx		transaction instance of caller */
-void MVCC::view_open(ReadView *&view, trx_t *trx) {
-  ut_ad(!srv_read_only_mode);
-
-  /** If no new RW transaction has been started since the last view
-  was created then reuse the the existing view. */
-  if (view != nullptr) {
-    uintptr_t p = reinterpret_cast<uintptr_t>(view);
-
-    view = reinterpret_cast<ReadView *>(p & ~1);
-
-    ut_ad(view->m_closed);
-
-    /* NOTE: This can be optimised further, for now we only
-    resuse the view iff there are no active RW transactions.
+/** Open a read view where exactly the transactions serialized before this point
+in time are seen in the view.
 
-    There is an inherent race here between purge and this
-    thread. Purge will skip views that are marked as closed.
-    Therefore we must set the low limit id after we reset the
-    closed status after the check. */
+View becomes visible to purge thread.
 
-    if (trx_is_autocommit_non_locking(trx) && view->empty()) {
-      view->m_closed = false;
-
-      if (view->m_low_limit_id == trx_sys_get_max_trx_id()) {
+@param[in,out] trx transaction*/
+void ReadView::open(trx_t *trx) {
+  ut_ad(this == &trx->read_view);
+  switch (m_state.load(std::memory_order_relaxed)) {
+    case READ_VIEW_STATE_OPEN:
+      ut_ad(!srv_read_only_mode);
+      return;
+    case READ_VIEW_STATE_CLOSED:
+      if (srv_read_only_mode) {
         return;
-      } else {
-        view->m_closed = true;
       }
-    }
-
-    mutex_enter(&trx_sys->mutex);
-
-    UT_LIST_REMOVE(m_views, view);
-
-  } else {
-    mutex_enter(&trx_sys->mutex);
-
-    view = get_view();
-  }
-
-  if (view != nullptr) {
-    view->prepare(trx->id);
-
-    UT_LIST_ADD_FIRST(m_views, view);
-
-    ut_ad(!view->is_closed());
-
-    ut_ad(validate());
-  }
-
-  trx_sys_mutex_exit();
-}
-
-ReadView *MVCC::get_view_created_by_trx_id(trx_id_t trx_id) const {
-  ReadView *view;
-
-  ut_ad(mutex_own(&trx_sys->mutex));
-
-  for (view = UT_LIST_GET_LAST(m_views); view != nullptr;
-       view = UT_LIST_GET_PREV(m_view_list, view)) {
-    if (view->is_closed()) {
-      continue;
-    }
-
-    if (view->m_creator_trx_id == trx_id) {
-      break;
-    }
-  }
-
-  return (view);
-}
-
-/**
-Get the oldest (active) view in the system.
-@return oldest view if found or NULL */
+      /* Reuse closed view if there were no read-write transactions since (and
+      at) its creation time.
+
+      Original comment states: there is an inherent race between purge and this
+      thread.
+
+      To avoid this race we should've checked trx_sys->get_max_trx_id() and set
+      state to READ_VIEW_STATE_OPEN atomically under trx_sys->mutex protection.
+      But we'are cutting edges to achieve greate scalability.
+
+      There's at least two types of concurrent threads interested in this value:
+      purge coordinator thread (sees trx_sys_t::clone_oldest_view()) and InnoDB
+      monitor thread (see lock_trx_print_wait_and_mvcc_state()).
+
+      What bad things can happen because we allow this race?
+
+      Speculative execution may reorder state change before get_max_trx_id(). In
+      this case purge thread has short gap to clone outdated view. Which is
+      probably not that bad: it just won't be able to purge things that it was
+      actually allowed to purge for a short while.
+
+      This thread may as well get suspended after trx_sys->get_max_trx_id() and
+      before state is set to READ_VIEW_STATE_OPEN. New read-write transaction
+      may get started, committed and purged meanwhile. It is acceptable as well,
+      since this view doesn't see it. */
+      if (trx_is_autocommit_non_locking(trx) && m_ids.empty() &&
+          m_low_limit_id == trx_sys->get_max_trx_id()) {
+        goto reopen;
+      }
 
-ReadView *MVCC::get_oldest_view() const {
-  ReadView *view;
+      /* Can't reuse view, take new snapshot.
 
-  ut_ad(mutex_own(&trx_sys->mutex));
+      Alas this empty critical section is simplest way to make sure concurrent
+      purge thread completed snapshot copy. Of course purge thread may come
+      again and try to copy once again after we release this mutex, but in this
+      case it is guaranteed to see READ_VIEW_STATE_REGISTERED and thus it'll
+      skip this view.
 
-  for (view = UT_LIST_GET_LAST(m_views); view != nullptr;
-       view = UT_LIST_GET_PREV(m_view_list, view)) {
-    if (!view->is_closed()) {
+      This critical section can be replaced with new state, which purge thread
+      would set to inform us to wait until it completes snapshot. However it'd
+      complicate m_state even further. */
+      mutex_enter(&trx_sys->mutex);
+      mutex_exit(&trx_sys->mutex);
+      m_state.store(READ_VIEW_STATE_SNAPSHOT, std::memory_order_relaxed);
       break;
-    }
+    default:
+      ut_ad(false);
   }
 
-  return (view);
-}
-
-/**
-Copy state from another view. Must call copy_complete() to finish.
-@param other		view to copy from */
-
-void ReadView::copy_prepare(const ReadView &other) {
-  ut_ad(&other != this);
-
-  if (!other.m_ids.empty()) {
-    const ids_t::value_type *p = other.m_ids.data();
-
-    m_ids.assign(p, p + other.m_ids.size());
-  } else {
-    m_ids.clear();
-  }
-
-  m_up_limit_id = other.m_up_limit_id;
-
-  m_low_limit_no = other.m_low_limit_no;
-
-  ut_d(m_view_low_limit_no = other.m_view_low_limit_no);
-
-  m_low_limit_id = other.m_low_limit_id;
-
-  m_creator_trx_id = other.m_creator_trx_id;
-}
-
-/**
-Complete the copy, insert the creator transaction id into the
-m_ids too and adjust the m_up_limit_id, if required */
-
-void ReadView::copy_complete() {
-  ut_ad(!trx_sys_mutex_own());
-
-  if (m_creator_trx_id > 0) {
-    m_ids.insert(m_creator_trx_id);
-  }
-
-  if (!m_ids.empty()) {
-    /* The last active transaction has the smallest id. */
-    m_up_limit_id = std::min(m_ids.front(), m_up_limit_id);
-  }
-
-  ut_ad(m_up_limit_id <= m_low_limit_id);
-
-  /* We added the creator transaction ID to the m_ids. */
-  m_creator_trx_id = 0;
-}
-
-/** Clones the oldest view and stores it in view. No need to
-call view_close(). The caller owns the view that is passed in.
-This function is called by Purge to determine whether it should
-purge the delete marked record or not.
-@param view		Preallocated view, owned by the caller */
-
-void MVCC::clone_oldest_view(ReadView *view) {
-  mutex_enter(&trx_sys->mutex);
-
-  ReadView *oldest_view = get_oldest_view();
-
-  if (oldest_view == nullptr) {
-    view->prepare(0);
-
-    trx_sys_mutex_exit();
-
-  } else {
-    view->copy_prepare(*oldest_view);
-
-    trx_sys_mutex_exit();
-
-    view->copy_complete();
-  }
-  /* Update view to block purging transaction till GTID is persisted. */
-  auto &gtid_persistor = clone_sys->get_gtid_persistor();
-  auto gtid_oldest_trxno = gtid_persistor.get_oldest_trx_no();
-  view->reduce_low_limit(gtid_oldest_trxno);
-}
-
-/**
-@return the number of active views */
-
-ulint MVCC::size() const {
-  trx_sys_mutex_enter();
-
-  ulint size = 0;
-
-  for (const ReadView *view = UT_LIST_GET_FIRST(m_views); view != nullptr;
-       view = UT_LIST_GET_NEXT(m_view_list, view)) {
-    if (!view->is_closed()) {
-      ++size;
-    }
-  }
-
-  trx_sys_mutex_exit();
-
-  return (size);
-}
-
-/**
-Close a view created by the above function.
-@param view		view allocated by trx_open.
-@param own_mutex	true if caller owns trx_sys_t::mutex */
-
-void MVCC::view_close(ReadView *&view, bool own_mutex) {
-  uintptr_t p = reinterpret_cast<uintptr_t>(view);
-
-  /* Note: The assumption here is that AC-NL-RO transactions will
-  call this function with own_mutex == false. */
-  if (!own_mutex) {
-    /* Sanitise the pointer first. */
-    ReadView *ptr = reinterpret_cast<ReadView *>(p & ~1);
-
-    /* Note this can be called for a read view that
-    was already closed. */
-    ptr->m_closed = true;
-
-    /* Set the view as closed. */
-    view = reinterpret_cast<ReadView *>(p | 0x1);
-  } else {
-    view = reinterpret_cast<ReadView *>(p & ~1);
-
-    view->close();
-
-    UT_LIST_REMOVE(m_views, view);
-    UT_LIST_ADD_LAST(m_free, view);
-
-    ut_ad(validate());
-
-    view = nullptr;
-  }
-}
-
-/**
-Set the view creator transaction id. Note: This shouldbe set only
-for views created by RW transactions.
-@param view		Set the creator trx id for this view
-@param id		Transaction id to set */
-
-void MVCC::set_view_creator_trx_id(ReadView *view, trx_id_t id) {
-  ut_ad(id > 0);
-  ut_ad(mutex_own(&trx_sys->mutex));
-
-  view->creator_trx_id(id);
+  snapshot(trx);
+reopen:
+  m_creator_trx_id = trx->id;
+  m_state.store(READ_VIEW_STATE_OPEN, std::memory_order_release);
 }
diff --git a/storage/innobase/row/row0ins.cc b/storage/innobase/row/row0ins.cc
index 63e4af7c9e1..362ad021587 100644
--- a/storage/innobase/row/row0ins.cc
+++ b/storage/innobase/row/row0ins.cc
@@ -710,8 +710,6 @@ static void row_ins_foreign_trx_print(trx_t *trx) /*!< in: transaction */
     heap_size = mem_heap_get_size(trx->lock.lock_heap);
   }
 
-  trx_sys_mutex_enter();
-
   mutex_enter(&dict_foreign_err_mutex);
   rewind(dict_foreign_err_file);
   ut_print_timestamp(dict_foreign_err_file);
@@ -720,8 +718,6 @@ static void row_ins_foreign_trx_print(trx_t *trx) /*!< in: transaction */
   trx_print_low(dict_foreign_err_file, trx, 600, n_rec_locks, n_trx_locks,
                 heap_size);
 
-  trx_sys_mutex_exit();
-
   ut_ad(mutex_own(&dict_foreign_err_mutex));
 }
 
diff --git a/storage/innobase/row/row0merge.cc b/storage/innobase/row/row0merge.cc
index ebfd5f3d869..f1d562555c8 100644
--- a/storage/innobase/row/row0merge.cc
+++ b/storage/innobase/row/row0merge.cc
@@ -1915,19 +1915,19 @@ static MY_ATTRIBUTE((warn_unused_result)) dberr_t
       ONLINE_INDEX_COMPLETE state between the time
       the DML thread has updated the clustered index
       but has not yet accessed secondary index. */
-      ut_ad(MVCC::is_view_active(trx->read_view));
+      ut_ad(trx->read_view.is_open());
 
-      if (!trx->read_view->changes_visible(
+      if (!trx->read_view.changes_visible(
               row_get_rec_trx_id(rec, clust_index, offsets), old_table->name)) {
         rec_t *old_vers;
 
         row_vers_build_for_consistent_read(rec, &mtr, clust_index, &offsets,
-                                           trx->read_view, &row_heap, row_heap,
+                                           &trx->read_view, &row_heap, row_heap,
                                            &old_vers, nullptr, nullptr);
 
         rec = old_vers;
 
-        if (!rec) {
+        if (rec == nullptr) {
           continue;
         }
       }
diff --git a/storage/innobase/row/row0mysql.cc b/storage/innobase/row/row0mysql.cc
index dd7fb46f29d..acec70828e1 100644
--- a/storage/innobase/row/row0mysql.cc
+++ b/storage/innobase/row/row0mysql.cc
@@ -4672,7 +4672,7 @@ dberr_t row_scan_index_for_mysql(row_prebuilt_t *prebuilt, dict_index_t *index,
       /* No INSERT INTO  ... SELECT  and non-locking selects only. */
       trx_start_if_not_started_xa(prebuilt->trx, false);
 
-      trx_assign_read_view(prebuilt->trx);
+      prebuilt->trx->read_view.open(prebuilt->trx);
 
       auto trx = prebuilt->trx;
 
diff --git a/storage/innobase/row/row0pread.cc b/storage/innobase/row/row0pread.cc
index 0639ac5c01f..9fadf7b4910 100644
--- a/storage/innobase/row/row0pread.cc
+++ b/storage/innobase/row/row0pread.cc
@@ -335,10 +335,8 @@ bool Parallel_reader::Scan_ctx::check_visibility(const rec_t *&rec,
                                                  mtr_t *mtr) {
   const auto table_name = m_config.m_index->table->name;
 
-  ut_ad(m_trx->read_view == nullptr || MVCC::is_view_active(m_trx->read_view));
-
-  if (m_trx->read_view != nullptr) {
-    auto view = m_trx->read_view;
+  if (m_trx->read_view.is_open()) {
+    ReadView* view = const_cast<ReadView*>(&m_trx->read_view);
 
     if (m_config.m_index->is_clustered()) {
       trx_id_t rec_trx_id;
diff --git a/storage/innobase/row/row0row.cc b/storage/innobase/row/row0row.cc
index 24ed70e4a59..a7fa0a65257 100644
--- a/storage/innobase/row/row0row.cc
+++ b/storage/innobase/row/row0row.cc
@@ -385,16 +385,9 @@ static inline dtuple_t *row_build_low(ulint type, const dict_index_t *index,
   }
 
 #if defined UNIV_DEBUG || defined UNIV_BLOB_LIGHT_DEBUG
-  /* Some blob refs can be NULL during crash recovery before
-  trx_rollback_active() has completed execution, or when a concurrently
-  executing insert or update has committed the B-tree mini-transaction
-  but has not yet managed to restore the cursor position for writing
-  the big_rec. Note that the mini-transaction can be committed multiple
-  times, and the cursor restore can happen multiple times for single
-  insert or update statement.  */
   ut_a(!rec_offs_any_null_extern(rec, offsets) ||
-       trx_rw_is_active(row_get_rec_trx_id(rec, index, offsets), nullptr,
-                        false));
+       trx_sys->is_registered(current_trx(),
+                              row_get_rec_trx_id(rec, index, offsets)));
 #endif /* UNIV_DEBUG || UNIV_BLOB_LIGHT_DEBUG */
 
   if (type != ROW_COPY_POINTERS) {
diff --git a/storage/innobase/row/row0sel.cc b/storage/innobase/row/row0sel.cc
index 34f116cf714..d85d3f19f48 100644
--- a/storage/innobase/row/row0sel.cc
+++ b/storage/innobase/row/row0sel.cc
@@ -752,9 +752,9 @@ static void row_sel_build_committed_vers_for_mysql(
     prebuilt->old_vers_heap = mem_heap_create(rec_offs_size(*offsets));
   }
 
-  row_vers_build_for_semi_consistent_read(rec, mtr, clust_index, offsets,
-                                          offset_heap, prebuilt->old_vers_heap,
-                                          old_vers, vrow);
+  row_vers_build_for_semi_consistent_read(
+      prebuilt->trx, rec, mtr, clust_index, offsets, offset_heap,
+      prebuilt->old_vers_heap, old_vers, vrow);
 }
 
 /** Tests the conditions which determine when the index segment we are searching
@@ -2169,10 +2169,11 @@ que_thr_t *row_sel_step(que_thr_t *thr) /*!< in: query thread */
 
     if (node->consistent_read) {
       /* Assign a read view for the query */
-      trx_assign_read_view(thr_get_trx(thr));
+      trx_t *trx = thr_get_trx(thr);
+      trx->read_view.open(trx);
 
-      if (thr_get_trx(thr)->read_view != nullptr) {
-        node->read_view = thr_get_trx(thr)->read_view;
+      if (trx->read_view.is_open()) {
+        node->read_view = &trx->read_view;
       } else {
         node->read_view = nullptr;
       }
@@ -3347,7 +3348,7 @@ dberr_t Row_sel_get_clust_rec_for_mysql::operator()(
       if (clust_rec != cached_clust_rec) {
         /* The following call returns 'offsets' associated with 'old_vers' */
         err = row_sel_build_prev_vers_for_mysql(
-            trx->read_view, clust_index, prebuilt, clust_rec, offsets,
+            &trx->read_view, clust_index, prebuilt, clust_rec, offsets,
             offset_heap, &old_vers, vrow, mtr, lob_undo);
 
         if (err != DB_SUCCESS) {
@@ -4647,7 +4648,7 @@ dberr_t row_search_mvcc(byte *buf, page_cur_mode_t mode,
     if (trx->mysql_n_tables_locked == 0 && !prebuilt->ins_sel_stmt &&
         prebuilt->select_lock_type == LOCK_NONE &&
         trx->isolation_level > TRX_ISO_READ_UNCOMMITTED &&
-        MVCC::is_view_active(trx->read_view)) {
+        trx->read_view.is_open()) {
       /* This is a SELECT query done as a consistent read,
       and the read view has already been allocated:
       let us try a search shortcut through the hash
@@ -4758,7 +4759,7 @@ dberr_t row_search_mvcc(byte *buf, page_cur_mode_t mode,
   ut_ad(!trx_is_started(trx) || trx->state == TRX_STATE_ACTIVE);
 
   ut_ad(prebuilt->sql_stat_start || prebuilt->select_lock_type != LOCK_NONE ||
-        MVCC::is_view_active(trx->read_view) || srv_read_only_mode);
+        trx->read_view.is_open() || srv_read_only_mode);
 
   trx_start_if_not_started(trx, false);
 
@@ -4801,7 +4802,7 @@ dberr_t row_search_mvcc(byte *buf, page_cur_mode_t mode,
   if (!prebuilt->sql_stat_start) {
     /* No need to set an intention lock or assign a read view */
 
-    if (!MVCC::is_view_active(trx->read_view) && !srv_read_only_mode &&
+    if (!trx->read_view.is_open() && !srv_read_only_mode &&
         prebuilt->select_lock_type == LOCK_NONE) {
       ib::error(ER_IB_MSG_1031) << "MySQL is trying to perform a"
                                    " consistent read but the read view is not"
@@ -4815,7 +4816,7 @@ dberr_t row_search_mvcc(byte *buf, page_cur_mode_t mode,
     /* Assign a read view for the query */
 
     if (!srv_read_only_mode) {
-      trx_assign_read_view(trx);
+      trx->read_view.open(trx);
     }
 
     prebuilt->sql_stat_start = FALSE;
@@ -5322,7 +5323,7 @@ rec_loop:
         rec_t *old_vers;
         /* The following call returns 'offsets' associated with 'old_vers' */
         err = row_sel_build_prev_vers_for_mysql(
-            trx->read_view, clust_index, prebuilt, rec, &offsets, &heap,
+            &trx->read_view, clust_index, prebuilt, rec, &offsets, &heap,
             &old_vers, need_vrow ? &vrow : nullptr, &mtr,
             prebuilt->get_lob_undo());
 
@@ -5350,7 +5351,7 @@ rec_loop:
       ut_ad(!index->is_clustered());
 
       if (!srv_read_only_mode &&
-          !lock_sec_rec_cons_read_sees(rec, index, trx->read_view)) {
+          !lock_sec_rec_cons_read_sees(rec, index, &trx->read_view)) {
         /* We should look at the clustered index.
         However, as this is a non-locking read,
         we can skip the clustered index lookup if
diff --git a/storage/innobase/row/row0upd.cc b/storage/innobase/row/row0upd.cc
index e46b5ffc4fe..2bdcf214530 100644
--- a/storage/innobase/row/row0upd.cc
+++ b/storage/innobase/row/row0upd.cc
@@ -3057,7 +3057,8 @@ static MY_ATTRIBUTE((warn_unused_result)) dberr_t
     }
   }
 
-  ut_ad(lock_trx_has_rec_x_lock(thr, index->table, btr_pcur_get_block(pcur),
+  ut_ad((row_get_rec_trx_id(rec, index, offsets) == trx->id) ||
+        lock_trx_has_rec_x_lock(thr, index->table, btr_pcur_get_block(pcur),
                                 page_rec_get_heap_no(rec)));
 
   /* NOTE: the following function calls will also commit mtr */
diff --git a/storage/innobase/row/row0vers.cc b/storage/innobase/row/row0vers.cc
index 511ed3c3911..6e59b596299 100644
--- a/storage/innobase/row/row0vers.cc
+++ b/storage/innobase/row/row0vers.cc
@@ -229,6 +229,7 @@ looking_for_match to the given sec_rec is found among versions created by trx_id
 or the one version before them
 */
 static bool row_vers_find_matching(
+    trx_t *caller_trx, /*!< in/out: trx of current thread */
     bool looking_for_match, const dict_index_t *const clust_index,
     const rec_t *const clust_rec, ulint *&clust_offsets,
     const dict_index_t *const sec_index, const rec_t *const sec_rec,
@@ -256,8 +257,8 @@ static bool row_vers_find_matching(
     /* The oldest visible clustered index version must not be
     delete-marked, because we never start a transaction by
     inserting a delete-marked record. */
-    ut_ad(prev_version || !rec_get_deleted_flag(version, comp) ||
-          !trx_rw_is_active(trx_id, nullptr, false));
+    ut_ad(prev_version != nullptr || rec_get_deleted_flag(version, comp) == 0 ||
+          !trx_sys->is_registered(caller_trx, trx_id));
 
     /* Free version and clust_offsets. */
     mem_heap_free(old_heap);
@@ -284,6 +285,7 @@ static bool row_vers_find_matching(
 
 /** Finds out if an active transaction has inserted or modified a secondary
  index record.
+ @param[in/out]   caller_trx    trx of current thread
  @param[in]       clust_rec     clustered index record
  @param[in]       clust_index   the clustered index
  @param[in]       sec_rec       secondary index record
@@ -295,7 +297,8 @@ static bool row_vers_find_matching(
  negatives. The caller must confirm all positive results by calling checking if
  the trx is still active.*/
 UNIV_INLINE
-trx_t *row_vers_impl_x_locked_low(const rec_t *const clust_rec,
+trx_t *row_vers_impl_x_locked_low(trx_t *caller_trx,
+                                  const rec_t *const clust_rec,
                                   const dict_index_t *const clust_index,
                                   const rec_t *const sec_rec,
                                   const dict_index_t *const sec_index,
@@ -500,17 +503,23 @@ trx_t *row_vers_impl_x_locked_low(const rec_t *const clust_rec,
   trx_id = row_get_rec_trx_id(clust_rec, clust_index, clust_offsets);
   corrupt = FALSE;
 
-  trx_t *trx = trx_rw_is_active(trx_id, &corrupt, true);
+  trx_t *trx;
 
-  if (trx == nullptr) {
-    /* The transaction that modified or inserted clust_rec is no
-    longer active, or it is corrupt: no implicit lock on rec */
-    if (corrupt) {
-      lock_report_trx_id_insanity(trx_id, clust_rec, clust_index, clust_offsets,
-                                  trx_sys_get_max_trx_id());
+  if (trx_id == caller_trx->id) {
+    trx = caller_trx;
+    trx->reference();
+  } else {
+    trx = trx_sys->find(caller_trx, trx_id);
+    if (trx == nullptr) {
+      /* The transaction that modified or inserted clust_rec is no
+      longer active, or it is corrupt: no implicit lock on rec */
+      if (corrupt != 0u) {
+        lock_report_trx_id_insanity(trx_id, clust_rec, clust_index,
+                                    clust_offsets, trx_sys->get_max_trx_id());
+      }
+      mem_heap_free(heap);
+      return nullptr;
     }
-    mem_heap_free(heap);
-    return nullptr;
   }
 
   comp = page_rec_is_comp(sec_rec);
@@ -520,10 +529,10 @@ trx_t *row_vers_impl_x_locked_low(const rec_t *const clust_rec,
 
   bool looking_for_match = rec_get_deleted_flag(sec_rec, comp);
 
-  if (!row_vers_find_matching(looking_for_match, clust_index, clust_rec,
-                              clust_offsets, sec_index, sec_rec, sec_offsets,
-                              comp, trx_id, mtr, heap)) {
-    trx_release_reference(trx);
+  if (!row_vers_find_matching(caller_trx, looking_for_match, clust_index,
+                              clust_rec, clust_offsets, sec_index, sec_rec,
+                              sec_offsets, comp, trx_id, mtr, heap)) {
+    trx->release_reference();
     trx = nullptr;
   }
 
@@ -533,8 +542,8 @@ trx_t *row_vers_impl_x_locked_low(const rec_t *const clust_rec,
   return trx;
 }
 
-trx_t *row_vers_impl_x_locked(const rec_t *rec, const dict_index_t *index,
-                              const ulint *offsets) {
+trx_t *row_vers_impl_x_locked(trx_t *caller_trx, const rec_t *rec,
+                              const dict_index_t *index, const ulint *offsets) {
   mtr_t mtr;
   trx_t *trx;
   const rec_t *clust_rec;
@@ -570,10 +579,10 @@ trx_t *row_vers_impl_x_locked(const rec_t *rec, const dict_index_t *index,
 
     trx = nullptr;
   } else {
-    trx = row_vers_impl_x_locked_low(clust_rec, clust_index, rec, index,
-                                     offsets, &mtr);
+    trx = row_vers_impl_x_locked_low(caller_trx, clust_rec, clust_index, rec,
+                                     index, offsets, &mtr);
 
-    ut_ad(trx == nullptr || trx_is_referenced(trx));
+    ut_ad(trx == nullptr || trx->is_referenced());
   }
 
   mtr_commit(&mtr);
@@ -1355,6 +1364,7 @@ dberr_t row_vers_build_for_consistent_read(
 /** Constructs the last committed version of a clustered index record,
  which should be seen by a semi-consistent read. */
 void row_vers_build_for_semi_consistent_read(
+    trx_t *caller_trx,        /*!< in/out: trx of current thread */
     const rec_t *rec,         /*!< in: record in a clustered index; the
                               caller must have a latch on the page; this
                               latch locks the top of the stack of versions
@@ -1391,7 +1401,6 @@ void row_vers_build_for_semi_consistent_read(
   ut_ad(!vrow || !(*vrow));
 
   for (;;) {
-    const trx_t *version_trx;
     mem_heap_t *heap2;
     rec_t *prev_version;
     trx_id_t version_trx_id;
@@ -1401,19 +1410,7 @@ void row_vers_build_for_semi_consistent_read(
       rec_trx_id = version_trx_id;
     }
 
-    trx_sys_mutex_enter();
-    version_trx = trx_get_rw_trx_by_id(version_trx_id);
-    /* Because version_trx is a read-write transaction,
-    its state cannot change from or to NOT_STARTED while
-    we are holding the trx_sys->mutex.  It may change from
-    ACTIVE to PREPARED or COMMITTED. */
-    if (version_trx &&
-        trx_state_eq(version_trx, TRX_STATE_COMMITTED_IN_MEMORY)) {
-      version_trx = nullptr;
-    }
-    trx_sys_mutex_exit();
-
-    if (!version_trx) {
+    if (!trx_sys->is_registered(caller_trx, version_trx_id)) {
     committed_version_trx:
       /* We found a version that belongs to a
       committed transaction: return it. */
diff --git a/storage/innobase/srv/srv0srv.cc b/storage/innobase/srv/srv0srv.cc
index d2d87cbb37a..43c0c43edf2 100644
--- a/storage/innobase/srv/srv0srv.cc
+++ b/storage/innobase/srv/srv0srv.cc
@@ -1419,9 +1419,8 @@ bool srv_printf_innodb_monitor(FILE *file, bool nowait, ulint *trx_start_pos,
           ULINTPF " queries inside InnoDB, " ULINTPF " queries in queue\n",
           srv_conc_get_active_threads(), srv_conc_get_waiting_threads());
 
-  /* This is a dirty read, without holding trx_sys->mutex. */
   fprintf(file, ULINTPF " read views open inside InnoDB\n",
-          trx_sys->mvcc->size());
+          trx_sys->view_count());
 
   n_reserved = fil_space_get_n_reserved_extents(0);
   if (n_reserved > 0) {
@@ -1659,15 +1658,12 @@ void srv_export_innodb_status(void) {
   transaction number. We are allowed to purge transactions with number
   below the low limit. */
   ReadView oldest_view;
-  trx_sys->mvcc->clone_oldest_view(&oldest_view);
+  trx_sys->clone_oldest_view(&oldest_view);
   trx_id_t low_limit_no = oldest_view.view_low_limit_no();
 
   rw_lock_s_unlock(&purge_sys->latch);
 
-  mutex_enter(&trx_sys->mutex);
-  /* Maximum transaction number added to history list for purge. */
   trx_id_t max_trx_no = trx_sys->rw_max_trx_no;
-  mutex_exit(&trx_sys->mutex);
 
   if (done_trx_no == 0 || max_trx_no < done_trx_no) {
     export_vars.innodb_purge_trx_id_age = 0;
diff --git a/storage/innobase/srv/srv0start.cc b/storage/innobase/srv/srv0start.cc
index f855274cbbc..67a00bfa55a 100644
--- a/storage/innobase/srv/srv0start.cc
+++ b/storage/innobase/srv/srv0start.cc
@@ -1347,7 +1347,7 @@ dberr_t srv_undo_tablespaces_upgrade() {
   rsegs and undo tablespaces they are in from being deleted.
   These transactions must be either committed or rolled back by
   the mysql server.*/
-  if (trx_sys->n_prepared_trx > 0) {
+  if (trx_sys->n_prepared_trx() > 0) {
     ib::warn(ER_IB_MSG_1094);
     return (DB_SUCCESS);
   }
@@ -2894,8 +2894,8 @@ void srv_dict_recover_on_restart() {
   that the data dictionary tables will be free of any locks.
   The data dictionary latch should guarantee that there is at
   most one data dictionary transaction active at a time. */
-  if (srv_force_recovery < SRV_FORCE_NO_TRX_UNDO && trx_sys_need_rollback()) {
-    trx_rollback_or_clean_recovered(FALSE);
+  if (srv_force_recovery < SRV_FORCE_NO_TRX_UNDO) {
+    trx_rollback_recovered(false);
   }
 
   /* Do after all DD transactions recovery, to get consistent metadata */
@@ -2976,8 +2976,7 @@ void srv_start_threads(bool bootstrap) {
     return;
   }
 
-  if (!bootstrap && srv_force_recovery < SRV_FORCE_NO_TRX_UNDO &&
-      trx_sys_need_rollback()) {
+  if (!bootstrap && srv_force_recovery < SRV_FORCE_NO_TRX_UNDO) {
     /* Rollback all recovered transactions that are
     not in committed nor in XA PREPARE state. */
     srv_threads.m_trx_recovery_rollback = os_thread_create(
@@ -3111,7 +3110,7 @@ void srv_pre_dd_shutdown() {
     prepared transactions and we don't want to lose them. */
 
     for (uint32_t count = 0;; ++count) {
-      const ulint total_trx = trx_sys_any_active_transactions();
+      const ulint total_trx = trx_sys->any_active_transactions();
 
       if (total_trx == 0) {
         break;
@@ -3202,7 +3201,7 @@ void srv_pre_dd_shutdown() {
 
   srv_is_being_shutdown = true;
 
-  ut_a(trx_sys_any_active_transactions() == 0);
+  ut_a(trx_sys->any_active_transactions() == 0);
 
   DBUG_EXECUTE_IF("wait_for_threads_in_pre_dd_shutdown",
                   srv_shutdown_background_threads(););
@@ -3479,7 +3478,7 @@ void srv_shutdown() {
 
   ut_a(!srv_is_being_started);
   ut_a(srv_is_being_shutdown);
-  ut_a(trx_sys_any_active_transactions() == 0);
+  ut_a(trx_sys->any_active_transactions() == 0);
 
   /* Ensure threads below have been stopped. */
   const auto threads_stopped_before_shutdown = {
diff --git a/storage/innobase/sync/sync0debug.cc b/storage/innobase/sync/sync0debug.cc
index 5c243345d9a..51778cb8d93 100644
--- a/storage/innobase/sync/sync0debug.cc
+++ b/storage/innobase/sync/sync0debug.cc
@@ -443,6 +443,7 @@ LatchDebug::LatchDebug() {
   LEVEL_MAP_INSERT(SYNC_TRX_SYS_HEADER);
   LEVEL_MAP_INSERT(SYNC_THREADS);
   LEVEL_MAP_INSERT(SYNC_TRX);
+  LEVEL_MAP_INSERT(SYNC_RW_TRX_HASH_ELEMENT);
   LEVEL_MAP_INSERT(SYNC_TRX_SYS);
   LEVEL_MAP_INSERT(SYNC_LOCK_SYS_GLOBAL);
   LEVEL_MAP_INSERT(SYNC_LOCK_SYS_SHARDED);
@@ -700,6 +701,7 @@ Latches *LatchDebug::check_order(const latch_t *latch,
     case SYNC_THREADS:
     case SYNC_LOCK_SYS_GLOBAL:
     case SYNC_LOCK_WAIT_SYS:
+    case SYNC_RW_TRX_HASH_ELEMENT:
     case SYNC_TRX_SYS:
     case SYNC_IBUF_BITMAP_MUTEX:
     case SYNC_TEMP_SPACE_RSEG:
@@ -1495,6 +1497,9 @@ static void sync_latch_meta_init() UNIV_NOTHROW {
 
   LATCH_ADD_MUTEX(TEST_MUTEX, SYNC_NO_ORDER_CHECK, PFS_NOT_INSTRUMENTED);
 
+  LATCH_ADD_MUTEX(RW_TRX_HASH_ELEMENT, SYNC_RW_TRX_HASH_ELEMENT,
+                  rw_trx_hash_element_mutex_key);
+
   latch_id_t id = LATCH_ID_NONE;
 
   /* The array should be ordered on latch ID.We need to
diff --git a/storage/innobase/sync/sync0sync.cc b/storage/innobase/sync/sync0sync.cc
index a3f853b6621..1847f91756f 100644
--- a/storage/innobase/sync/sync0sync.cc
+++ b/storage/innobase/sync/sync0sync.cc
@@ -125,6 +125,7 @@ mysql_pfs_key_t temp_pool_manager_mutex_key;
 mysql_pfs_key_t lock_sys_table_mutex_key;
 mysql_pfs_key_t lock_sys_page_mutex_key;
 mysql_pfs_key_t lock_wait_mutex_key;
+mysql_pfs_key_t rw_trx_hash_element_mutex_key;
 mysql_pfs_key_t trx_sys_mutex_key;
 mysql_pfs_key_t srv_sys_mutex_key;
 mysql_pfs_key_t srv_threads_mutex_key;
diff --git a/storage/innobase/trx/trx0i_s.cc b/storage/innobase/trx/trx0i_s.cc
index 9013822a5a9..8896ff3a39a 100644
--- a/storage/innobase/trx/trx0i_s.cc
+++ b/storage/innobase/trx/trx0i_s.cc
@@ -434,8 +434,6 @@ static ibool fill_trx_row(
   size_t stmt_len;
   const char *s;
 
-  /* We are going to read various trx->lock fields protected by trx->mutex */
-  ut_ad(trx_mutex_own(trx));
   /* We are going to read TRX_WEIGHT, lock_number_of_rows_locked() and
   lock_number_of_tables_locked() which requires latching the lock_sys.
   Also, we need it to avoid reading temporary NULL value set to wait_lock by a
@@ -863,69 +861,27 @@ static void trx_i_s_cache_clear(
  table cache buffer. Cache must be locked for write. */
 static void fetch_data_into_cache_low(
     trx_i_s_cache_t *cache,  /*!< in/out: cache */
-    bool read_write,         /*!< in: only read-write
-                             transactions */
-    trx_ut_list_t *trx_list) /*!< in: trx list */
+    const trx_t *trx)        /*!< in/out: transaction */
 {
   /* We are going to iterate over many different shards of lock_sys so we need
   exclusive access */
   ut_ad(locksys::owns_exclusive_global_latch());
-  trx_t *trx;
-  bool rw_trx_list = trx_list == &trx_sys->rw_trx_list;
-
-  ut_ad(rw_trx_list || trx_list == &trx_sys->mysql_trx_list);
-
-  /* Iterate over the transaction list and add each one
-  to innodb_trx's cache. We also add all locks that are relevant
-  to each transaction into innodb_locks' and innodb_lock_waits'
-  caches. */
-
-  for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       trx = (rw_trx_list ? UT_LIST_GET_NEXT(trx_list, trx)
-                          : UT_LIST_GET_NEXT(mysql_trx_list, trx))) {
-    i_s_trx_row_t *trx_row;
-    i_s_locks_row_t *requested_lock_row;
-
-    trx_mutex_enter(trx);
-
-    /* Note: Read only transactions that modify temporary
-    tables an have a transaction ID */
-    if (!trx_is_started(trx) ||
-        (!rw_trx_list && trx->id != 0 && !trx->read_only)) {
-      trx_mutex_exit(trx);
-      continue;
-    }
-
-    assert_trx_nonlocking_or_in_list(trx);
-
-    ut_ad(trx->in_rw_trx_list == rw_trx_list);
-
-    if (!add_trx_relevant_locks_to_cache(cache, trx, &requested_lock_row)) {
-      cache->is_truncated = true;
-      trx_mutex_exit(trx);
-      return;
-    }
+  i_s_locks_row_t *requested_lock_row;
 
-    trx_row = reinterpret_cast<i_s_trx_row_t *>(
+  assert_trx_nonlocking_or_in_list(trx);
+  if (add_trx_relevant_locks_to_cache(cache, trx, &requested_lock_row) != 0u) {
+    auto trx_row = static_cast<i_s_trx_row_t *>(
         table_cache_create_empty_row(&cache->innodb_trx, cache));
-
-    /* memory could not be allocated */
-    if (trx_row == nullptr) {
-      cache->is_truncated = true;
-      trx_mutex_exit(trx);
-      return;
-    }
-
-    if (!fill_trx_row(trx_row, trx, requested_lock_row, cache)) {
-      /* memory could not be allocated */
+    if (trx_row != nullptr) {
+      if (fill_trx_row(trx_row, trx, requested_lock_row, cache) == TRUE) {
+        return;
+      }
       --cache->innodb_trx.rows_used;
-      cache->is_truncated = true;
-      trx_mutex_exit(trx);
-      return;
     }
-
-    trx_mutex_exit(trx);
   }
+
+  /* memory could not be allocated */
+  cache->is_truncated = TRUE;
 }
 
 /** Fetches the data needed to fill the 3 INFORMATION SCHEMA tables into the
@@ -935,16 +891,20 @@ static void fetch_data_into_cache(trx_i_s_cache_t *cache) /*!< in/out: cache */
   /* We are going to iterate over many different shards of lock_sys so we need
   exclusive access */
   ut_ad(locksys::owns_exclusive_global_latch());
-  ut_ad(trx_sys_mutex_own());
 
   trx_i_s_cache_clear(cache);
 
-  /* Capture the state of the read-write transactions. This includes
-  internal transactions too. They are not on mysql_trx_list */
-  fetch_data_into_cache_low(cache, true, &trx_sys->rw_trx_list);
-
-  /* Capture the state of the read-only active transactions */
-  fetch_data_into_cache_low(cache, false, &trx_sys->mysql_trx_list);
+  mutex_enter(&trx_sys->mutex);
+  for (const trx_t *trx = UT_LIST_GET_FIRST(trx_sys->trx_list); trx != nullptr;
+       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+    if (trx_is_started(trx) && trx != purge_sys->query->trx) {
+      fetch_data_into_cache_low(cache, trx);
+      if (cache->is_truncated == TRUE) {
+        break;
+      }
+    }
+  }
+  mutex_exit(&trx_sys->mutex);
 
   cache->is_truncated = false;
 }
@@ -963,11 +923,7 @@ int trx_i_s_possibly_fetch_data_into_cache(
     /* We need to read trx_sys and record/table lock queues */
     locksys::Global_exclusive_latch_guard guard{};
 
-    trx_sys_mutex_enter();
-
     fetch_data_into_cache(cache);
-
-    trx_sys_mutex_exit();
   }
 
   return (0);
diff --git a/storage/innobase/trx/trx0purge.cc b/storage/innobase/trx/trx0purge.cc
index 29290bc9d5b..d8d8cf4b4bc 100644
--- a/storage/innobase/trx/trx0purge.cc
+++ b/storage/innobase/trx/trx0purge.cc
@@ -240,7 +240,7 @@ void trx_purge_sys_create(ulint n_purge_threads, purge_pq_t *purge_queue) {
 
   new (&purge_sys->view) ReadView();
 
-  trx_sys->mvcc->clone_oldest_view(&purge_sys->view);
+  trx_sys->clone_oldest_view();
 
   purge_sys->view_active = true;
 
@@ -348,7 +348,7 @@ void trx_purge_add_update_undo_to_history(
                  undo_header + TRX_UNDO_HISTORY_NODE, mtr);
 
   if (update_rseg_history_len) {
-    os_atomic_increment_ulint(&trx_sys->rseg_history_len, n_added_logs);
+    trx_sys->rseg_history_len.fetch_add(n_added_logs);
     srv_wake_purge_thread_if_not_active();
   }
 
@@ -384,7 +384,7 @@ static void trx_purge_remove_log_hdr(trx_rsegf_t *rseg_hdr,
   flst_remove(rseg_hdr + TRX_RSEG_HISTORY, log_hdr + TRX_UNDO_HISTORY_NODE,
               mtr);
 
-  os_atomic_decrement_ulint(&trx_sys->rseg_history_len, 1);
+  trx_sys->rseg_history_len.fetch_sub(1);
 }
 
 /** Frees a rollback segment which is in the history list.
@@ -1648,8 +1648,6 @@ static void trx_purge_rseg_get_next_history_log(
     rseg->unlatch();
 
 #ifdef UNIV_DEBUG
-    trx_sys_mutex_enter();
-
     /* Add debug code to track history list corruption reported
     on the MySQL mailing list on Nov 9, 2004. The fut0lst.cc
     file-based list was corrupt. The prev node pointer was
@@ -1672,8 +1670,6 @@ static void trx_purge_rseg_get_next_history_log(
       ib::info(ER_IB_MSG_1180) << "2. Try increasing the number of purge"
                                   " threads to expedite purging of undo logs.";
     }
-
-    trx_sys_mutex_exit();
 #endif
     return;
   }
@@ -2205,7 +2201,7 @@ ulint trx_purge(ulint n_purge_threads, /*!< in: number of purge tasks
 
   purge_sys->view_active = false;
 
-  trx_sys->mvcc->clone_oldest_view(&purge_sys->view);
+  trx_sys->clone_oldest_view();
 
   purge_sys->view_active = true;
 
diff --git a/storage/innobase/trx/trx0roll.cc b/storage/innobase/trx/trx0roll.cc
index bd9f49869f9..4c9479ece09 100644
--- a/storage/innobase/trx/trx0roll.cc
+++ b/storage/innobase/trx/trx0roll.cc
@@ -187,11 +187,11 @@ static dberr_t trx_rollback_low(trx_t *trx) {
     case TRX_STATE_FORCED_ROLLBACK:
     case TRX_STATE_NOT_STARTED:
       trx->will_lock = 0;
-      ut_ad(trx->in_mysql_trx_list);
+      ut_ad(trx->mysql_thd != nullptr);
       return (DB_SUCCESS);
 
     case TRX_STATE_ACTIVE:
-      ut_ad(trx->in_mysql_trx_list);
+      ut_ad(trx->mysql_thd != nullptr);
       assert_trx_nonlocking_or_in_list(trx);
       /* Check an validate that undo is available for GTID. */
       trx_undo_gtid_add_update_undo(trx, false, true);
@@ -286,7 +286,7 @@ dberr_t trx_rollback_last_sql_stat_for_mysql(
   here, because the statement rollback should be invoked for a
   running active MySQL transaction that is associated with the
   current thread. */
-  ut_ad(trx->in_mysql_trx_list);
+  ut_ad(trx->mysql_thd != nullptr);
 
   switch (trx->state) {
     case TRX_STATE_FORCED_ROLLBACK:
@@ -391,7 +391,7 @@ executed after the savepoint */
   dberr_t err;
 
   ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE));
-  ut_ad(trx->in_mysql_trx_list);
+  ut_ad(trx->mysql_thd != nullptr);
 
   /* Free all savepoints strictly later than savep. */
 
@@ -438,7 +438,7 @@ dberr_t trx_rollback_to_savepoint_for_mysql(
   here, because the savepoint rollback should be invoked for a
   running active MySQL transaction that is associated with the
   current thread. */
-  ut_ad(trx->in_mysql_trx_list);
+  ut_ad(trx->mysql_thd != nullptr);
 
   savep = trx_savepoint_find(trx, savepoint_name);
 
@@ -525,7 +525,7 @@ dberr_t trx_release_savepoint_for_mysql(
   trx_named_savept_t *savep;
 
   ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE));
-  ut_ad(trx->in_mysql_trx_list);
+  ut_ad(trx->mysql_thd != nullptr);
 
   savep = trx_savepoint_find(trx, savepoint_name);
 
@@ -582,8 +582,6 @@ static void trx_rollback_active(trx_t *trx) /*!< in/out: transaction */
 
   ut_a(thr == que_fork_start_command(fork));
 
-  trx_sys_mutex_enter();
-
   trx_roll_crash_recv_trx = trx;
 
   trx_roll_max_undo_no = trx->undo_no;
@@ -592,8 +590,6 @@ static void trx_rollback_active(trx_t *trx) /*!< in/out: transaction */
 
   rows_to_undo = trx_roll_max_undo_no;
 
-  trx_sys_mutex_exit();
-
   if (rows_to_undo > 1000000000) {
     rows_to_undo = rows_to_undo / 1000000;
     unit = "M";
@@ -624,110 +620,67 @@ static void trx_rollback_active(trx_t *trx) /*!< in/out: transaction */
   trx_roll_crash_recv_trx = nullptr;
 }
 
-/** Rollback or clean up any resurrected incomplete transactions. It assumes
- that the caller holds the trx_sys_t::mutex and it will release the
- lock if it does a clean up or rollback.
- @return true if the transaction was cleaned up or rolled back
- and trx_sys->mutex was released. */
-static ibool trx_rollback_resurrected(
-    trx_t *trx, /*!< in: transaction to rollback or clean */
-    ibool all)  /*!< in: FALSE=roll back dictionary transactions;
-                TRUE=roll back all non-PREPARED transactions */
-{
-  ut_ad(trx_sys_mutex_own());
-
-  /* The trx->is_recovered flag and trx->state are set
-  atomically under the protection of the trx->mutex . We do not want
-  to accidentally clean up a non-recovered transaction here. */
-
-  trx_mutex_enter(trx);
-  bool is_recovered = trx->is_recovered;
-  trx_state_t state = trx->state;
-  trx_mutex_exit(trx);
-
-  if (!is_recovered) {
-    return (FALSE);
-  }
-
-  switch (state) {
-    case TRX_STATE_COMMITTED_IN_MEMORY:
-      trx_sys_mutex_exit();
-      ib::info(ER_IB_MSG_1188)
-          << "Cleaning up trx with id " << trx_get_id_for_print(trx);
-
-      trx_cleanup_at_db_startup(trx);
-      trx_free_resurrected(trx);
-      return (TRUE);
-    case TRX_STATE_ACTIVE:
-      if (all || trx->ddl_operation) {
-        trx_sys_mutex_exit();
-        trx_rollback_active(trx);
-        trx_free_for_background(trx);
-        return (TRUE);
-      }
-      return (FALSE);
-    case TRX_STATE_PREPARED:
-      return (FALSE);
-    case TRX_STATE_NOT_STARTED:
-    case TRX_STATE_FORCED_ROLLBACK:
-      break;
+static bool trx_rollback_recovered_callback(rw_trx_hash_element_t *element,
+                                            std::vector<trx_t *> *trx_list) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    mutex_enter(&trx->mutex);
+    if (trx->is_recovered && trx_state_eq(trx, TRX_STATE_ACTIVE)) {
+      ut_ad(trx != trx_dummy_sess->trx);
+      trx_list->push_back(trx);
+    }
+    mutex_exit(&trx->mutex);
   }
-
-  ut_error;
-  return (FALSE);
+  mutex_exit(&element->mutex);
+  return false;
 }
 
-/** Rollback or clean up any incomplete transactions which were
- encountered in crash recovery.  If the transaction already was
- committed, then we clean up a possible insert undo log. If the
- transaction was not yet committed, then we roll it back. */
-void trx_rollback_or_clean_recovered(
-    ibool all) /*!< in: FALSE=roll back dictionary transactions;
-               TRUE=roll back all non-PREPARED transactions */
-{
-  trx_t *trx;
+/** Rollback any incomplete transactions which were encountered in crash
+ recovery.
 
-  ut_a(srv_force_recovery < SRV_FORCE_NO_TRX_UNDO);
-  ut_ad(!all || trx_sys_need_rollback());
-
-  if (all) {
-    ib::info(ER_IB_MSG_1189) << "Starting in background the rollback"
-                                " of uncommitted transactions";
-  }
+ If the transaction already was committed, then we cleanup a possible insert
+ undo log. If the transaction was not yet committed, then we roll it back.
 
-  /* Note: For XA recovered transactions, we rely on MySQL to
-  do rollback. They will be in TRX_STATE_PREPARED state. If the server
-  is shutdown and they are still lingering in trx_sys_t::trx_list
-  then the shutdown will hang. */
+ Note: For XA recovered transactions, we rely on MySQL to do rollback. They
+ will be in TRX_STATE_PREPARED state. If the server is shutdown and they are
+ still lingering in trx_sys_t::trx_list then the shutdown will hang.
 
-  /* Loop over the transaction list as long as there are
-  recovered transactions to clean up or recover. */
+ @param [in] all true=roll back all recovered active transactions;
+                 fasle=roll back any incomplete dictionary transaction */
+void trx_rollback_recovered(bool all) {
+  std::vector<trx_t *> trx_list;
 
-  do {
-    trx_sys_mutex_enter();
-
-    for (trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-         trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-      assert_trx_in_rw_list(trx);
+  ut_a(srv_force_recovery < SRV_FORCE_NO_TRX_UNDO);
 
-      /* If this function does a cleanup or rollback
-      then it will release the trx_sys->mutex, therefore
-      we need to reacquire it before retrying the loop. */
+  /* Collect list of recovered ACTIVE transaction ids first. Once collected, no
+  other thread is allowed to modify or remove these transactions from
+  rw_trx_hash.*/
+  trx_sys->rw_trx_hash.iterate_no_dups(
+      reinterpret_cast<lf_hash_walk_func *>(trx_rollback_recovered_callback),
+      &trx_list);
 
-      if (trx_rollback_resurrected(trx, all)) {
-        trx_sys_mutex_enter();
+  while (!trx_list.empty()) {
+    trx_t *trx = trx_list.back();
+    trx_list.pop_back();
 
-        break;
+#ifdef UNIV_DEBUG
+    ut_ad(trx != nullptr);
+    trx_mutex_enter(trx);
+    ut_ad(trx->is_recovered && trx_state_eq(trx, TRX_STATE_ACTIVE));
+    trx_mutex_exit(trx);
+#endif /* UNIV_DEBUG */
+
+    if (all || trx_get_dict_operation(trx) != TRX_DICT_OP_NONE) {
+      trx_rollback_active(trx);
+      if (trx->error_state != DB_SUCCESS) {
+        trx->error_state = DB_SUCCESS;
+        trx_sys->deregister_rw(trx);
+        trx_free_at_shutdown(trx);
+      } else {
+        trx_free(trx);
       }
     }
-
-    trx_sys_mutex_exit();
-
-  } while (trx != nullptr);
-
-  if (all) {
-    ib::info(ER_IB_MSG_1190) << "Rollback of non-prepared transactions"
-                                " completed";
   }
 }
 
@@ -746,7 +699,13 @@ void trx_recovery_rollback_thread() {
 
   ut_ad(!srv_read_only_mode);
 
-  trx_rollback_or_clean_recovered(TRUE);
+  if (trx_sys->rw_trx_hash.size() > 0) {
+    ib::info(ER_IB_MSG_1189) << "Starting in background the rollback"
+                                " of uncommitted transactions";
+    trx_rollback_recovered(true);
+    ib::info(ER_IB_MSG_1190) << "Rollback of non-prepared transactions"
+                                " completed";
+  }
 
   destroy_thd(thd);
 }
diff --git a/storage/innobase/trx/trx0sys.cc b/storage/innobase/trx/trx0sys.cc
index 33a85e120fa..bba8e158cd0 100644
--- a/storage/innobase/trx/trx0sys.cc
+++ b/storage/innobase/trx/trx0sys.cc
@@ -49,6 +49,8 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "srv0srv.h"
 #include "srv0start.h"
 #include "trx0purge.h"
+#include "trx0roll.h"
+#include "clone0clone.h"
 #include "trx0rseg.h"
 #include "trx0trx.h"
 #include "trx0undo.h"
@@ -67,7 +69,7 @@ void ReadView::check_trx_id_sanity(trx_id_t id, const table_name_t &name) {
     return;
   }
 
-  if (id >= trx_sys->max_trx_id) {
+  if (id >= trx_sys->get_max_trx_id()) {
     ib::warn(ER_IB_MSG_1196)
         << "A transaction id"
         << " in a record of table " << name << " is newer than the"
@@ -99,14 +101,12 @@ void trx_sys_flush_max_trx_id(void) {
   mtr_t mtr;
   trx_sysf_t *sys_header;
 
-  ut_ad(trx_sys_mutex_own());
-
   if (!srv_read_only_mode) {
     mtr_start(&mtr);
 
     sys_header = trx_sysf_get(&mtr);
 
-    mlog_write_ull(sys_header + TRX_SYS_TRX_ID_STORE, trx_sys->max_trx_id,
+    mlog_write_ull(sys_header + TRX_SYS_TRX_ID_STORE, trx_sys->get_max_trx_id(),
                    &mtr);
 
     mtr_commit(&mtr);
@@ -125,31 +125,35 @@ void trx_sys_persist_gtid_num(trx_id_t gtid_trx_no) {
 }
 
 trx_id_t trx_sys_oldest_trx_no() {
-  ut_ad(trx_sys_mutex_own());
   /* Get the oldest transaction from serialisation list. */
-  if (UT_LIST_GET_LEN(trx_sys->serialisation_list) > 0) {
-    auto trx = UT_LIST_GET_FIRST(trx_sys->serialisation_list);
-    return (trx->no);
+  trx_id_t max_trx_id = TRX_ID_MAX;
+  for (auto trx = UT_LIST_GET_FIRST(trx_sys->trx_list); trx != nullptr;
+    trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+      if (trx->serialised && trx->no < max_trx_id) {
+          max_trx_id = trx->no;
+      }
   }
-  return (trx_sys->max_trx_id);
+  return max_trx_id != TRX_ID_MAX ? max_trx_id : trx_sys->get_max_trx_id();
 }
 
 void trx_sys_get_binlog_prepared(std::vector<trx_id_t> &trx_ids) {
-  trx_sys_mutex_enter();
+  mutex_enter(&trx_sys->mutex);
   /* Exit fast if no prepared transaction. */
-  if (trx_sys->n_prepared_trx == 0) {
-    trx_sys_mutex_exit();
+  if (!trx_sys->n_prepared_trx()) {
+    mutex_exit(&trx_sys->mutex);
     return;
   }
   /* Check and find binary log prepared transaction. */
-  for (auto trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
+  for (auto trx = UT_LIST_GET_FIRST(trx_sys->trx_list); trx != nullptr;
        trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    assert_trx_in_rw_list(trx);
+    if (trx->state != TRX_STATE_ACTIVE && trx->state != TRX_STATE_PREPARED) {
+        continue;
+    }
     if (trx_state_eq(trx, TRX_STATE_PREPARED) && trx_is_mysql_xa(trx)) {
       trx_ids.push_back(trx->id);
     }
   }
-  trx_sys_mutex_exit();
+  mutex_exit(&trx_sys->mutex);
 }
 
 /** Read binary log positions from buffer passed.
@@ -440,10 +444,11 @@ purge_pq_t *trx_sys_init_at_db_start(void) {
 
   sys_header = trx_sysf_get(&mtr);
 
-  trx_sys->max_trx_id =
+  trx_id_t max_trx_id =
       2 * TRX_SYS_TRX_ID_WRITE_MARGIN +
       ut_uint64_align_up(mach_read_from_8(sys_header + TRX_SYS_TRX_ID_STORE),
                          TRX_SYS_TRX_ID_WRITE_MARGIN);
+    trx_sys->init_max_trx_id(max_trx_id);
 
   mtr.commit();
 
@@ -451,7 +456,7 @@ purge_pq_t *trx_sys_init_at_db_start(void) {
   /* max_trx_id is the next transaction ID to assign. Initialize maximum
   transaction number to one less if all transactions are already purged. */
   if (trx_sys->rw_max_trx_no == 0) {
-    trx_sys->rw_max_trx_no = trx_sys->max_trx_id - 1;
+    trx_sys->rw_max_trx_no = trx_sys->get_max_trx_id() - 1;
   }
 #endif /* UNIV_DEBUG */
 
@@ -463,15 +468,20 @@ purge_pq_t *trx_sys_init_at_db_start(void) {
   the debug code (assertions). We are still running in single threaded
   bootstrap mode. */
 
-  trx_sys_mutex_enter();
+  mutex_enter(&trx_sys->mutex);
 
-  if (UT_LIST_GET_LEN(trx_sys->rw_trx_list) > 0) {
+  if (UT_LIST_GET_LEN(trx_sys->trx_list) > 0) {
     const trx_t *trx;
 
-    for (trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
+    for (trx = UT_LIST_GET_FIRST(trx_sys->trx_list); trx != nullptr;
          trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+      if (trx->state != TRX_STATE_ACTIVE && trx->state != TRX_STATE_PREPARED) {
+        /* trx_dummy_sess is a transaction in TRX_STATE_NOT_STARTED state. */
+        continue;
+      }
       ut_ad(trx->is_recovered);
-      assert_trx_in_rw_list(trx);
+      ut_ad(trx->rsegs.m_redo.rseg->trx_ref_count > 0);
+      ut_ad(trx_sys->find(nullptr, trx->id, false) != nullptr);
 
       if (trx_state_eq(trx, TRX_STATE_ACTIVE)) {
         rows_to_undo += trx->undo_no;
@@ -484,209 +494,498 @@ purge_pq_t *trx_sys_init_at_db_start(void) {
     }
 
     ib::info(ER_IB_MSG_1198)
-        << UT_LIST_GET_LEN(trx_sys->rw_trx_list)
+        << UT_LIST_GET_LEN(trx_sys->trx_list)
         << " transaction(s) which must be rolled back or"
            " cleaned up in total "
         << rows_to_undo << unit << " row operations to undo";
 
-    ib::info(ER_IB_MSG_1199) << "Trx id counter is " << trx_sys->max_trx_id;
+    ib::info(ER_IB_MSG_1199)
+        << "Trx id counter is " << trx_sys->get_max_trx_id();
   }
 
-  trx_sys->found_prepared_trx = trx_sys->n_prepared_trx > 0;
+  trx_sys->found_prepared_trx = trx_sys->n_prepared_trx() > 0;
 
-  trx_sys_mutex_exit();
+  mutex_exit(&trx_sys->mutex);
 
   return (purge_queue);
 }
 
-/** Creates the trx_sys instance and initializes purge_queue and mutex. */
-void trx_sys_create(void) {
-  ut_ad(trx_sys == nullptr);
+/** Constructor callback for lock-free allocator.
 
-  trx_sys = static_cast<trx_sys_t *>(ut_zalloc_nokey(sizeof(*trx_sys)));
+Object is just allocated and is not yet accessible via rw_trx_hash by
+concurrent threads. Object can be reused multiple times before it is freed.
+Every time object is being reused initialize() callback is called. */
+void rw_trx_hash_t::rw_trx_hash_constructor(uchar *arg) {
+  new (arg + LF_HASH_OVERHEAD) rw_trx_hash_element_t();
+}
 
-  mutex_create(LATCH_ID_TRX_SYS, &trx_sys->mutex);
+/** Destructor callback for lock-free allocator.
 
-  UT_LIST_INIT(trx_sys->serialisation_list, &trx_t::no_list);
-  UT_LIST_INIT(trx_sys->rw_trx_list, &trx_t::trx_list);
-  UT_LIST_INIT(trx_sys->mysql_trx_list, &trx_t::mysql_trx_list);
+Object is about to be freed and is not accessible via rw_trx_hash by
+concurrent threads. */
+void rw_trx_hash_t::rw_trx_hash_destructor(uchar *arg) {
+  reinterpret_cast<rw_trx_hash_element_t *>(arg + LF_HASH_OVERHEAD)
+      ->~rw_trx_hash_element_t();
+}
 
-  trx_sys->mvcc = UT_NEW_NOKEY(MVCC(1024));
+/** Destructor callback for lock-free allocator.
+
+This destructor is used at shutdown. It frees remaining transaction objects.
+
+XA PREPARED transactions may remain if they haven't been committed or rolled
+back. ACTIVE transactions may remain if startup was interrupted or server is
+running in read-only mode or for certain srv_force_recovery levels. */
+void rw_trx_hash_t::rw_trx_hash_shutdown_destructor(uchar *arg) {
+  auto element =
+      reinterpret_cast<rw_trx_hash_element_t *>(arg + LF_HASH_OVERHEAD);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    ut_ad(trx_state_eq(trx, TRX_STATE_PREPARED) ||
+          (trx_state_eq(trx, TRX_STATE_ACTIVE) &&
+           (srv_is_being_started || srv_read_only_mode ||
+            (srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO))));
+    trx_free_at_shutdown(trx);
+  }
+  element->~rw_trx_hash_element_t();
+}
 
-  trx_sys->min_active_id = 0;
+/** Initializer callback for lock-free hash.
+
+Object is not yet accessible via rw_trx_hash by concurrent threads, but is
+about to become such. Object id can be changed only by this callback and
+remains the same until all pins to this object are released.
+
+Object trx can be changed to 0 by erase() under object mutex protection,
+which indicates it is about to be removed from lock-free hash and become not
+accessible by concurrent threads. */
+void rw_trx_hash_t::rw_trx_hash_initialize(rw_trx_hash_element_t *element,
+                                           trx_t *trx) {
+  ut_ad(element->trx == nullptr);
+  element->trx = trx;
+  element->id = trx->id;
+  element->no = TRX_ID_MAX;
+  trx->rw_trx_hash_element = element;
+}
 
-  ut_d(trx_sys->rw_max_trx_no = 0);
+/** Gets LF_HASH pins.
 
-  new (&trx_sys->rw_trx_ids)
-      trx_ids_t(ut_allocator<trx_id_t>(mem_key_trx_sys_t_rw_trx_ids));
+Pins are used to protect object from being destroyed or reused. They are
+normally stored in trx object for quick access. If caller doesn't have trx
+available, we try to get it using current_trx(). If caller doesn't have trx at
+all, temporary pins are allocated. */
+LF_PINS *rw_trx_hash_t::get_pins(trx_t *trx) {
+  if (trx->rw_trx_hash_pins == nullptr) {
+    trx->rw_trx_hash_pins = lf_hash_get_pins(&hash);
+    ut_a(trx->rw_trx_hash_pins != nullptr);
+  }
 
-  new (&trx_sys->rw_trx_set) TrxIdSet();
+  return trx->rw_trx_hash_pins;
+}
 
-  new (&trx_sys->rsegs) Rsegs();
-  trx_sys->rsegs.set_empty();
+bool rw_trx_hash_t::eliminate_duplicates(rw_trx_hash_element_t *element,
+                                         eliminate_duplicates_arg *arg) {
+  for (auto id : arg->ids) {
+    if (id == element->id) {
+      return false;
+    }
+  }
 
-  new (&trx_sys->tmp_rsegs) Rsegs();
-  trx_sys->tmp_rsegs.set_empty();
+  arg->ids.push_back(element->id);
+  return arg->action(element, arg->argument);
 }
 
-/** Creates and initializes the transaction system at the database creation. */
-void trx_sys_create_sys_pages(void) {
-  mtr_t mtr;
+#ifdef UNIV_DEBUG
+void rw_trx_hash_t::validate_element(trx_t *trx) {
+  ut_ad(!trx->read_only || trx->rsegs.m_redo.rseg == nullptr);
+  ut_ad(!trx_is_autocommit_non_locking(trx));
+  mutex_enter(&trx->mutex);
+  ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE) ||
+        trx_state_eq(trx, TRX_STATE_PREPARED));
+  mutex_exit(&trx->mutex);
+}
 
-  mtr_start(&mtr);
+bool rw_trx_hash_t::debug_iterator(rw_trx_hash_element_t *element,
+                                   debug_iterator_arg *arg) {
+  mutex_enter(&element->mutex);
+  if (element->trx != nullptr) {
+    validate_element(element->trx);
+  }
+  mutex_exit(&element->mutex);
+  return arg->action(element, arg->argument);
+}
+#endif /* UNIV_DEBUG */
 
-  trx_sysf_create(&mtr);
+void rw_trx_hash_t::init() {
+  lf_hash_init(&hash, sizeof(rw_trx_hash_element_t), LF_HASH_UNIQUE, 0,
+               sizeof(trx_id_t), nullptr, &my_charset_bin);
+  hash.alloc.constructor = rw_trx_hash_constructor;
+  hash.alloc.destructor = rw_trx_hash_destructor;
+  hash.initialize =
+      reinterpret_cast<lf_hash_init_func *>(rw_trx_hash_initialize);
+}
 
-  mtr_commit(&mtr);
+void rw_trx_hash_t::destroy() {
+  hash.alloc.destructor = rw_trx_hash_shutdown_destructor;
+  lf_hash_destroy(&hash);
 }
 
-/*********************************************************************
-Shutdown/Close the transaction system. */
-void trx_sys_close(void) {
-  ut_ad(srv_shutdown_state.load() == SRV_SHUTDOWN_EXIT_THREADS);
+/** Releases LF_HASH pins.
 
-  if (trx_sys == nullptr) {
-    return;
+Must be called by thread that owns trx_t object when the later is being
+"detached" from thread (e.g. released to the pool by trx_free()). Can be
+called earlier if thread is expected not to use rw_trx_hash.
+
+Since pins are not allowed to be transferred to another thread,
+initialisation thread calls this for recovered transactions. */
+void rw_trx_hash_t::put_pins(trx_t *trx) {
+  if (trx->rw_trx_hash_pins != nullptr) {
+    lf_hash_put_pins(trx->rw_trx_hash_pins);
+    trx->rw_trx_hash_pins = nullptr;
   }
+}
 
-  ulint size = trx_sys->mvcc->size();
+/** Finds trx object in lock-free hash with given id.
+
+Only ACTIVE or PREPARED trx objects may participate in hash. Nevertheless the
+transaction may get committed before this method returns.
+
+With do_ref_count == false the caller may dereference returned trx pointer
+only if lock_sys.mutex was acquired before calling find().
+
+With do_ref_count == true caller dereferemce trx even if it is not holding
+lock_sys.mutex. Caller is responsible for calling trx->release_reference()
+when it is done playing with trx.
+
+Ideally this method should get caller rw_trx_hash_pins along with trx object
+as a parameter, similar to insert() and erase(). However most callers lose trx
+early in their call chains and it is not that easy to pass them through.
+
+So we take more expensive approach: get trx through current_thd()->ha_data.
+Some threads don't have trx attached to THD, and at least server
+initialisation thread, fts_optimize_thread, srv_master_thread,
+dict_stats_thread, srv_monitor_thread, btr_defragment_thread don't even have
+THD at all. For such cases we allocate pins only for duration of search and
+free them immediately.
+
+This has negative performance impact and should be fixed eventually (by
+passing caller_trx as a parameter). Still stream of DML is more or less Ok.
+
+@return pointer to trx or nullptr if not found */
+trx_t *rw_trx_hash_t::find(trx_t *caller_trx, trx_id_t trx_id,
+                           bool do_ref_count) {
+  /* In Taurus, purge will reset DB_TRX_ID to 0 when the histroy is lost.
+  Read/Write transactions will always have a nonzero trx_t::id; there the
+  value 0 is reserved for transactions that did not write or lock anything
+  yet.
+
+  The caller should already have handled trx_id==0 specially. */
+  ut_ad(trx_id > 0);
+  if (caller_trx != nullptr && caller_trx->id == trx_id) {
+    if (do_ref_count) {
+      caller_trx->reference();
+    }
+    return caller_trx;
+  }
 
-  if (size > 0) {
-    ib::error(ER_IB_MSG_1201) << "All read views were not closed before"
-                                 " shutdown: "
-                              << size << " read views open";
+  trx_t *trx = nullptr;
+  LF_PINS *pins =
+      (caller_trx != nullptr) ? get_pins(caller_trx) : lf_hash_get_pins(&hash);
+  ut_a(pins != nullptr);
+
+  rw_trx_hash_element_t *element = reinterpret_cast<rw_trx_hash_element_t *>(
+      lf_hash_search(&hash, pins, reinterpret_cast<const void *>(&trx_id),
+                     sizeof(trx_id_t)));
+  if (element != nullptr) {
+    mutex_enter(&element->mutex);
+    lf_hash_search_unpin(pins);
+    trx = element->trx;
+    if (trx == nullptr) {
+    } else if (UNIV_UNLIKELY(trx_id != trx->id)) {
+      trx = nullptr;
+    } else {
+      if (do_ref_count) {
+        trx->reference();
+      }
+      ut_d(validate_element(trx));
+    }
+    mutex_exit(&element->mutex);
+  } else {
+    lf_hash_search_unpin(pins);
+  }
+  if (caller_trx == nullptr) {
+    lf_hash_put_pins(pins);
   }
+  return trx;
+}
 
-  sess_close(trx_dummy_sess);
-  trx_dummy_sess = nullptr;
+/** Inserts trx to lock-free hash.
 
-  trx_purge_sys_close();
+Object becomes accessible via rw_trx_hash. */
+void rw_trx_hash_t::insert(trx_t *trx) {
+  ut_d(validate_element(trx));
+  int res = lf_hash_insert(&hash, get_pins(trx), static_cast<void *>(trx));
+  ut_a(res == 0);
+}
 
-  /* Only prepared transactions may be left in the system. Free them. */
-  ut_a(UT_LIST_GET_LEN(trx_sys->rw_trx_list) == trx_sys->n_prepared_trx);
+/** Removes trx from lock-free hash.
+
+Object becomes not accessible via rw_trx_hash. But it still can be pinned by
+concurrent find(), which is supposed to release it immediately after it sees
+object trx is nullptr. */
+void rw_trx_hash_t::erase(trx_t *trx) {
+  ut_d(validate_element(trx));
+  mutex_enter(&trx->rw_trx_hash_element->mutex);
+  trx->rw_trx_hash_element->trx = nullptr;
+  mutex_exit(&trx->rw_trx_hash_element->mutex);
+  int res =
+      lf_hash_delete(&hash, get_pins(trx), static_cast<const void *>(&trx->id),
+                     sizeof(trx_id_t));
+  ut_a(res == 0);
+}
+
+/** Returns the number of elements in the hash.
+
+The number is exact only if hash is protected against concurrent modifications
+(e.g., single threaded startup or hash is protected by some mutex). Otherwise
+the number maybe used as a hint only, because it may change even before this
+method returns. */
+uint32_t rw_trx_hash_t::size() {
+  return static_cast<uint32_t>(hash.count.load(std::memory_order_relaxed));
+}
 
-  for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-       trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list)) {
-    trx_free_prepared(trx);
+/** Iterates the hash.
+
+@param caller_trx used to get/set pins
+@param action     called for every element in hash
+@param argument   opque argument passed to action
+
+May return the same element multiple times if hash is under contention. If
+caller doesn't like to see the same transaction multiple times, it has to call
+iterate_no_dups() instead.
+
+May return element with committed transaction. If caller doesn't like to see
+committed transactions, it has to skip those under element mutex:
+
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    // trx is protected against commit in this branch
   }
+  mutex_exit(&element->mutex);
 
-  /* There can't be any active transactions. */
-  trx_sys->rsegs.~Rsegs();
+May miss concurrently inserted transactions.
 
-  trx_sys->tmp_rsegs.~Rsegs();
+@return 0 if iteration completed successfuly, or 1 if iteration was
+interrupted (action returned true) */
+int rw_trx_hash_t::iterate(trx_t *caller_trx, const lf_hash_walk_func *action,
+                           const void *argument) {
+  LF_PINS *pins =
+      (caller_trx != nullptr) ? get_pins(caller_trx) : lf_hash_get_pins(&hash);
+  ut_a(pins != nullptr);
+#ifdef UNIV_DEBUG
+  debug_iterator_arg debug_arg = {action, const_cast<void *>(argument)};
+  action = reinterpret_cast<lf_hash_walk_func *>(debug_iterator);
+  argument = &debug_arg;
+#endif /* UNIV_DEBUG */
+  int res = lf_hash_iterate(&hash, pins, action, argument);
+  if (caller_trx == nullptr) {
+    lf_hash_put_pins(pins);
+  }
+  return res;
+}
 
-  UT_DELETE(trx_sys->mvcc);
+int rw_trx_hash_t::iterate(const lf_hash_walk_func *action, const void *argument) {
+  return iterate(current_trx(), action, argument);
+}
 
-  ut_a(UT_LIST_GET_LEN(trx_sys->rw_trx_list) == 0);
-  ut_a(UT_LIST_GET_LEN(trx_sys->mysql_trx_list) == 0);
-  ut_a(UT_LIST_GET_LEN(trx_sys->serialisation_list) == 0);
+/** Iterates the hash and eliminates duplicate elements.
 
-  /* We used placement new to create this mutex. Call the destructor. */
-  mutex_free(&trx_sys->mutex);
+@sa iterate() */
+int rw_trx_hash_t::iterate_no_dups(trx_t *caller_trx, lf_hash_walk_func *action,
+                                   void *argument) {
+  eliminate_duplicates_arg arg(size() + 32, action, argument);
+  return iterate(caller_trx,
+                 reinterpret_cast<lf_hash_walk_func *>(eliminate_duplicates),
+                 &arg);
+}
+
+int rw_trx_hash_t::iterate_no_dups(lf_hash_walk_func *action, void *argument) {
+  return iterate_no_dups(current_trx(), action, argument);
+}
+
+/** Creates the trx_sys instance and initializes purge_queue and mutex. */
+void trx_sys_create() {
+  ut_ad(trx_sys == nullptr);
 
-  trx_sys->rw_trx_ids.~trx_ids_t();
+  trx_sys = static_cast<trx_sys_t *>(ut_zalloc_nokey(sizeof(*trx_sys)));
 
-  trx_sys->rw_trx_set.~TrxIdSet();
+  mutex_create(LATCH_ID_TRX_SYS, &trx_sys->mutex);
 
-  ut_free(trx_sys);
+  UT_LIST_INIT(trx_sys->trx_list, &trx_t::trx_list);
 
-  trx_sys = nullptr;
+  ut_d(trx_sys->rw_max_trx_no = 0);
+
+  new (&trx_sys->rsegs) Rsegs();
+  trx_sys->rsegs.set_empty();
+
+  new (&trx_sys->tmp_rsegs) Rsegs();
+  trx_sys->tmp_rsegs.set_empty();
+
+  trx_sys->rw_trx_hash.init();
 }
 
-/** @brief Convert an undo log to TRX_UNDO_PREPARED state on shutdown.
+/** Creates and initializes the transaction system at the database creation. */
+void trx_sys_create_sys_pages(void) {
+  mtr_t mtr;
 
-If any prepared ACTIVE transactions exist, and their rollback was
-prevented by innodb_force_recovery, we convert these transactions to
-XA PREPARE state in the main-memory data structures, so that shutdown
-will proceed normally. These transactions will again recover as ACTIVE
-on the next restart, and they will be rolled back unless
-innodb_force_recovery prevents it again.
+  mtr_start(&mtr);
 
-@param[in]	trx	transaction
-@param[in,out]	undo	undo log to convert to TRX_UNDO_PREPARED */
-static void trx_undo_fake_prepared(const trx_t *trx, trx_undo_t *undo) {
-  ut_ad(srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO);
-  ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE));
-  ut_ad(trx->is_recovered);
+  trx_sysf_create(&mtr);
 
-  if (undo != nullptr) {
-    ut_ad(undo->state == TRX_UNDO_ACTIVE);
-    undo->state = TRX_UNDO_PREPARED;
-  }
+  mtr_commit(&mtr);
 }
 
-/*********************************************************************
-Check if there are any active (non-prepared) transactions.
-@return total number of active transactions or 0 if none */
-ulint trx_sys_any_active_transactions(void) {
-  trx_sys_mutex_enter();
-
-  ulint total_trx = UT_LIST_GET_LEN(trx_sys->mysql_trx_list);
-
-  if (total_trx == 0) {
-    total_trx = UT_LIST_GET_LEN(trx_sys->rw_trx_list);
-    ut_a(total_trx >= trx_sys->n_prepared_trx);
-
-    if (total_trx > trx_sys->n_prepared_trx &&
-        srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO) {
-      for (trx_t *trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-           trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-        if (!trx_state_eq(trx, TRX_STATE_ACTIVE) || !trx->is_recovered) {
-          continue;
-        }
-        /* This was a recovered transaction whose rollback was disabled by
-        the innodb_force_recovery setting. Pretend that it is in XA PREPARE
-        state so that shutdown will work. */
-        trx_undo_fake_prepared(trx, trx->rsegs.m_redo.insert_undo);
-        trx_undo_fake_prepared(trx, trx->rsegs.m_redo.update_undo);
-        trx_undo_fake_prepared(trx, trx->rsegs.m_noredo.insert_undo);
-        trx_undo_fake_prepared(trx, trx->rsegs.m_noredo.update_undo);
-        trx->state = TRX_STATE_PREPARED;
-        trx_sys->n_prepared_trx++;
+/** @return total number of active (non-prepared) transactions. */
+ulint trx_sys_t::any_active_transactions() {
+  ulint total_trx = 0;
+
+  mutex_enter(&mutex);
+  for (trx_t *trx = UT_LIST_GET_FIRST(trx_list); trx != nullptr;
+       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+    if (trx->state == TRX_STATE_COMMITTED_IN_MEMORY ||
+        (trx->state == TRX_STATE_ACTIVE && trx->id > 0)) {
+      total_trx++;
+    }
+
+    if (srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO) {
+      if (trx->state == TRX_STATE_ACTIVE && trx->is_recovered) {
+        total_trx--;
       }
     }
+  }
+  mutex_exit(&mutex);
+  return total_trx;
+}
 
-    ut_a(total_trx >= trx_sys->n_prepared_trx);
-    total_trx -= trx_sys->n_prepared_trx;
+/** Clones the oldest view and stores it in view.
+
+No need to call ReadView::close(). The caller owns the view that is passed in.
+This function is called by purge thread to determine whether it should purge the
+delete marked record or not. */
+void trx_sys_t::clone_oldest_view(ReadView *view) {
+  if (view == NULL) {
+      purge_sys->view.snapshot(nullptr);
   }
+  mutex_enter(&mutex);
+  /* Find oldest view. */
+  for (const trx_t *trx = UT_LIST_GET_FIRST(trx_list); trx != nullptr;
+       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+    int32_t state;
+    while ((state = trx->read_view.get_state()) == READ_VIEW_STATE_SNAPSHOT) {
+      ut_delay(1);
+    }
 
-  trx_sys_mutex_exit();
+    if (state == READ_VIEW_STATE_OPEN) {
+      if (view == NULL) {
+          purge_sys->view.copy(trx->read_view);
+      } else {
+          view->copy(trx->read_view);
+      }
+    }
+  }
+  mutex_exit(&mutex);
+  /* Update view to block purging transaction till GTID is persisted. */
+  auto &gtid_persistor = clone_sys->get_gtid_persistor();
+  auto gtid_oldest_trxno = gtid_persistor.get_oldest_trx_no();
+  purge_sys->view.reduce_low_limit(gtid_oldest_trxno);
+}
 
-  return (total_trx);
+struct trx_sys_found_prepared_trx_callback_arg {
+  uint count;
+};
+
+static bool trx_sys_found_prepared_trx_callback(
+    rw_trx_hash_element_t *element,
+    trx_sys_found_prepared_trx_callback_arg *arg) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
+    if (trx_state_eq(trx, TRX_STATE_PREPARED)) {
+      arg->count++;
+    }
+  }
+  mutex_exit(&element->mutex);
+  return false;
 }
 
-#ifdef UNIV_DEBUG
-/** Validate the trx_ut_list_t.
- @return true if valid. */
-static bool trx_sys_validate_trx_list_low(
-    trx_ut_list_t *trx_list) /*!< in: &trx_sys->rw_trx_list */
-{
-  const trx_t *trx;
-  const trx_t *prev_trx = nullptr;
+/** @return true if found prepared transaction(s). */
+ulint trx_sys_t::n_prepared_trx() {
+  trx_sys_found_prepared_trx_callback_arg arg = {0};
 
-  ut_ad(trx_sys_mutex_own());
+  trx_sys->rw_trx_hash.iterate(reinterpret_cast<lf_hash_walk_func *>(
+                                   trx_sys_found_prepared_trx_callback),
+                               &arg);
 
-  ut_ad(trx_list == &trx_sys->rw_trx_list);
+  return arg.count;
+}
 
-  for (trx = UT_LIST_GET_FIRST(*trx_list); trx != nullptr;
-       prev_trx = trx, trx = UT_LIST_GET_NEXT(trx_list, prev_trx)) {
-    check_trx_state(trx);
-    ut_a(prev_trx == nullptr || prev_trx->id > trx->id);
+trx_id_t trx_sys_t::get_new_trx_id_no_refresh() {
+  /* TODO wcy: why is this function called when doing undo? */
+
+  /* VERY important: after the database is started, max_trx_id value is
+  divisible by TRX_SYS_TRX_ID_WRITE_MARGIN, and the following if
+  will evaluate to TRUE when this function is first time called,
+  and the value for trx id will be written to disk-based header!
+  Thus trx id values will not overlap when the database is
+  repeatedly started! */
+
+  if (get_max_trx_id() % TRX_SYS_TRX_ID_WRITE_MARGIN == 0) {
+    trx_sys_flush_max_trx_id();
   }
 
-  return (true);
+  return max_trx_id.fetch_add(1, std::memory_order_relaxed);
 }
 
-/** Validate the trx_sys_t::rw_trx_list.
- @return true if the list is valid. */
-bool trx_sys_validate_trx_list() {
-  ut_ad(trx_sys_mutex_own());
+/*********************************************************************
+Shutdown/Close the transaction system. */
+void trx_sys_close() {
+  ut_ad(srv_shutdown_state == SRV_SHUTDOWN_EXIT_THREADS);
 
-  ut_a(trx_sys_validate_trx_list_low(&trx_sys->rw_trx_list));
+  if (trx_sys == nullptr) {
+    return;
+  }
 
-  return (true);
+  ulint size = trx_sys->view_count();
+
+  if (size > 0) {
+    ib::error(ER_IB_MSG_1201) << "All read views were not closed before"
+                                 " shutdown: "
+                              << size << " read views open";
+  }
+
+  if (trx_dummy_sess != nullptr) {
+    sess_close(trx_dummy_sess);
+    trx_dummy_sess = nullptr;
+  }
+
+  trx_purge_sys_close();
+
+  trx_sys->rw_trx_hash.destroy();
+
+  /* There can't be any active transactions. */
+  trx_sys->rsegs.~Rsegs();
+
+  trx_sys->tmp_rsegs.~Rsegs();
+
+  ut_a(UT_LIST_GET_LEN(trx_sys->trx_list) == 0);
+
+  /* We used placement new to create this mutex. Call the destructor. */
+  mutex_free(&trx_sys->mutex);
+
+  ut_free(trx_sys);
+
+  trx_sys = nullptr;
 }
-#endif /* UNIV_DEBUG */
 #endif /* !UNIV_HOTBACKUP */
 
 /** A list of undo tablespace IDs found in the TRX_SYS page. These are the
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index 832b901fe00..2faec37a492 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -111,7 +111,7 @@ committed.
                             list of things we need to clean up during
                             trx_erase_lists.
 */
-static void trx_release_impl_and_expl_locks(trx_t *trx, bool serialized);
+static void trx_release_impl_and_expl_locks(trx_t *trx, bool atShutdown = false);
 
 /** Set flush observer for the transaction
 @param[in,out]	trx		transaction struct
@@ -206,7 +206,7 @@ static void trx_init(trx_t *trx) {
 
   trx->last_sql_stat_start.least_undo_no = 0;
 
-  ut_ad(!MVCC::is_view_active(trx->read_view));
+  ut_ad(!trx->read_view.is_open());
 
   trx->lock.rec_cached = 0;
 
@@ -253,6 +253,12 @@ struct TrxFactory {
 
     new (&trx->lock.table_locks) lock_pool_t();
 
+    new (&trx->read_view) ReadView();
+
+    trx->rw_trx_hash_element = nullptr;
+
+    trx->rw_trx_hash_pins = nullptr;
+
     trx_init(trx);
 
     trx->state = TRX_STATE_NOT_STARTED;
@@ -280,8 +286,6 @@ struct TrxFactory {
   @param trx the transaction for which to release resources */
   static void destroy(trx_t *trx) {
     ut_a(trx->magic_n == TRX_MAGIC_N);
-    ut_ad(!trx->in_rw_trx_list);
-    ut_ad(!trx->in_mysql_trx_list);
 
     ut_a(trx->lock.wait_lock == nullptr);
     ut_a(trx->lock.wait_thr == nullptr);
@@ -306,7 +310,8 @@ struct TrxFactory {
 
     trx->mod_tables.~trx_mod_tables_t();
 
-    ut_ad(trx->read_view == nullptr);
+    ut_ad(!trx->read_view.is_open());
+    trx->read_view.~ReadView();
 
     if (!trx->lock.rec_pool.empty()) {
       /* See lock_trx_alloc_locks() why we only free
@@ -346,9 +351,6 @@ struct TrxFactory {
 
     ut_ad(trx->mysql_thd == nullptr);
 
-    ut_ad(!trx->in_rw_trx_list);
-    ut_ad(!trx->in_mysql_trx_list);
-
     ut_a(trx->lock.wait_thr == nullptr);
     ut_a(trx->lock.wait_lock == nullptr);
     ut_a(trx->lock.blocking_trx.load() == nullptr);
@@ -449,6 +451,9 @@ static trx_t *trx_create_low() {
   /* We just got trx from pool, it should be non locking */
   ut_ad(trx->will_lock == 0);
 
+  ut_ad(trx->state == TRX_STATE_NOT_STARTED);
+  ut_ad(trx->rw_trx_hash_pins == nullptr);
+
   trx->persists_gtid = false;
 
   trx->api_trx = false;
@@ -476,15 +481,52 @@ static trx_t *trx_create_low() {
   trx_free(). */
   ut_a(trx->mod_tables.size() == 0);
 
+  trx_sys->register_trx(trx);
   return (trx);
 }
 
 /**
 Release a trx_t instance back to the pool.
 @param trx the instance to release. */
-static void trx_free(trx_t *&trx) {
+void trx_free(trx_t *&trx) {
+  ut_ad(!trx->declared_to_be_inside_innodb);
+  ut_ad(trx->n_mysql_tables_in_use == 0);
+  ut_ad(trx->mysql_n_tables_locked == 0);
+  ut_ad(!trx->internal);
+
+  if (trx->declared_to_be_inside_innodb) {
+    ib::error(ER_IB_MSG_1270)
+        << "Freeing a trx (" << trx << ", " << trx_get_id_for_print(trx)
+        << ") which is declared"
+           " to be processing inside InnoDB";
+
+    trx_print(stderr, trx, 600);
+    putc('\n', stderr);
+
+    /* This is an error but not a fatal error. We must keep the coutners like
+    srv_conc.n_active accurate. */
+    srv_conc_force_exit_innodb(trx);
+  }
+
+  if (trx->n_mysql_tables_in_use > 0 || trx->mysql_n_tables_locked > 0) {
+    ib::error(ER_IB_MSG_1270)
+        << "MySQL is freeing a thd though"
+           " trx->n_mysql_tables_in_use is "
+        << trx->n_mysql_tables_in_use << " and trx->mysql_n_tables_locked is "
+        << trx->mysql_n_tables_locked << ".";
+
+    trx_print(stderr, trx, 600);
+    ut_print_buf(stderr, trx, sizeof(trx_t));
+    putc('\n', stderr);
+  }
+
+  trx->dict_operation = TRX_DICT_OP_NONE;
+  assert_trx_is_inactive(trx);
+
+  trx_sys->deregister_trx(trx);
   assert_trx_is_free(trx);
 
+  trx_sys->rw_trx_hash.put_pins(trx);
   trx->mysql_thd = nullptr;
 
   // FIXME: We need to avoid this heap free/alloc for each commit.
@@ -497,7 +539,6 @@ static void trx_free(trx_t *&trx) {
 
   trx->mod_tables.clear();
 
-  ut_ad(trx->read_view == nullptr);
   ut_ad(trx->is_dd_trx == false);
 
   /* trx locking state should have been reset before returning trx
@@ -528,13 +569,6 @@ trx_t *trx_allocate_for_mysql(void) {
 
   trx = trx_allocate_for_background();
 
-  trx_sys_mutex_enter();
-
-  ut_d(trx->in_mysql_trx_list = TRUE);
-  UT_LIST_ADD_FIRST(trx_sys->mysql_trx_list, trx);
-
-  trx_sys_mutex_exit();
-
   return (trx);
 }
 
@@ -570,16 +604,6 @@ static void trx_validate_state_before_free(trx_t *trx) {
   assert_trx_is_inactive(trx);
 }
 
-/** Free and initialize a transaction object instantiated during recovery.
-@param[in,out]	trx	transaction object to free and initialize */
-void trx_free_resurrected(trx_t *trx) {
-  trx_validate_state_before_free(trx);
-
-  trx_init(trx);
-
-  trx_free(trx);
-}
-
 /** Free a transaction that was allocated by background or user threads.
 @param[in,out]	trx	transaction object to free */
 void trx_free_for_background(trx_t *trx) {
@@ -588,25 +612,28 @@ void trx_free_for_background(trx_t *trx) {
   trx_free(trx);
 }
 
-/** At shutdown, frees a transaction object that is in the PREPARED state. */
-void trx_free_prepared(trx_t *trx) /*!< in, own: trx object */
-{
-  ut_a(trx_state_eq(trx, TRX_STATE_PREPARED));
+/** At shutdown, frees a transaction object. */
+void trx_free_at_shutdown(trx_t *trx) {
+  ut_ad(trx->is_recovered);
+  ut_a(trx_state_eq(trx, TRX_STATE_PREPARED) ||
+       trx_state_eq(trx, TRX_STATE_ACTIVE));
   ut_a(trx->magic_n == TRX_MAGIC_N);
 
-  assert_trx_in_rw_list(trx);
+  trx_release_impl_and_expl_locks(trx, true);
+  trx_undo_free_at_shutdown(trx);
 
-  trx_release_impl_and_expl_locks(trx, false);
-  trx_undo_free_prepared(trx);
-
-  ut_ad(!trx->in_rw_trx_list);
   ut_a(!trx->read_only);
 
+  DBUG_LOG("trx", "Free prepared: " << trx);
   trx->state = TRX_STATE_NOT_STARTED;
 
   /* Undo trx_resurrect_table_locks(). */
   lock_trx_lock_list_init(&trx->lock.trx_locks);
 
+  /* Note: This vector is not guaranteed to be empty because the transaction
+  was never committed and therefore lock_trx_release() was not called. */
+  trx->lock.table_locks.clear();
+
   trx_free(trx);
 }
 
@@ -619,18 +646,7 @@ finally freed.
 @param[in]	prepared	boolean value to specify whether trx is
                                 for recovery or not. */
 inline void trx_disconnect_from_mysql(trx_t *trx, bool prepared) {
-  trx_sys_mutex_enter();
-
-  ut_ad(trx->in_mysql_trx_list);
-  ut_d(trx->in_mysql_trx_list = FALSE);
-
-  UT_LIST_REMOVE(trx_sys->mysql_trx_list, trx);
-
-  if (trx->read_view != nullptr) {
-    trx_sys->mvcc->view_close(trx->read_view, true);
-  }
-
-  ut_ad(trx_sys_validate_trx_list());
+    trx->read_view.close();
 
   if (prepared) {
     ut_ad(trx_state_eq(trx, TRX_STATE_PREPARED));
@@ -640,8 +656,6 @@ inline void trx_disconnect_from_mysql(trx_t *trx, bool prepared) {
     /* todo/fixme: suggest to do it at innodb prepare */
     trx->will_lock = 0;
   }
-
-  trx_sys_mutex_exit();
 }
 
 /** Disconnect a transaction from MySQL.
@@ -794,7 +808,6 @@ static trx_t *trx_resurrect_insert(
 
       if (srv_force_recovery == 0) {
         trx->state = TRX_STATE_PREPARED;
-        ++trx_sys->n_prepared_trx;
       } else {
         ib::info(ER_IB_MSG_1205) << "Since innodb_force_recovery"
                                     " > 0, we will force a rollback.";
@@ -858,7 +871,6 @@ static void trx_resurrect_update_in_prepared_state(
     ut_ad(trx->state != TRX_STATE_FORCED_ROLLBACK);
 
     if (trx_state_eq(trx, TRX_STATE_NOT_STARTED)) {
-      ++trx_sys->n_prepared_trx;
     } else {
       ut_ad(trx_state_eq(trx, TRX_STATE_PREPARED));
     }
@@ -951,7 +963,8 @@ static void trx_resurrect(trx_rseg_t *rseg) {
        undo = UT_LIST_GET_NEXT(undo_list, undo)) {
     trx = trx_resurrect_insert(undo, rseg);
 
-    trx_sys_rw_trx_add(trx);
+    trx_sys->rw_trx_hash.insert(trx);
+    trx_sys->rw_trx_hash.put_pins(trx);
 
     trx_resurrect_table_ids(trx, &trx->rsegs.m_redo, undo);
   }
@@ -959,23 +972,22 @@ static void trx_resurrect(trx_rseg_t *rseg) {
   /* Ressurrect transactions that were doing updates. */
   for (undo = UT_LIST_GET_FIRST(rseg->update_undo_list); undo != nullptr;
        undo = UT_LIST_GET_NEXT(undo_list, undo)) {
-    /* Check the trx_sys->rw_trx_set first. */
-    trx_sys_mutex_enter();
-
-    trx_t *trx = trx_get_rw_trx_by_id(undo->trx_id);
-
-    trx_sys_mutex_exit();
+    /* Check the trx_sys->rw_trx_hash first. */
+    trx_t *trx = trx_sys->find(nullptr, undo->trx_id, false);
 
     if (trx == nullptr) {
       trx = trx_allocate_for_background();
 
       ut_d(trx->start_file = __FILE__);
       ut_d(trx->start_line = __LINE__);
-    }
 
-    trx_resurrect_update(trx, undo, rseg);
+      trx_resurrect_update(trx, undo, rseg);
 
-    trx_sys_rw_trx_add(trx);
+      trx_sys->rw_trx_hash.insert(trx);
+      trx_sys->rw_trx_hash.put_pins(trx);
+    } else {
+      trx_resurrect_update(trx, undo, rseg);
+    }
 
     trx_resurrect_table_ids(trx, &trx->rsegs.m_redo, undo);
   }
@@ -1006,19 +1018,6 @@ void trx_lists_init_at_db_start(void) {
     undo_space->rsegs()->s_unlock();
   }
   undo::spaces->s_unlock();
-
-  TrxIdSet::iterator end = trx_sys->rw_trx_set.end();
-
-  for (TrxIdSet::iterator it = trx_sys->rw_trx_set.begin(); it != end; ++it) {
-    ut_ad(it->m_trx->in_rw_trx_list);
-
-    if (it->m_trx->state == TRX_STATE_ACTIVE ||
-        it->m_trx->state == TRX_STATE_PREPARED) {
-      trx_sys->rw_trx_ids.push_back(it->m_id);
-    }
-
-    UT_LIST_ADD_FIRST(trx_sys->rw_trx_list, it->m_trx);
-  }
 }
 
 /** Get next redo rollback segment in round-robin fashion.
@@ -1186,15 +1185,7 @@ void trx_assign_rseg_temp(trx_t *trx) {
       srv_read_only_mode ? nullptr : get_next_temp_rseg();
 
   if (trx->id == 0) {
-    mutex_enter(&trx_sys->mutex);
-
-    trx->id = trx_sys_get_new_trx_id();
-
-    trx_sys->rw_trx_ids.push_back(trx->id);
-
-    trx_sys->rw_trx_set.insert(TrxTrack(trx->id, trx));
-
-    mutex_exit(&trx_sys->mutex);
+    trx_sys->register_rw(trx);
   }
 }
 
@@ -1257,17 +1248,14 @@ static void trx_start_low(
   ut_a(ib_vector_is_empty(trx->lock.autoinc_locks));
   ut_a(trx->lock.table_locks.empty());
 
-  /* If this transaction came from trx_allocate_for_mysql(),
-  trx->in_mysql_trx_list would hold. In that case, the trx->state
-  change must be protected by the trx_sys->mutex, so that
-  lock_print_info_all_transactions() will have a consistent view. */
+  /* No other thread can access this trx object through rw_trx_hash, thus we
+  don't need trx_sys.mutex protection for that purposes. Still this trx can be
+  found through trx_sys->trx_list, which means state change must be protected by
+  e.g. trx->mutex.
 
-  ut_ad(!trx->in_rw_trx_list);
-
-  /* We tend to over assert and that complicates the code somewhat.
-  e.g., the transaction state can be set earlier but we are forced to
-  set it under the protection of the trx_sys_t::mutex because some
-  trx list assertions are triggered unnecessarily. */
+  For now we update it without mutex protection, because original code did it in
+  this way. It has to be reviewed and fixed properly. */
+  trx->state = TRX_STATE_ACTIVE;
 
   /* By default all transactions are in the read-only list unless they
   are non-locking auto-commit read only transactions or background
@@ -1282,27 +1270,10 @@ static void trx_start_low(
     /* Temporary rseg is assigned only if the transaction
     updates a temporary table */
 
-    trx_sys_mutex_enter();
-
-    trx->id = trx_sys_get_new_trx_id();
-
-    trx_sys->rw_trx_ids.push_back(trx->id);
-
-    trx_sys_rw_trx_add(trx);
-
     ut_ad(trx->rsegs.m_redo.rseg != nullptr || srv_read_only_mode ||
           srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO);
 
-    UT_LIST_ADD_FIRST(trx_sys->rw_trx_list, trx);
-
-    ut_d(trx->in_rw_trx_list = true);
-
-    trx->state = TRX_STATE_ACTIVE;
-
-    ut_ad(trx_sys_validate_trx_list());
-
-    trx_sys_mutex_exit();
-
+    trx_sys->register_rw(trx);
   } else {
     trx->id = 0;
 
@@ -1312,24 +1283,11 @@ static void trx_start_low(
       to write to the temporary table. */
 
       if (read_write) {
-        trx_sys_mutex_enter();
-
         ut_ad(!srv_read_only_mode);
-
-        trx->id = trx_sys_get_new_trx_id();
-
-        trx_sys->rw_trx_ids.push_back(trx->id);
-
-        trx_sys->rw_trx_set.insert(TrxTrack(trx->id, trx));
-
-        trx_sys_mutex_exit();
+        trx_sys->register_rw(trx);
       }
-
-      trx->state = TRX_STATE_ACTIVE;
-
     } else {
       ut_ad(!read_write);
-      trx->state = TRX_STATE_ACTIVE;
     }
   }
 
@@ -1350,8 +1308,9 @@ static void trx_start_low(
 
 /** Set the transaction serialisation number.
  @return true if the transaction number was added to the serialisation_list. */
-static bool trx_serialisation_number_get(
+static void trx_serialise(
     trx_t *trx,                         /*!< in/out: transaction */
+    mtr_t *mtr,                         /*!< in/out: mini-transaction */
     trx_undo_ptr_t *redo_rseg_undo_ptr, /*!< in/out: Set trx
                                         serialisation number in
                                         referred undo rseg. */
@@ -1359,7 +1318,6 @@ static bool trx_serialisation_number_get(
                                         serialisation number in
                                         referred undo rseg. */
 {
-  bool added_trx_no;
   trx_rseg_t *redo_rseg = nullptr;
   trx_rseg_t *temp_rseg = nullptr;
 
@@ -1375,19 +1333,27 @@ static bool trx_serialisation_number_get(
 
   trx_sys_mutex_enter();
 
-  trx->no = trx_sys_get_new_trx_id();
+  if ((redo_rseg != nullptr && redo_rseg->last_page_no == FIL_NULL) ||
+      (temp_rseg != nullptr && temp_rseg->last_page_no == FIL_NULL)) {
+    /* NOTE: here we MUST acquire the mutex to protect the purge queue before
+    invoking assign_new_trx_no, otherwise it might be possible to push a
+    segment with smaller trx_no than last time popped. */
+    mutex_enter(&purge_sys->pq_mutex);
+  }
+
+  trx_sys->assign_new_trx_no(trx);
 
   /* Update the latest transaction number. */
   ut_d(trx_sys->rw_max_trx_no = trx->no);
 
-  /* Track the minimum serialisation number. */
   if (!trx->read_only) {
-    UT_LIST_ADD_LAST(trx_sys->serialisation_list, trx);
-    added_trx_no = true;
+    trx->serialised = true;
   } else {
-    added_trx_no = false;
+    trx->serialised = false;
   }
 
+  trx_sys_mutex_exit();
+
   /* If the rollack segment is not empty then the
   new trx_t::no can't be less than any trx_t::no
   already in the rollback segment. User threads only
@@ -1404,29 +1370,22 @@ static bool trx_serialisation_number_get(
       elem.push_back(temp_rseg);
     }
 
-    mutex_enter(&purge_sys->pq_mutex);
-
-    /* This is to reduce the pressure on the trx_sys_t::mutex
-    though in reality it should make very little (read no)
-    difference because this code path is only taken when the
-    rbs is empty. */
-
-    trx_sys_mutex_exit();
-
     purge_sys->purge_queue->push(elem);
 
     mutex_exit(&purge_sys->pq_mutex);
-  } else {
-    trx_sys_mutex_exit();
   }
 
-  return (added_trx_no);
+  /* new_id is a special trx id created for r/w trx which has update
+  undo. Such trx id is used by purge and the trx itself has no activity
+  so it's "effectived" committed right away. We need explicitly log it
+  in redo log, otherwise replica nodes would think this trx remains
+  active for ever. */
 }
 
 /** Assign the transaction its history serialisation number and write the
  update UNDO log record to the assigned rollback segment.
  @return true if a serialisation log was written */
-static bool trx_write_serialisation_history(
+static void trx_write_serialisation_history(
     trx_t *trx, /*!< in/out: transaction */
     mtr_t *mtr) /*!< in/out: mini-transaction */
 {
@@ -1467,8 +1426,6 @@ static bool trx_write_serialisation_history(
     trx_undo_set_state_at_finish(trx->rsegs.m_noredo.insert_undo, &temp_mtr);
   }
 
-  bool serialised = false;
-
   /* If transaction involves update then add rollback segments
   to purge queue. */
   if (trx->rsegs.m_redo.update_undo != nullptr ||
@@ -1486,8 +1443,7 @@ static bool trx_write_serialisation_history(
                                                    : nullptr;
 
     /* Will set trx->no and will add rseg to purge queue. */
-    serialised = trx_serialisation_number_get(trx, redo_rseg_undo_ptr,
-                                              temp_rseg_undo_ptr);
+    trx_serialise(trx, mtr, redo_rseg_undo_ptr, temp_rseg_undo_ptr);
 
     /* It is not necessary to obtain trx->undo_mutex here because
     only a single OS thread is allowed to do the transaction commit
@@ -1546,8 +1502,6 @@ static bool trx_write_serialisation_history(
   if (Clone_handler::need_commit_order()) {
     trx_sys_update_mysql_binlog_offset(trx, mtr);
   }
-
-  return (serialised);
 }
 
 /********************************************************************
@@ -1690,85 +1644,34 @@ static void trx_update_mod_tables_timestamp(trx_t *trx) /*!< in: transaction */
   trx->mod_tables.clear();
 }
 
-/**
-Erase the transaction from running transaction lists and serialization
-list. Active RW transaction list of a MVCC snapshot(ReadView::prepare)
-won't include this transaction after this call. All implicit locks are
-also released by this call as trx is removed from rw_trx_list.
-@param[in]	trx		Transaction to erase, must have an ID > 0
-@param[in]	serialised	true if serialisation log was written
-@param[in]	gtid_desc	GTID information to persist */
-static void trx_erase_lists(trx_t *trx, bool serialised, Gtid_desc &gtid_desc) {
-  ut_ad(trx->id > 0);
-  ut_ad(trx_sys_mutex_own());
-
-  if (serialised) {
-    UT_LIST_REMOVE(trx_sys->serialisation_list, trx);
-
-    /* Add GTID to be persisted to disk table. It must be done ...
-    1.After the transaction is marked committed in undo. Otherwise
-      GTID might get committed before the transaction commit on disk.
-    2.Before it is removed from serialization list. Otherwise the transaction
-      undo could get purged before persisting GTID on disk table. */
-    if (gtid_desc.m_is_set) {
-      auto &gtid_persistor = clone_sys->get_gtid_persistor();
-      gtid_persistor.add(gtid_desc);
-    }
-  }
-
-  trx_ids_t::iterator it = std::lower_bound(trx_sys->rw_trx_ids.begin(),
-                                            trx_sys->rw_trx_ids.end(), trx->id);
-  ut_ad(*it == trx->id);
-  trx_sys->rw_trx_ids.erase(it);
-
-  if (trx->read_only || trx->rsegs.m_redo.rseg == nullptr) {
-    ut_ad(!trx->in_rw_trx_list);
-  } else {
-    UT_LIST_REMOVE(trx_sys->rw_trx_list, trx);
-    ut_d(trx->in_rw_trx_list = false);
-    ut_ad(trx_sys_validate_trx_list());
-
-    if (trx->read_view != nullptr) {
-      trx_sys->mvcc->view_close(trx->read_view, true);
-    }
-  }
-
-  trx_sys->rw_trx_set.erase(TrxTrack(trx->id));
-
-  /* Set minimal active trx id. */
-  trx_id_t min_id = trx_sys->rw_trx_ids.empty() ? trx_sys->max_trx_id
-                                                : trx_sys->rw_trx_ids.front();
-
-  trx_sys->min_active_id.store(min_id);
-}
-
-static void trx_release_impl_and_expl_locks(trx_t *trx, bool serialized) {
+static void trx_release_impl_and_expl_locks(trx_t *trx, bool atShutdown) {
   check_trx_state(trx);
   ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE) ||
         trx_state_eq(trx, TRX_STATE_PREPARED));
 
-  bool trx_sys_latch_is_needed =
-      (trx->id > 0) || trx_state_eq(trx, TRX_STATE_PREPARED);
-
   /* Check and get GTID to be persisted. Do it outside trx_sys mutex. */
   Gtid_desc gtid_desc;
   auto &gtid_persistor = clone_sys->get_gtid_persistor();
   gtid_persistor.get_gtid_info(trx, gtid_desc);
 
-  if (trx_sys_latch_is_needed) {
-    trx_sys_mutex_enter();
-  }
-
   if (trx->id > 0) {
     /* For consistent snapshot, we need to remove current
     transaction from running transaction id list for mvcc
     before doing commit and releasing locks. */
-    trx_erase_lists(trx, serialized, gtid_desc);
-  }
-
-  if (trx_state_eq(trx, TRX_STATE_PREPARED)) {
-    ut_a(trx_sys->n_prepared_trx > 0);
-    --trx_sys->n_prepared_trx;
+    if (!atShutdown) {
+      trx_sys->deregister_rw(trx);
+    }
+    if (trx->serialised) {
+      /* Add GTID to be persisted to disk table. It must be done ...
+      1.After the transaction is marked committed in undo. Otherwise
+        GTID might get committed before the transaction commit on disk.
+      2.Before it is removed from serialization list. Otherwise the transaction
+        undo could get purged before persisting GTID on disk table. */
+      if (gtid_desc.m_is_set) {
+        auto &gtid_persistor = clone_sys->get_gtid_persistor();
+        gtid_persistor.add(gtid_desc);
+      }
+    }
   }
 
   trx_mutex_enter(trx);
@@ -1792,32 +1695,25 @@ static void trx_release_impl_and_expl_locks(trx_t *trx, bool serialized) {
   trx->state = TRX_STATE_COMMITTED_IN_MEMORY;
   trx_mutex_exit(trx);
 
-  if (trx_sys_latch_is_needed) {
-    trx_sys_mutex_exit();
-  }
-
   lock_trx_release_locks(trx);
 }
 
 /** Commits a transaction in memory. */
 static void trx_commit_in_memory(
     trx_t *trx,       /*!< in/out: transaction */
-    const mtr_t *mtr, /*!< in: mini-transaction of
+    const mtr_t *mtr) /*!< in: mini-transaction of
                       trx_write_serialisation_history(), or NULL if
                       the transaction did not modify anything */
-    bool serialised)
-/*!< in: true if serialisation log was
-written */
 {
   trx->must_flush_log_later = false;
   trx->ddl_must_flush = false;
+  trx->read_view.close();
 
   if (trx_is_autocommit_non_locking(trx)) {
     ut_ad(trx->id == 0);
     ut_ad(trx->read_only);
     ut_a(!trx->is_recovered);
     ut_ad(trx->rsegs.m_redo.rseg == nullptr);
-    ut_ad(!trx->in_rw_trx_list);
 
     /* Note: We are asserting without holding the locksys latch. But
     that is OK because this transaction is not waiting and cannot
@@ -1835,10 +1731,6 @@ written */
 
     ut_ad(trx_state_eq(trx, TRX_STATE_ACTIVE));
 
-    if (trx->read_view != nullptr) {
-      trx_sys->mvcc->view_close(trx->read_view, false);
-    }
-
     MONITOR_INC(MONITOR_TRX_NL_RO_COMMIT);
 
     /* AC-NL-RO transactions can't be rolled back asynchronously. */
@@ -1848,7 +1740,8 @@ written */
     trx->state = TRX_STATE_NOT_STARTED;
 
   } else {
-    trx_release_impl_and_expl_locks(trx, serialised);
+    trx_release_impl_and_expl_locks(trx);
+    DEBUG_SYNC_C("after_remove_serial_list_release_locks");
 
     /* Remove the transaction from the list of active
     transactions now that it no longer holds any user locks. */
@@ -1858,10 +1751,6 @@ written */
 
     if (trx->read_only || trx->rsegs.m_redo.rseg == nullptr) {
       MONITOR_INC(MONITOR_TRX_RO_COMMIT);
-      if (trx->read_view != nullptr) {
-        trx_sys->mvcc->view_close(trx->read_view, false);
-      }
-
     } else {
       ut_ad(trx->id > 0);
       MONITOR_INC(MONITOR_TRX_RW_COMMIT);
@@ -2016,12 +1905,10 @@ void trx_commit_low(
     }
   }
 
-  bool serialised;
-
   if (mtr != nullptr) {
     mtr->set_sync();
 
-    serialised = trx_write_serialisation_history(trx, mtr);
+    trx_write_serialisation_history(trx, mtr);
 
     /* The following call commits the mini-transaction, making the
     whole transaction committed in the file-based world, at this
@@ -2058,8 +1945,6 @@ void trx_commit_low(
         });
     /*--------------*/
 
-  } else {
-    serialised = false;
   }
 #ifdef UNIV_DEBUG
   /* In case of this function is called from a stack executing
@@ -2075,7 +1960,7 @@ void trx_commit_low(
   }
 #endif
 
-  trx_commit_in_memory(trx, mtr, serialised);
+  trx_commit_in_memory(trx, mtr);
 }
 
 /** Commits a transaction. */
@@ -2101,64 +1986,6 @@ void trx_commit(trx_t *trx) /*!< in/out: transaction */
   trx_commit_low(trx, mtr);
 }
 
-/** Cleans up a transaction at database startup. The cleanup is needed if
- the transaction already got to the middle of a commit when the database
- crashed, and we cannot roll it back. */
-void trx_cleanup_at_db_startup(trx_t *trx) /*!< in: transaction */
-{
-  ut_ad(trx->is_recovered);
-
-  /* Cleanup any durable undo logs in non-temporary rollback segments.
-  At database start-up there are no active transactions recorded in
-  any rollback segments in the temporary tablespace because all those
-  changes are all lost on restart. */
-  if (trx->rsegs.m_redo.insert_undo != nullptr) {
-    trx_undo_insert_cleanup(&trx->rsegs.m_redo, false);
-  }
-
-  memset(&trx->rsegs, 0x0, sizeof(trx->rsegs));
-  trx->undo_no = 0;
-  trx->undo_rseg_space = 0;
-  trx->last_sql_stat_start.least_undo_no = 0;
-
-  trx_sys_mutex_enter();
-
-  ut_a(!trx->read_only);
-
-  UT_LIST_REMOVE(trx_sys->rw_trx_list, trx);
-
-  ut_d(trx->in_rw_trx_list = FALSE);
-
-  trx_sys_mutex_exit();
-
-  /* Change the transaction state without mutex protection, now
-  that it no longer is in the trx_list. Recovered transactions
-  are never placed in the mysql_trx_list. */
-  ut_ad(trx->is_recovered);
-  ut_ad(!trx->in_rw_trx_list);
-  ut_ad(!trx->in_mysql_trx_list);
-  trx->state = TRX_STATE_NOT_STARTED;
-}
-
-/** Assigns a read view for a consistent read query. All the consistent reads
- within the same transaction will get the same read view, which is created
- when this function is first called for a new started transaction.
- @return consistent read view */
-ReadView *trx_assign_read_view(trx_t *trx) /*!< in/out: active transaction */
-{
-  ut_ad(trx->state == TRX_STATE_ACTIVE);
-
-  if (srv_read_only_mode) {
-    ut_ad(trx->read_view == nullptr);
-    return (nullptr);
-
-  } else if (!MVCC::is_view_active(trx->read_view)) {
-    trx_sys->mvcc->view_open(trx->read_view, trx);
-  }
-
-  return (trx->read_view);
-}
-
 /** Prepares a transaction for commit/rollback. */
 void trx_commit_or_rollback_prepare(trx_t *trx) /*!< in/out: transaction */
 {
@@ -2380,8 +2207,6 @@ void trx_print_low(FILE *f,
   ibool newline;
   const char *op_info;
 
-  ut_ad(trx_sys_mutex_own());
-
   fprintf(f, "TRANSACTION " TRX_ID_FMT, trx_get_id_for_print(trx));
 
   /* trx->state cannot change from or to NOT_STARTED while we
@@ -2491,7 +2316,6 @@ void trx_print_latched(FILE *f, const trx_t *trx, ulint max_query_len) {
   /* We need exclusive access to lock_sys for lock_number_of_rows_locked(),
   and accessing trx->lock fields without trx->mutex.*/
   ut_ad(locksys::owns_exclusive_global_latch());
-  ut_ad(trx_sys_mutex_own());
 
   trx_print_low(f, trx, max_query_len, lock_number_of_rows_locked(&trx->lock),
                 UT_LIST_GET_LEN(trx->lock.trx_locks),
@@ -2501,9 +2325,7 @@ void trx_print_latched(FILE *f, const trx_t *trx, ulint max_query_len) {
 void trx_print(FILE *f, const trx_t *trx, ulint max_query_len) {
   /* trx_print_latched() requires exclusive global latch */
   locksys::Global_exclusive_latch_guard guard{};
-  mutex_enter(&trx_sys->mutex);
   trx_print_latched(f, trx, max_query_len);
-  mutex_exit(&trx_sys->mutex);
 }
 
 #ifdef UNIV_DEBUG
@@ -2516,8 +2338,6 @@ bool trx_can_be_handled_by_current_thread(const trx_t *trx) {
  @return true if started */
 ibool trx_assert_started(const trx_t *trx) /*!< in: transaction */
 {
-  ut_ad(trx_sys_mutex_own());
-
   /* Non-locking autocommits should not hold any locks and this
   function is only called from the locking code. */
   check_trx_state(trx);
@@ -2806,14 +2626,13 @@ static void trx_prepare(trx_t *trx) /*!< in/out: transaction */
 
   /*--------------------------------------*/
   ut_a(trx->state == TRX_STATE_ACTIVE);
-  trx_sys_mutex_enter();
+  trx_mutex_enter(trx);
   trx->state = TRX_STATE_PREPARED;
-  trx_sys->n_prepared_trx++;
   /* Add GTID to be persisted to disk table, if needed. */
   if (gtid_desc.m_is_set) {
     gtid_persistor.add(gtid_desc);
   }
-  trx_sys_mutex_exit();
+  trx_mutex_exit(trx);
   /*--------------------------------------*/
 
   /* Reset after successfully adding GTID to in memory table. */
@@ -2952,38 +2771,30 @@ static bool get_info_about_prepared_transaction(XA_recover_txn *txn_list,
   return false;
 }
 
-/** This function is used to find number of prepared transactions and
- their transaction objects for a recovery.
- @return number of prepared transactions stored in xid_list */
-int trx_recover_for_mysql(
-    XA_recover_txn *txn_list, /*!< in/out: prepared transactions */
-    ulint len,                /*!< in: number of slots in xid_list */
-    MEM_ROOT *mem_root)       /*!< in: memory for table names */
-{
-  const trx_t *trx;
-  ulint count = 0;
-
-  ut_ad(txn_list);
-  ut_ad(len);
-
-  /* We should set those transactions which are in the prepared state
-  to the xid_list */
-
-  trx_sys_mutex_enter();
-
-  for (trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    assert_trx_in_rw_list(trx);
+struct trx_recover_for_mysql_callback_arg {
+  XA_recover_txn *txn_list;
+  MEM_ROOT *mem_root;
+  ulint len;
+  ulint count;
+};
 
+static bool trx_recover_for_mysql_callback(
+    rw_trx_hash_element_t *element, trx_recover_for_mysql_callback_arg *arg) {
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
     /* The state of a read-write transaction cannot change
     from or to NOT_STARTED while we are holding the
     trx_sys->mutex. It may change to PREPARED, but not if
-    trx->is_recovered. */
+    trx->is_recovered. It may also change to COMMITTED. */
     if (trx_state_eq(trx, TRX_STATE_PREPARED)) {
-      if (get_info_about_prepared_transaction(&txn_list[count], trx, mem_root))
-        break;
+      if (get_info_about_prepared_transaction(&arg->txn_list[arg->count], trx,
+                                              arg->mem_root)) {
+        mutex_exit(&element->mutex);
+        return true;
+      }
 
-      if (count == 0) {
+      if (arg->count == 0) {
         ib::info(ER_IB_MSG_1207) << "Starting recovery for"
                                     " XA transactions...";
       }
@@ -2994,74 +2805,80 @@ int trx_recover_for_mysql(
       ib::info(ER_IB_MSG_1209)
           << "Transaction contains changes to " << trx->undo_no << " rows";
 
-      count++;
-
-      if (count == len) {
-        break;
-      }
+      arg->txn_list[arg->count++].id = *trx->xid;
     }
   }
+  mutex_exit(&element->mutex);
+  return (arg->count == arg->len);
+}
 
-  trx_sys_mutex_exit();
+/** This function is used to find number of prepared transactions and
+ their transaction objects for a recovery.
+ @return number of prepared transactions stored in xid_list */
+int trx_recover_for_mysql(
+    XA_recover_txn *txn_list, /*!< in/out: prepared transactions */
+    ulint len,                /*!< in: number of slots in xid_list */
+    MEM_ROOT *mem_root)       /*!< in: memory for table names */
+{
+  ut_ad(txn_list != nullptr);
+  ut_ad(len > 0);
+
+  trx_recover_for_mysql_callback_arg arg = {txn_list, mem_root, len, 0U};
+
+  /* Fill txb_list with PREPARED transactions. */
+  trx_sys->rw_trx_hash.iterate_no_dups(
+      reinterpret_cast<lf_hash_walk_func *>(trx_recover_for_mysql_callback),
+      &arg);
 
-  if (count > 0) {
-    ib::info(ER_IB_MSG_1210) << count
+  if (arg.count > 0) {
+    ib::info(ER_IB_MSG_1210) << arg.count
                              << " transactions in prepared state"
                                 " after recovery";
   }
 
-  return (int(count));
+  return (static_cast<int>(arg.count));
 }
 
-/** This function is used to find one X/Open XA distributed transaction
- which is in the prepared state
- @return trx on match, the trx->xid will be invalidated;
- */
-static MY_ATTRIBUTE((warn_unused_result)) trx_t *trx_get_trx_by_xid_low(
-    const XID *xid) /*!< in: X/Open XA transaction
-                    identifier */
-{
+struct trx_get_trx_by_xid_callback_arg {
+  const XID *xid;
   trx_t *trx;
+};
 
-  ut_ad(trx_sys_mutex_own());
-
-  for (trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list); trx != nullptr;
-       trx = UT_LIST_GET_NEXT(trx_list, trx)) {
-    assert_trx_in_rw_list(trx);
+static bool trx_get_trx_by_xid_callback(rw_trx_hash_element_t *element,
+                                        trx_get_trx_by_xid_callback_arg *arg) {
+  bool found = false;
 
+  mutex_enter(&element->mutex);
+  trx_t *trx = element->trx;
+  if (trx != nullptr) {
     /* Compare two X/Open XA transaction id's: their
     length should be the same and binary comparison
     of gtrid_length+bqual_length bytes should be
     the same */
 
     if (trx->is_recovered && trx_state_eq(trx, TRX_STATE_PREPARED) &&
-        xid->eq(trx->xid)) {
+        arg->xid->eq(trx->xid)) {
       /* Invalidate the XID, so that subsequent calls
       will not find it. */
       trx->xid->reset();
-      break;
+      arg->trx = trx;
+      found = true;
     }
   }
+  mutex_exit(&element->mutex);
 
-  return (trx);
+  return found;
 }
 
 trx_t *trx_get_trx_by_xid(const XID *xid) {
-  trx_t *trx;
-
-  if (xid == nullptr) {
-    return (nullptr);
+  trx_get_trx_by_xid_callback_arg arg = {xid, nullptr};
+  if (xid != nullptr) {
+    trx_sys->rw_trx_hash.iterate(
+        reinterpret_cast<lf_hash_walk_func *>(trx_get_trx_by_xid_callback),
+        &arg);
   }
 
-  trx_sys_mutex_enter();
-
-  /* Recovered/Resurrected transactions are always only on the
-  trx_sys_t::rw_trx_list. */
-  trx = trx_get_trx_by_xid_low(xid);
-
-  trx_sys_mutex_exit();
-
-  return (trx);
+  return arg.trx;
 }
 
 /** Starts the transaction if it is not yet started. */
@@ -3080,7 +2897,7 @@ void trx_start_if_not_started_xa_low(
         /* If the transaction is tagged as read-only then
         it can only write to temp tables and for such
         transactions we don't want to move them to the
-        trx_sys_t::rw_trx_list. */
+        trx_sys_t::rw_trx_hash. */
         if (!trx->read_only) {
           trx_set_rw_mode(trx);
         } else if (!srv_read_only_mode) {
@@ -3158,7 +2975,6 @@ void trx_start_internal_read_only_low(trx_t *trx) {
 void trx_set_rw_mode(trx_t *trx) /*!< in/out: transaction that is RW */
 {
   ut_ad(trx->rsegs.m_redo.rseg == nullptr);
-  ut_ad(!trx->in_rw_trx_list);
   ut_ad(!trx_is_autocommit_non_locking(trx));
   ut_ad(!trx->read_only);
 
@@ -3177,25 +2993,12 @@ void trx_set_rw_mode(trx_t *trx) /*!< in/out: transaction that is RW */
 
   ut_ad(trx->rsegs.m_redo.rseg != nullptr);
 
-  mutex_enter(&trx_sys->mutex);
-
-  ut_ad(trx->id == 0);
-  trx->id = trx_sys_get_new_trx_id();
-
-  trx_sys->rw_trx_ids.push_back(trx->id);
-
-  trx_sys->rw_trx_set.insert(TrxTrack(trx->id, trx));
+  trx_sys->register_rw(trx);
 
   /* So that we can see our own changes. */
-  if (MVCC::is_view_active(trx->read_view)) {
-    MVCC::set_view_creator_trx_id(trx->read_view, trx->id);
+  if (trx->read_view.is_open()) {
+    trx->read_view.set_creator_trx_id(trx->id);
   }
-
-  UT_LIST_ADD_FIRST(trx_sys->rw_trx_list, trx);
-
-  ut_d(trx->in_rw_trx_list = true);
-
-  mutex_exit(&trx_sys->mutex);
 }
 
 void trx_kill_blocking(trx_t *trx) {
diff --git a/storage/innobase/trx/trx0undo.cc b/storage/innobase/trx/trx0undo.cc
index 65fe702f331..47569a51028 100644
--- a/storage/innobase/trx/trx0undo.cc
+++ b/storage/innobase/trx/trx0undo.cc
@@ -1881,13 +1881,22 @@ void trx_undo_insert_cleanup(trx_undo_ptr_t *undo_ptr, bool noredo) {
   rseg->unlatch();
 }
 
-/** At shutdown, frees the undo logs of a PREPARED transaction. */
-void trx_undo_free_prepared(trx_t *trx) /*!< in/out: PREPARED transaction */
-{
-  ut_ad(srv_shutdown_state.load() == SRV_SHUTDOWN_EXIT_THREADS);
+/** At shutdown, frees the undo logs of a transaction. */
+void trx_undo_free_at_shutdown(trx_t *trx) {
+  if (trx->rsegs.m_redo.update_undo != nullptr) {
+    switch (trx->rsegs.m_redo.update_undo->state) {
+      case TRX_UNDO_PREPARED:
+        break;
+      case TRX_UNDO_CACHED:
+      case TRX_UNDO_TO_FREE:
+      case TRX_UNDO_TO_PURGE:
+      case TRX_UNDO_ACTIVE:
+        ut_ad(trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY));
+        break;
+      default:
+        ut_error;
+    }
 
-  if (trx->rsegs.m_redo.update_undo) {
-    ut_a(trx->rsegs.m_redo.update_undo->state == TRX_UNDO_PREPARED);
     UT_LIST_REMOVE(trx->rsegs.m_redo.rseg->update_undo_list,
                    trx->rsegs.m_redo.update_undo);
     trx_undo_mem_free(trx->rsegs.m_redo.update_undo);
@@ -1895,8 +1904,20 @@ void trx_undo_free_prepared(trx_t *trx) /*!< in/out: PREPARED transaction */
     trx->rsegs.m_redo.update_undo = nullptr;
   }
 
-  if (trx->rsegs.m_redo.insert_undo) {
-    ut_a(trx->rsegs.m_redo.insert_undo->state == TRX_UNDO_PREPARED);
+  if (trx->rsegs.m_redo.insert_undo != nullptr) {
+    switch (trx->rsegs.m_redo.insert_undo->state) {
+      case TRX_UNDO_PREPARED:
+        break;
+      case TRX_UNDO_CACHED:
+      case TRX_UNDO_TO_FREE:
+      case TRX_UNDO_TO_PURGE:
+      case TRX_UNDO_ACTIVE:
+        ut_ad(trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY));
+        break;
+      default:
+        ut_error;
+    }
+
     UT_LIST_REMOVE(trx->rsegs.m_redo.rseg->insert_undo_list,
                    trx->rsegs.m_redo.insert_undo);
     trx_undo_mem_free(trx->rsegs.m_redo.insert_undo);
@@ -1904,7 +1925,7 @@ void trx_undo_free_prepared(trx_t *trx) /*!< in/out: PREPARED transaction */
     trx->rsegs.m_redo.insert_undo = nullptr;
   }
 
-  if (trx->rsegs.m_noredo.update_undo) {
+  if (trx->rsegs.m_noredo.update_undo != nullptr) {
     ut_a(trx->rsegs.m_noredo.update_undo->state == TRX_UNDO_PREPARED);
 
     UT_LIST_REMOVE(trx->rsegs.m_noredo.rseg->update_undo_list,
@@ -1913,7 +1934,7 @@ void trx_undo_free_prepared(trx_t *trx) /*!< in/out: PREPARED transaction */
 
     trx->rsegs.m_noredo.update_undo = nullptr;
   }
-  if (trx->rsegs.m_noredo.insert_undo) {
+  if (trx->rsegs.m_noredo.insert_undo != nullptr) {
     ut_a(trx->rsegs.m_noredo.insert_undo->state == TRX_UNDO_PREPARED);
 
     UT_LIST_REMOVE(trx->rsegs.m_noredo.rseg->insert_undo_list,
diff --git a/storage/innobase/ut/ut0new.cc b/storage/innobase/ut/ut0new.cc
index 46ce424c273..d1b2a1d8de3 100644
--- a/storage/innobase/ut/ut0new.cc
+++ b/storage/innobase/ut/ut0new.cc
@@ -54,7 +54,6 @@ PSI_memory_key mem_key_partitioning;
 PSI_memory_key mem_key_row_log_buf;
 PSI_memory_key mem_key_row_merge_sort;
 PSI_memory_key mem_key_std;
-PSI_memory_key mem_key_trx_sys_t_rw_trx_ids;
 PSI_memory_key mem_key_undo_spaces;
 PSI_memory_key mem_key_ut_lock_free_hash_t;
 /* Please obey alphabetical order in the definitions above. */
@@ -94,8 +93,6 @@ static PSI_memory_info pfs_info[] = {
     {&mem_key_row_log_buf, "row_log_buf", 0, 0, PSI_DOCUMENT_ME},
     {&mem_key_row_merge_sort, "row_merge_sort", 0, 0, PSI_DOCUMENT_ME},
     {&mem_key_std, "std", 0, 0, PSI_DOCUMENT_ME},
-    {&mem_key_trx_sys_t_rw_trx_ids, "trx_sys_t::rw_trx_ids", 0, 0,
-     PSI_DOCUMENT_ME},
     {&mem_key_undo_spaces, "undo::Tablespaces", 0, 0, PSI_DOCUMENT_ME},
     {&mem_key_ut_lock_free_hash_t, "ut_lock_free_hash_t", 0, 0,
      PSI_DOCUMENT_ME},
diff --git a/unittest/gunit/mysys_lf-t.cc b/unittest/gunit/mysys_lf-t.cc
index 8429bb5e35a..14dce2a4b08 100644
--- a/unittest/gunit/mysys_lf-t.cc
+++ b/unittest/gunit/mysys_lf-t.cc
@@ -122,10 +122,14 @@ extern "C" void *test_lf_alloc(void *arg) {
   return nullptr;
 }
 
+bool do_sum(void *num, void *acc) {
+  *static_cast<int *>(acc) += *static_cast<int *>(num);
+  return false;
+}
 const int N_TLH = 1000;
 extern "C" void *test_lf_hash(void *arg) {
   int m = (*(int *)arg) / (2 * N_TLH);
-  int32 x, y, z, sum = 0, ins = 0;
+  int32 x, y, z, sum = 0, ins = 0, scans = 0;
   LF_PINS *pins;
 
   if (with_my_thread_init) my_thread_init();
@@ -142,6 +146,11 @@ extern "C" void *test_lf_hash(void *arg) {
         sum += z;
         ins++;
       }
+      else {
+        int unused = 0;
+        lf_hash_iterate(&lf_hash, pins, do_sum, &unused);
+        scans++;
+      }
     }
     for (i = 0; i < N_TLH; i++) {
       y = (y * (m + i) + 0x87654321) & INT_MAX32;
-- 
2.28.0.windows.1


From 4ca90fdc56ddb1dc4e195700fab69f9d61706448 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Thu, 19 Nov 2020 17:53:30 +0800
Subject: [PATCH 04/19] Fix trx sys mtr.

---
 .../constraint_check_locks_in_read_committed.result  |  2 --
 .../r/innodb-lock-inherit-read_commited.result       | 12 ------------
 .../suite/innodb/r/innodb_i_s_innodb_trx.result      |  2 +-
 mysql-test/suite/innodb/t/innodb_i_s_innodb_trx.test |  1 +
 4 files changed, 2 insertions(+), 15 deletions(-)

diff --git a/mysql-test/suite/innodb/r/constraint_check_locks_in_read_committed.result b/mysql-test/suite/innodb/r/constraint_check_locks_in_read_committed.result
index 0f52df8af94..f659fa67eed 100644
--- a/mysql-test/suite/innodb/r/constraint_check_locks_in_read_committed.result
+++ b/mysql-test/suite/innodb/r/constraint_check_locks_in_read_committed.result
@@ -202,8 +202,6 @@ WHERE object_schema='test' AND object_name = 't1';
 index_name	lock_type	lock_mode	lock_status	lock_data
 NULL	TABLE	IX	GRANTED	NULL
 PRIMARY	RECORD	X,REC_NOT_GAP	GRANTED	0
-a	RECORD	X,REC_NOT_GAP	GRANTED	0, 0
-a	RECORD	X,GAP	GRANTED	0, 0
 a	RECORD	X	GRANTED	1, 1
 COMMIT;
 SELECT * FROM t1;
diff --git a/mysql-test/suite/innodb/r/innodb-lock-inherit-read_commited.result b/mysql-test/suite/innodb/r/innodb-lock-inherit-read_commited.result
index 1b1aad24e7f..e8df63b4578 100644
--- a/mysql-test/suite/innodb/r/innodb-lock-inherit-read_commited.result
+++ b/mysql-test/suite/innodb/r/innodb-lock-inherit-read_commited.result
@@ -189,9 +189,6 @@ con1	a	RECORD	X	GRANTED	1, 1
 con1	a	RECORD	X	GRANTED	2, 2
 con1	a	RECORD	X	GRANTED	supremum pseudo-record
 con1	a	RECORD	X,GAP	GRANTED	2, 2
-con1	a	RECORD	X,GAP	GRANTED	7, 6
-con1	a	RECORD	X,GAP	GRANTED	7, 7
-con1	a	RECORD	X,REC_NOT_GAP	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	supremum pseudo-record
 con1	c	RECORD	X,GAP	GRANTED	7, 6
@@ -212,9 +209,6 @@ con1	a	RECORD	X	GRANTED	1, 1
 con1	a	RECORD	X	GRANTED	2, 2
 con1	a	RECORD	X	GRANTED	supremum pseudo-record
 con1	a	RECORD	X,GAP	GRANTED	2, 2
-con1	a	RECORD	X,GAP	GRANTED	7, 6
-con1	a	RECORD	X,GAP	GRANTED	7, 7
-con1	a	RECORD	X,REC_NOT_GAP	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	supremum pseudo-record
 con1	c	RECORD	X,GAP	GRANTED	7, 6
@@ -239,9 +233,6 @@ con	index_name	lock_type	lock_mode	lock_status	lock_data
 con1	NULL	TABLE	IX	GRANTED	NULL
 con1	a	RECORD	X	GRANTED	supremum pseudo-record
 con1	a	RECORD	X,GAP	GRANTED	5, 5
-con1	a	RECORD	X,GAP	GRANTED	7, 6
-con1	a	RECORD	X,GAP	GRANTED	7, 7
-con1	a	RECORD	X,REC_NOT_GAP	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	supremum pseudo-record
 con1	c	RECORD	X,GAP	GRANTED	7, 6
@@ -258,9 +249,6 @@ con	index_name	lock_type	lock_mode	lock_status	lock_data
 con1	NULL	TABLE	IX	GRANTED	NULL
 con1	a	RECORD	X	GRANTED	supremum pseudo-record
 con1	a	RECORD	X,GAP	GRANTED	5, 5
-con1	a	RECORD	X,GAP	GRANTED	7, 6
-con1	a	RECORD	X,GAP	GRANTED	7, 7
-con1	a	RECORD	X,REC_NOT_GAP	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	7, 7
 con1	c	RECORD	X	GRANTED	supremum pseudo-record
 con1	c	RECORD	X,GAP	GRANTED	7, 6
diff --git a/mysql-test/suite/innodb/r/innodb_i_s_innodb_trx.result b/mysql-test/suite/innodb/r/innodb_i_s_innodb_trx.result
index 3400dd6da92..513c9198382 100644
--- a/mysql-test/suite/innodb/r/innodb_i_s_innodb_trx.result
+++ b/mysql-test/suite/innodb/r/innodb_i_s_innodb_trx.result
@@ -56,7 +56,7 @@ trx_rows_locked, trx_rows_modified, trx_concurrency_tickets,
 trx_isolation_level, trx_unique_checks, trx_foreign_key_checks
 FROM INFORMATION_SCHEMA.INNODB_TRX;
 trx_state	trx_weight	trx_tables_in_use	trx_tables_locked	trx_rows_locked	trx_rows_modified	trx_concurrency_tickets	trx_isolation_level	trx_unique_checks	trx_foreign_key_checks
-RUNNING	5	0	1	7	1	0	REPEATABLE READ	1	1
+RUNNING	3	0	1	5	1	0	REPEATABLE READ	1	1
 ROLLBACK;
 SET FOREIGN_KEY_CHECKS = 0;
 SET UNIQUE_CHECKS = 0;
diff --git a/mysql-test/suite/innodb/t/innodb_i_s_innodb_trx.test b/mysql-test/suite/innodb/t/innodb_i_s_innodb_trx.test
index 9707f270bf0..42574e432e6 100644
--- a/mysql-test/suite/innodb/t/innodb_i_s_innodb_trx.test
+++ b/mysql-test/suite/innodb/t/innodb_i_s_innodb_trx.test
@@ -6,6 +6,7 @@
 
 # Make sure the locks keep waiting until they are released,
 # even on a busy system.
+
 SET GLOBAL innodb_lock_wait_timeout=600;
 
 DESCRIBE INFORMATION_SCHEMA.INNODB_TRX;
-- 
2.28.0.windows.1


From fd6f49d73e6533ad0e6c669cff4cff8e0660d532 Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Sat, 21 Nov 2020 16:07:17 +0800
Subject: [PATCH 05/19] trx->serialised should be initialized.

---
 storage/innobase/trx/trx0trx.cc | 1 +
 1 file changed, 1 insertion(+)

diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index 2faec37a492..e2d769424be 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -1244,6 +1244,7 @@ static void trx_start_low(
   read_view_open_now: */
 
   trx->no = TRX_ID_MAX;
+  trx->serialised = false;
 
   ut_a(ib_vector_is_empty(trx->lock.autoinc_locks));
   ut_a(trx->lock.table_locks.empty());
-- 
2.28.0.windows.1


From e9b7dc5fd4f6c6cdd6e906e6f46611ef17571b29 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Sat, 21 Nov 2020 15:19:40 +0800
Subject: [PATCH 06/19] fix codedex warning "INTEGER_OVERFLOW".

---
 storage/innobase/include/sync0sharded_rw.h | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/storage/innobase/include/sync0sharded_rw.h b/storage/innobase/include/sync0sharded_rw.h
index 0431fc7ed74..659462ce61d 100644
--- a/storage/innobase/include/sync0sharded_rw.h
+++ b/storage/innobase/include/sync0sharded_rw.h
@@ -107,8 +107,8 @@ class Sharded_rw_lock {
   bool try_x_lock() {
     for (size_t shard_no = 0; shard_no < m_n_shards; ++shard_no) {
       if (!rw_lock_x_lock_nowait(&m_shards[shard_no])) {
-        while (0 < shard_no--) {
-          rw_lock_x_unlock(&m_shards[shard_no]);
+        while (0 < shard_no) {
+          rw_lock_x_unlock(&m_shards[--shard_no]);
         }
         return (false);
       }
-- 
2.28.0.windows.1


From 56e6e743beb23a900ab7534d6d71bec1d14f67df Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Mon, 23 Nov 2020 10:19:10 +0800
Subject: [PATCH 07/19] fix codedex warning "FORWARD_NULL".

---
 mysys/lf_hash.cc | 118 +++++++++++++++++++++++++++++++----------------
 1 file changed, 79 insertions(+), 39 deletions(-)

diff --git a/mysys/lf_hash.cc b/mysys/lf_hash.cc
index 2a4b709cb29..daa56981314 100644
--- a/mysys/lf_hash.cc
+++ b/mysys/lf_hash.cc
@@ -102,46 +102,35 @@ static inline T *SET_DELETED(T *ptr) {
 }
 
 /**
-  Walk the list, searching for an element or invoking a callback.
+  Walk the list, searching for an element.
 
   Search for hashnr/key/keylen in the list starting from 'head' and position the
   cursor. The list is ORDER by hashnr, key
 
   @param head         start walking the list from this node
-  @param cs           charset for comparing keys, nullptr if callback is used
+  @param cs           charset for comparing keys
   @param hashnr       hash number to searching for
-  @param key          key to search for OR data for the callback
-  @param keylen       length of the key to compare, 0 if callback is used
+  @param key          key to search for
+  @param keylen       length of the key to compare
   @param cursor       for returning the found element
   @param pins         see lf_alloc-pin.cc
-  @param callback     callback action, invoked for every element
 
   @note
     cursor is positioned in either case
     pins[0..2] are used, they are not removed on return
-    callback might see some elements twice (because of retries)
 
   @return
-    if find: 0 - not found
-             1 - found
-    if callback:
-             0 - ok
-             1 - error (callback returned true)
+    0 - not found
+    1 - found
 */
 static int my_lfind(std::atomic<LF_SLIST *> *head, CHARSET_INFO *cs,
                     uint32 hashnr, const uchar *key, size_t keylen,
-                    CURSOR *cursor, LF_PINS *pins,
-                    lf_hash_walk_func *callback) {
+                    CURSOR *cursor, LF_PINS *pins) {
   uint32 cur_hashnr;
   const uchar *cur_key;
   size_t cur_keylen;
   LF_SLIST *link;
 
-  /* should not be set both */
-  DBUG_ASSERT((cs == nullptr) || (callback == nullptr));
-  /* should not be set both */
-  DBUG_ASSERT((keylen == 0) || (callback == nullptr));
-
 retry:
   cursor->prev = head;
   do /* PTR() isn't necessary below, head is a dummy node */
@@ -153,9 +142,11 @@ retry:
     if (unlikely(!cursor->curr)) {
       return 0; /* end of the list */
     }
+
     cur_hashnr = cursor->curr->hashnr;
     cur_keylen = cursor->curr->keylen;
     cur_key = cursor->curr->key;
+
     do {
       link = cursor->curr->link;
       cursor->next = PTR(link);
@@ -163,13 +154,7 @@ retry:
     } while (link != cursor->curr->link && LF_BACKOFF);
 
     if (!DELETED(link)) {
-      if (likely(callback != nullptr)) {
-        if ((cur_hashnr & 1) > 0 &&
-            callback(cursor->curr + 1,
-                     const_cast<void *>(static_cast<const void *>(key)))) {
-          return 1;
-        }
-      } else if (cur_hashnr >= hashnr) {
+      if (cur_hashnr >= hashnr) {
         int r = 1;
         if (cur_hashnr > hashnr ||
             (r = my_strnncoll(cs, cur_key, cur_keylen, key, keylen)) >= 0) {
@@ -198,23 +183,78 @@ retry:
   }
 }
 
-/*
-  DESCRIPTION
-    Search for hashnr/key/keylen in the list starting from 'head' and
-    position the cursor. The list is ORDER BY hashnr, key
+/**
+  Walk the list, invoking a callback.
 
-  RETURN
-    0 - not found
-    1 - found
+  @param head         start walking the list from this node
+  @param key          data for the callback
+  @param cursor       for returning the found element
+  @param pins         see lf_alloc-pin.cc
+  @param callback     callback action, invoked for every element
 
-  NOTE
+  @note
     cursor is positioned in either case
-    pins[0..2] are used, they are NOT removed on return
+    pins[0..2] are used, they are not removed on return
+    callback might see some elements twice (because of retries)
+
+  @return
+    0 - ok
+    1 - error (callback returned true)
 */
-static int my_lfind(std::atomic<LF_SLIST *> *head, CHARSET_INFO *cs,
-                    uint32 hashnr, const uchar *key, size_t keylen,
-                    CURSOR *cursor, LF_PINS *pins) {
-  return my_lfind(head, cs, hashnr, key, keylen, cursor, pins, nullptr);
+static int my_lfind(std::atomic<LF_SLIST *> *head, const uchar *key,
+                    CURSOR *cursor, LF_PINS *pins,
+                    lf_hash_walk_func *callback) {
+  uint32 cur_hashnr;
+  LF_SLIST *link;
+
+  DBUG_ASSERT(callback != nullptr);
+
+retry:
+  cursor->prev = head;
+  do /* PTR() isn't necessary below, head is a dummy node */
+  {
+    cursor->curr = (LF_SLIST *)(*cursor->prev);
+    lf_pin(pins, 1, cursor->curr);
+  } while (*cursor->prev != cursor->curr && LF_BACKOFF);
+  for (;;) {
+    if (unlikely(!cursor->curr)) {
+      return 0; /* end of the list */
+    }
+
+    cur_hashnr = cursor->curr->hashnr;
+
+    do {
+      link = cursor->curr->link;
+      cursor->next = PTR(link);
+      lf_pin(pins, 0, cursor->next);
+    } while (link != cursor->curr->link && LF_BACKOFF);
+
+    if (!DELETED(link)) {
+      if ((cur_hashnr & 1) > 0 &&
+          callback(cursor->curr + 1,
+                    const_cast<void *>(static_cast<const void *>(key)))) {
+        return 1;
+      }
+      cursor->prev = &(cursor->curr->link);
+      if (!(cur_hashnr & 1)) /* dummy node */
+        head = cursor->prev;
+      lf_pin(pins, 2, cursor->curr);
+    } else {
+      /*
+        we found a deleted node - be nice, help the other thread
+        and remove this deleted node
+      */
+      if (atomic_compare_exchange_strong(cursor->prev, &cursor->curr,
+                                         cursor->next)) {
+        lf_pinbox_free(pins, cursor->curr);
+      } else {
+        (void)LF_BACKOFF;
+        goto retry;
+      }
+    }
+    cursor->curr = cursor->next;
+    lf_pin(pins, 1, cursor->curr);
+  }
 }
 
 /**
@@ -641,7 +681,7 @@ int lf_hash_iterate(LF_HASH *hash, LF_PINS *pins, const lf_hash_walk_func *callb
     return 0;
   }
 
-  res = my_lfind(el, nullptr, 0, static_cast<const uchar *>(argument), 0, &cursor,
+  res = my_lfind(el, static_cast<const uchar *>(argument), &cursor,
                  pins, callback);
 
   lf_unpin(pins, 2);
-- 
2.28.0.windows.1


From b3ea552fea91bf686df99b0c61fdc527980835a3 Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Fri, 13 Nov 2020 15:16:43 +0800
Subject: [PATCH 08/19] Use AArch64 word_add_fetch in rw_lock_lock_word_incr

---
 storage/innobase/CMakeLists.txt                |  6 ++++++
 storage/innobase/include/lock0aarch64_atomic.h |  8 ++++++++
 storage/innobase/include/sync0rw.ic            |  3 ++-
 storage/innobase/lock/lock0aarch64_atomic.cc   | 12 ++++++++++++
 4 files changed, 28 insertions(+), 1 deletion(-)
 create mode 100644 storage/innobase/include/lock0aarch64_atomic.h
 create mode 100644 storage/innobase/lock/lock0aarch64_atomic.cc

diff --git a/storage/innobase/CMakeLists.txt b/storage/innobase/CMakeLists.txt
index 80bebd0acfd..91d6ba15af7 100644
--- a/storage/innobase/CMakeLists.txt
+++ b/storage/innobase/CMakeLists.txt
@@ -138,6 +138,7 @@ SET(INNOBASE_SOURCES
   lock/lock0guards.cc
   lock/lock0iter.cc
   lock/lock0prdt.cc
+  lock/lock0aarch64_atomic.cc
   lock/lock0latches.cc
   lock/lock0lock.cc
   lock/lock0wait.cc
@@ -264,3 +265,8 @@ IF(HAS_WARN_FLAG)
   ADD_COMPILE_FLAGS(fts/fts0pars.cc
     COMPILE_FLAGS "${HAS_WARN_FLAG}")
 ENDIF()
+
+ADD_COMPILE_FLAGS(
+  lock/lock0aarch64_atomic.cc
+  COMPILE_FLAGS "-march=armv8-a+lse"
+)
diff --git a/storage/innobase/include/lock0aarch64_atomic.h b/storage/innobase/include/lock0aarch64_atomic.h
new file mode 100644
index 00000000000..772a169355d
--- /dev/null
+++ b/storage/innobase/include/lock0aarch64_atomic.h
@@ -0,0 +1,8 @@
+#ifndef lock0aarch64_atomic_h
+#define lock0aarch64_atomic_h
+
+#include "univ.i"
+
+lint word_add_fetch(volatile lint *word, ulint amount);
+
+#endif /* lock0aarch64_atomic_h */
\ No newline at end of file
diff --git a/storage/innobase/include/sync0rw.ic b/storage/innobase/include/sync0rw.ic
index 2380841f26a..2af0993e0ea 100644
--- a/storage/innobase/include/sync0rw.ic
+++ b/storage/innobase/include/sync0rw.ic
@@ -38,6 +38,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
  *******************************************************/
 
 #include "os0event.h"
+#include "lock0aarch64_atomic.h"
 
 /** Lock an rw-lock in shared mode for the current thread. If the rw-lock is
  locked in exclusive mode, or there is an exclusive lock request waiting,
@@ -259,7 +260,7 @@ lint rw_lock_lock_word_incr(rw_lock_t *lock, /*!< in/out: rw-lock */
                             ulint amount)    /*!< in: amount of increment */
 {
 #ifdef INNODB_RW_LOCKS_USE_ATOMICS
-  return (os_atomic_increment_lint(&lock->lock_word, amount));
+  return word_add_fetch(&lock->lock_word, amount);
 #else  /* INNODB_RW_LOCKS_USE_ATOMICS */
   lint local_lock_word;
 
diff --git a/storage/innobase/lock/lock0aarch64_atomic.cc b/storage/innobase/lock/lock0aarch64_atomic.cc
new file mode 100644
index 00000000000..4b70694b2d1
--- /dev/null
+++ b/storage/innobase/lock/lock0aarch64_atomic.cc
@@ -0,0 +1,12 @@
+#include "lock0aarch64_atomic.h"
+
+lint word_add_fetch(volatile lint *word, ulint amount) {
+  asm volatile (
+    "ldaddal %0, x3, [%1]\n\t"
+    "add %0, x3, %0"
+      :"+r"(amount)
+      :"r"(word)
+      :"x3","memory"
+  );
+  return amount;
+}
\ No newline at end of file
-- 
2.28.0.windows.1


From e079f63a410631139bc559bf8f6d675738797062 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Tue, 24 Nov 2020 14:26:54 +0800
Subject: [PATCH 09/19] fix pclintplus

---
 storage/innobase/include/lock0guards.h         |  1 -
 storage/innobase/include/lock0latches.h        |  2 +-
 storage/innobase/include/lock0lock.h           |  2 --
 storage/innobase/include/rem0rec.h             |  2 +-
 storage/innobase/include/sync0sharded_rw.h     |  4 ++--
 storage/innobase/include/trx0sys.h             |  2 +-
 storage/innobase/include/ut0class_life_cycle.h |  2 --
 storage/innobase/include/ut0cpu_cache.h        |  1 -
 storage/innobase/lock/lock0guards.cc           |  2 --
 storage/innobase/lock/lock0latches.cc          |  2 --
 storage/innobase/lock/lock0lock.cc             | 12 +++++-------
 storage/innobase/read/read0read.cc             |  2 +-
 storage/innobase/row/row0mysql.cc              |  2 +-
 storage/innobase/row/row0vers.cc               |  2 +-
 storage/innobase/trx/trx0roll.cc               |  2 +-
 storage/innobase/trx/trx0sys.cc                |  6 ++----
 storage/innobase/trx/trx0trx.cc                |  8 ++++----
 17 files changed, 20 insertions(+), 34 deletions(-)

diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
index 8b50d0efad1..15ee99be846 100644
--- a/storage/innobase/include/lock0guards.h
+++ b/storage/innobase/include/lock0guards.h
@@ -141,7 +141,6 @@ class Shard_latches_guard {
   /** The ordering on shard mutexes used to avoid deadlocks */
   static constexpr std::less<Lock_mutex *> MUTEX_ORDER{};
 };
-
 }  // namespace locksys
 
 #endif /* lock0guards_h */
diff --git a/storage/innobase/include/lock0latches.h b/storage/innobase/include/lock0latches.h
index cc28380f98a..5b3733e5fca 100644
--- a/storage/innobase/include/lock0latches.h
+++ b/storage/innobase/include/lock0latches.h
@@ -119,7 +119,7 @@ class Latches {
     static constexpr size_t NOT_IN_USE = std::numeric_limits<size_t>::max();
 
     /** The id of the rw_lock's shard which this thread has s-latched, or
-    NOT_IN_USE if it has not s-latched any*/
+    NOT_IN_USE if it has not s-latched any */
     static thread_local size_t m_shard_id;
 
    public:
diff --git a/storage/innobase/include/lock0lock.h b/storage/innobase/include/lock0lock.h
index ba98ea8c0a6..81cb43ceded 100644
--- a/storage/innobase/include/lock0lock.h
+++ b/storage/innobase/include/lock0lock.h
@@ -1049,7 +1049,6 @@ extern lock_sys_t *lock_sys;
 #include "lock0lock.ic"
 
 namespace locksys {
-
 /* OWNERSHIP TESTS */
 #ifdef UNIV_DEBUG
 
@@ -1090,7 +1089,6 @@ on whole lock_sys is held) by current thread
 bool owns_lock_shard(const lock_t *lock);
 
 #endif /* UNIV_DEBUG */
-
 }  // namespace locksys
 
 #include "lock0guards.h"
diff --git a/storage/innobase/include/rem0rec.h b/storage/innobase/include/rem0rec.h
index bccba812ba3..fe57edb69f4 100644
--- a/storage/innobase/include/rem0rec.h
+++ b/storage/innobase/include/rem0rec.h
@@ -504,7 +504,7 @@ class Rec_offsets : private ut::Non_copyable {
   mem_heap_t *m_heap{nullptr};
 
   /** Buffer with size large enough to handle common cases without having to use
-  heap. This is the initial value of m_offsets.*/
+  heap. This is the initial value of m_offsets. */
   ulint m_preallocated_buffer[REC_OFFS_NORMAL_SIZE];
 
   /* Initially points to m_preallocated_buffer (which is uninitialized memory).
diff --git a/storage/innobase/include/sync0sharded_rw.h b/storage/innobase/include/sync0sharded_rw.h
index 659462ce61d..2b03132aaaf 100644
--- a/storage/innobase/include/sync0sharded_rw.h
+++ b/storage/innobase/include/sync0sharded_rw.h
@@ -102,12 +102,12 @@ class Sharded_rw_lock {
   /**
   Tries to obtain exclusive latch - similar to x_lock(), but non-blocking, and
   thus can fail.
-  @return true iff succeeded to acquire the exclusive latch
+  @return true if succeeded to acquire the exclusive latch
   */
   bool try_x_lock() {
     for (size_t shard_no = 0; shard_no < m_n_shards; ++shard_no) {
       if (!rw_lock_x_lock_nowait(&m_shards[shard_no])) {
-        while (0 < shard_no) {
+        while (shard_no > 0) {
           rw_lock_x_unlock(&m_shards[--shard_no]);
         }
         return (false);
diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index 6bc643b4950..4f4bc51da8a 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -768,7 +768,7 @@ struct trx_sys_t {
   No need to call ReadView::close(). The caller owns the view that is passed in.
   This function is called by purge thread to determine whether it should purge
   the delete marked record or not. */
-  void clone_oldest_view(ReadView *view = NULL);
+  void clone_oldest_view(ReadView *view = nullptr);
 
   /** @return the number of active views. */
   size_t view_count() const {
diff --git a/storage/innobase/include/ut0class_life_cycle.h b/storage/innobase/include/ut0class_life_cycle.h
index c1d760977f0..ae6ad02dfaa 100644
--- a/storage/innobase/include/ut0class_life_cycle.h
+++ b/storage/innobase/include/ut0class_life_cycle.h
@@ -31,7 +31,6 @@ Utilities related to class lifecycle. */
 #define ut0class_life_cycle_h
 
 namespace ut {
-
 /**
 A utility class which, if inherited from, prevents the descendant class
 from being copied, moved, or assigned.
@@ -46,7 +45,6 @@ class Non_copyable {
   Non_copyable() = default;
   ~Non_copyable() = default;  /// Protected non-virtual destructor
 };
-
 } /* namespace ut */
 
 #endif /* ut0class_life_cycle_h */
\ No newline at end of file
diff --git a/storage/innobase/include/ut0cpu_cache.h b/storage/innobase/include/ut0cpu_cache.h
index 6e5bd19a05f..09d4d3a4f42 100644
--- a/storage/innobase/include/ut0cpu_cache.h
+++ b/storage/innobase/include/ut0cpu_cache.h
@@ -32,7 +32,6 @@ Utilities related to CPU cache. */
 
 #include "ut0ut.h"
 namespace ut {
-
 /** CPU cache line size */
 #if (defined(__powerpc__)) || (defined(__aarch64__))
 constexpr size_t INNODB_CACHE_LINE_SIZE = 128;
diff --git a/storage/innobase/lock/lock0guards.cc b/storage/innobase/lock/lock0guards.cc
index 9b0db062c00..3aa41fd3cc9 100644
--- a/storage/innobase/lock/lock0guards.cc
+++ b/storage/innobase/lock/lock0guards.cc
@@ -31,7 +31,6 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "sync0rw.h"
 
 namespace locksys {
-
 /* Global_exclusive_latch_guard */
 
 Global_exclusive_latch_guard::Global_exclusive_latch_guard() {
@@ -110,5 +109,4 @@ Shard_latches_guard::~Shard_latches_guard() {
     mutex_exit(&m_shard_mutex_1);
   }
 }
-
 }  // namespace locksys
diff --git a/storage/innobase/lock/lock0latches.cc b/storage/innobase/lock/lock0latches.cc
index f2a09ba2317..83c03288cd7 100644
--- a/storage/innobase/lock/lock0latches.cc
+++ b/storage/innobase/lock/lock0latches.cc
@@ -31,7 +31,6 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "lock0priv.h"
 
 namespace locksys {
-
 size_t Latches::Page_shards::get_shard(const page_id_t &page_id) {
   /* We always use lock_rec_hash regardless of the exact type of the lock.
   It may happen that the lock is a predicate lock, in which case,
@@ -103,5 +102,4 @@ Latches::Table_shards::~Table_shards() {
     mutex_destroy(mutexes + i);
   }
 }
-
 }  // namespace locksys
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index 948ed914fdb..b52412a55ea 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -1830,9 +1830,7 @@ void lock_make_trx_hit_list(trx_t *hp_trx, hit_list_t &hit_list) {
   check if this transaction is waiting for a lock at all. It's unsafe to read
   hp->lock.wait_lock without latching whole lock_sys as it might temporarily
   change to NULL during a concurrent B-tree reorganization, even though the
-  trx actually is still waiting.
-  TBD: Is it safe to use hp_trx->lock.que_state == TRX_QUE_LOCK_WAIT given that
-  que_state is not atomic, and writes to it happen without trx->mutex ? */
+  trx actually is still waiting. */
   const bool is_waiting = (hp_trx->lock.blocking_trx.load() != nullptr);
   trx_mutex_exit(hp_trx);
   if (!is_waiting) {
@@ -4055,7 +4053,7 @@ merged or split, or due to implicit-to-explicit conversion).
 It is called during XA prepare to release locks early.
 @param[in,out]	trx		transaction
 @param[in]	only_gap	release only GAP locks
-@return true if and only if it succeeded to do the job*/
+@return true if and only if it succeeded to do the job */
 static bool try_release_read_locks_in_s_mode(trx_t *trx, bool only_gap) {
   /* In order to access trx->lock.trx_locks safely we need to hold trx->mutex.
   So, conceptually we'd love to hold trx->mutex while iterating through
@@ -4093,7 +4091,7 @@ static bool try_release_read_locks_in_s_mode(trx_t *trx, bool only_gap) {
     if (lock_get_type_low(lock) == LOCK_REC) {
       /* Following call temporarily releases trx->mutex */
       if (!try_relatch_trx_and_shard_and_do(
-              lock, [=]() { lock_release_read_lock(lock, only_gap); })) {
+          lock, [=]() { lock_release_read_lock(lock, only_gap); })) {
         /* Someone has modified the list while we were re-acquiring the latches
         so we need to start over again. */
         return false;
@@ -4698,7 +4696,7 @@ class TrxLockIterator {
 @param[in]	trx	transaction */
 void lock_trx_print_wait_and_mvcc_state(FILE *file, const trx_t *trx) {
   /* We require exclusive lock_sys access so that trx->lock.wait_lock is
-  not being modified, and to access trx->lock.wait_started without trx->mutex.*/
+  not being modified, and to access trx->lock.wait_started without trx->mutex. */
   ut_ad(locksys::owns_exclusive_global_latch());
   fprintf(file, "---");
 
@@ -5062,7 +5060,7 @@ function_exit:
 }
 
 static bool lock_validate_table_locks_callback(rw_trx_hash_element_t *element,
-                                               void * /*unused*/) {
+                                               void * /* unused */) {
   mutex_enter(&element->mutex);
   trx_t *trx = element->trx;
   if (trx != nullptr) {
diff --git a/storage/innobase/read/read0read.cc b/storage/innobase/read/read0read.cc
index d5539bd0682..5b5c1740235 100644
--- a/storage/innobase/read/read0read.cc
+++ b/storage/innobase/read/read0read.cc
@@ -252,7 +252,7 @@ in time are seen in the view.
 
 View becomes visible to purge thread.
 
-@param[in,out] trx transaction*/
+@param[in,out] trx transaction */
 void ReadView::open(trx_t *trx) {
   ut_ad(this == &trx->read_view);
   switch (m_state.load(std::memory_order_relaxed)) {
diff --git a/storage/innobase/row/row0mysql.cc b/storage/innobase/row/row0mysql.cc
index acec70828e1..b3f7a4b2b3f 100644
--- a/storage/innobase/row/row0mysql.cc
+++ b/storage/innobase/row/row0mysql.cc
@@ -1156,7 +1156,7 @@ dberr_t row_lock_table_autoinc_for_mysql(
   Note: We peek at the value of the current owner without acquiring any latch,
   which is OK, because if the equality holds, it means we were granted the lock,
   and the only way table->autoinc_trx can subsequently change is by releasing
-  the lock, which can not happen concurrently with the thread running the trx.*/
+  the lock, which can not happen concurrently with the thread running the trx. */
   ut_ad(trx_can_be_handled_by_current_thread(trx));
   if (trx == table->autoinc_trx) {
     return (DB_SUCCESS);
diff --git a/storage/innobase/row/row0vers.cc b/storage/innobase/row/row0vers.cc
index 6e59b596299..42df7a1105c 100644
--- a/storage/innobase/row/row0vers.cc
+++ b/storage/innobase/row/row0vers.cc
@@ -295,7 +295,7 @@ static bool row_vers_find_matching(
  @return 0 if committed, else the active transaction id;
  NOTE that this function can return false positives but never false
  negatives. The caller must confirm all positive results by calling checking if
- the trx is still active.*/
+ the trx is still active. */
 UNIV_INLINE
 trx_t *row_vers_impl_x_locked_low(trx_t *caller_trx,
                                   const rec_t *const clust_rec,
diff --git a/storage/innobase/trx/trx0roll.cc b/storage/innobase/trx/trx0roll.cc
index 4c9479ece09..23eb719fc77 100644
--- a/storage/innobase/trx/trx0roll.cc
+++ b/storage/innobase/trx/trx0roll.cc
@@ -655,7 +655,7 @@ void trx_rollback_recovered(bool all) {
 
   /* Collect list of recovered ACTIVE transaction ids first. Once collected, no
   other thread is allowed to modify or remove these transactions from
-  rw_trx_hash.*/
+  rw_trx_hash. */
   trx_sys->rw_trx_hash.iterate_no_dups(
       reinterpret_cast<lf_hash_walk_func *>(trx_rollback_recovered_callback),
       &trx_list);
diff --git a/storage/innobase/trx/trx0sys.cc b/storage/innobase/trx/trx0sys.cc
index bba8e158cd0..7c4012ea346 100644
--- a/storage/innobase/trx/trx0sys.cc
+++ b/storage/innobase/trx/trx0sys.cc
@@ -873,7 +873,7 @@ No need to call ReadView::close(). The caller owns the view that is passed in.
 This function is called by purge thread to determine whether it should purge the
 delete marked record or not. */
 void trx_sys_t::clone_oldest_view(ReadView *view) {
-  if (view == NULL) {
+  if (view == nullptr) {
       purge_sys->view.snapshot(nullptr);
   }
   mutex_enter(&mutex);
@@ -886,7 +886,7 @@ void trx_sys_t::clone_oldest_view(ReadView *view) {
     }
 
     if (state == READ_VIEW_STATE_OPEN) {
-      if (view == NULL) {
+      if (view == nullptr) {
           purge_sys->view.copy(trx->read_view);
       } else {
           view->copy(trx->read_view);
@@ -930,8 +930,6 @@ ulint trx_sys_t::n_prepared_trx() {
 }
 
 trx_id_t trx_sys_t::get_new_trx_id_no_refresh() {
-  /* TODO wcy: why is this function called when doing undo? */
-
   /* VERY important: after the database is started, max_trx_id value is
   divisible by TRX_SYS_TRX_ID_WRITE_MARGIN, and the following if
   will evaluate to TRUE when this function is first time called,
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index e2d769424be..196a43ec379 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -2315,7 +2315,7 @@ state_ok:
 
 void trx_print_latched(FILE *f, const trx_t *trx, ulint max_query_len) {
   /* We need exclusive access to lock_sys for lock_number_of_rows_locked(),
-  and accessing trx->lock fields without trx->mutex.*/
+  and accessing trx->lock fields without trx->mutex. */
   ut_ad(locksys::owns_exclusive_global_latch());
 
   trx_print_low(f, trx, max_query_len, lock_number_of_rows_locked(&trx->lock),
@@ -2471,7 +2471,7 @@ static thread_local int32_t trx_latched_count = 0;
 static thread_local bool trx_allowed_two_latches = false;
 
 void trx_before_mutex_enter(const trx_t *trx, bool first_of_two) {
-  if (0 == trx_latched_count++) {
+  if (trx_latched_count++ == 0) {
     ut_a(trx_first_latched_trx == nullptr);
     trx_first_latched_trx = trx;
     if (first_of_two) {
@@ -2494,8 +2494,8 @@ void trx_before_mutex_enter(const trx_t *trx, bool first_of_two) {
   }
 }
 void trx_before_mutex_exit(const trx_t *trx) {
-  ut_a(0 < trx_latched_count);
-  if (0 == --trx_latched_count) {
+  ut_a(trx_latched_count > 0);
+  if (--trx_latched_count == 0) {
     ut_a(trx_first_latched_trx == trx);
     trx_first_latched_trx = nullptr;
     trx_allowed_two_latches = false;
-- 
2.28.0.windows.1


From 522efca6a0479f5052d88fed4df702b893bba3a7 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Tue, 24 Nov 2020 21:14:34 +0800
Subject: [PATCH 10/19] fix pclintplus

---
 storage/innobase/btr/btr0btr.cc      | 1 +
 storage/innobase/buf/buf0buf.cc      | 1 +
 storage/innobase/gis/gis0sea.cc      | 1 +
 storage/innobase/handler/p_s.cc      | 1 +
 storage/innobase/include/lock0lock.h | 2 --
 storage/innobase/include/trx0sys.h   | 8 ++++----
 storage/innobase/include/trx0trx.h   | 4 +++-
 storage/innobase/lock/lock0lock.cc   | 1 +
 storage/innobase/lock/lock0prdt.cc   | 1 +
 storage/innobase/lock/lock0wait.cc   | 3 ++-
 storage/innobase/row/row0ins.cc      | 1 +
 storage/innobase/srv/srv0srv.cc      | 1 +
 storage/innobase/trx/trx0i_s.cc      | 1 +
 storage/innobase/trx/trx0trx.cc      | 4 ++--
 14 files changed, 20 insertions(+), 10 deletions(-)

diff --git a/storage/innobase/btr/btr0btr.cc b/storage/innobase/btr/btr0btr.cc
index e343371205e..8c00776f01f 100644
--- a/storage/innobase/btr/btr0btr.cc
+++ b/storage/innobase/btr/btr0btr.cc
@@ -46,6 +46,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "gis0rtree.h"
 #include "ibuf0ibuf.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "my_dbug.h"
 #include "page0page.h"
 #include "page0zip.h"
diff --git a/storage/innobase/buf/buf0buf.cc b/storage/innobase/buf/buf0buf.cc
index bf9c6ac1723..f77742e99af 100644
--- a/storage/innobase/buf/buf0buf.cc
+++ b/storage/innobase/buf/buf0buf.cc
@@ -54,6 +54,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "dict0stats_bg.h"
 #include "ibuf0ibuf.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "log0log.h"
 #include "sync0rw.h"
 #include "trx0purge.h"
diff --git a/storage/innobase/gis/gis0sea.cc b/storage/innobase/gis/gis0sea.cc
index 0871677693a..698b48a6b1b 100644
--- a/storage/innobase/gis/gis0sea.cc
+++ b/storage/innobase/gis/gis0sea.cc
@@ -44,6 +44,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "btr0sea.h"
 #include "ibuf0ibuf.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "rem0cmp.h"
 #include "srv0mon.h"
 #include "sync0sync.h"
diff --git a/storage/innobase/handler/p_s.cc b/storage/innobase/handler/p_s.cc
index d3e8d821d55..d8bce9ced68 100644
--- a/storage/innobase/handler/p_s.cc
+++ b/storage/innobase/handler/p_s.cc
@@ -36,6 +36,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 
 #include "lock0iter.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "sql_table.h"
 #include "table.h"
 #include "trx0i_s.h"
diff --git a/storage/innobase/include/lock0lock.h b/storage/innobase/include/lock0lock.h
index 81cb43ceded..1ccded369a3 100644
--- a/storage/innobase/include/lock0lock.h
+++ b/storage/innobase/include/lock0lock.h
@@ -1091,6 +1091,4 @@ bool owns_lock_shard(const lock_t *lock);
 #endif /* UNIV_DEBUG */
 }  // namespace locksys
 
-#include "lock0guards.h"
-
 #endif
diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index 4f4bc51da8a..bf6b0f02ebf 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -348,7 +348,7 @@ class Space_Ids : public std::vector<space_id_t, ut_allocator<space_id_t>> {
 trx_t *current_trx();
 
 struct rw_trx_hash_element_t {
-  rw_trx_hash_element_t() : trx(nullptr) {
+  rw_trx_hash_element_t() : no(TRX_ID_MAX), trx(nullptr) {
     mutex_create(LATCH_ID_RW_TRX_HASH_ELEMENT, &mutex);
   }
 
@@ -622,7 +622,7 @@ struct trx_sys_t {
 
   @return maximum currently allocated trx id; will be stale after the next call
   to trx_sys.assign_new_trx_no */
-  trx_id_t get_max_trx_id() {
+  trx_id_t get_max_trx_id() const {
     return max_trx_id.load(std::memory_order_relaxed);
   }
 
@@ -805,7 +805,7 @@ struct trx_sys_t {
   }
 
   struct snapshot_ids_arg {
-    snapshot_ids_arg(trx_ids_t *_ids) : ids(_ids), id(0), no(0) {}
+    explicit snapshot_ids_arg(trx_ids_t *_ids) : ids(_ids), id(0), no(0) {}
 
     trx_ids_t *ids;
     trx_id_t id;
@@ -825,7 +825,7 @@ struct trx_sys_t {
   }
 
   /** Get for rw_trx_hash_version, must issue ACQUIRE memory barrier. */
-  trx_id_t get_rw_trx_hash_version() {
+  trx_id_t get_rw_trx_hash_version() const {
     return rw_trx_hash_version.load(std::memory_order_acquire);
   }
 
diff --git a/storage/innobase/include/trx0trx.h b/storage/innobase/include/trx0trx.h
index 9894452d6ec..c41befa550a 100644
--- a/storage/innobase/include/trx0trx.h
+++ b/storage/innobase/include/trx0trx.h
@@ -1203,7 +1203,9 @@ struct trx_t {
 
   bool allow_semi_consistent() const { return (skip_gap_locks()); }
 
-  bool is_referenced() { return (n_ref.load(std::memory_order_relaxed) > 0); }
+  bool is_referenced() const {
+    return (n_ref.load(std::memory_order_relaxed) > 0);
+  }
 
   void reference() {
     int32_t old_n_ref = n_ref.fetch_add(1, std::memory_order_relaxed);
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index b52412a55ea..e12dda5a021 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -48,6 +48,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "ha_prototypes.h"
 #include "lock0lock.h"
 #include "lock0priv.h"
+#include "lock0guards.h"
 #include "pars0pars.h"
 #include "row0mysql.h"
 #include "row0sel.h"
diff --git a/storage/innobase/lock/lock0prdt.cc b/storage/innobase/lock/lock0prdt.cc
index 9d0700c1cbf..5b90b5641f3 100644
--- a/storage/innobase/lock/lock0prdt.cc
+++ b/storage/innobase/lock/lock0prdt.cc
@@ -41,6 +41,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "lock0lock.h"
 #include "lock0prdt.h"
 #include "lock0priv.h"
+#include "lock0guards.h"
 #include "srv0mon.h"
 #include "trx0purge.h"
 #include "trx0sys.h"
diff --git a/storage/innobase/lock/lock0wait.cc b/storage/innobase/lock/lock0wait.cc
index ba17c6242f4..61fc88c67d5 100644
--- a/storage/innobase/lock/lock0wait.cc
+++ b/storage/innobase/lock/lock0wait.cc
@@ -38,6 +38,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "ha_prototypes.h"
 #include "lock0lock.h"
 #include "lock0priv.h"
+#include "lock0guards.h"
 #include "os0thread-create.h"
 #include "que0que.h"
 #include "row0mysql.h"
@@ -688,7 +689,7 @@ static void lock_wait_rollback_deadlock_victim(trx_t *chosen_victim) {
   /* The call to lock_cancel_waiting_and_release requires exclusive latch on
   whole lock_sys.
   Also, we need to latch the shard containing wait_lock to read it and access
-  the lock itself.*/
+  the lock itself. */
   ut_ad(locksys::owns_exclusive_global_latch());
   trx_mutex_enter(chosen_victim);
   chosen_victim->lock.was_chosen_as_deadlock_victim = true;
diff --git a/storage/innobase/row/row0ins.cc b/storage/innobase/row/row0ins.cc
index 362ad021587..7516a54babd 100644
--- a/storage/innobase/row/row0ins.cc
+++ b/storage/innobase/row/row0ins.cc
@@ -47,6 +47,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "ha_prototypes.h"
 #include "lob0lob.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "log0log.h"
 #include "m_string.h"
 #include "mach0data.h"
diff --git a/storage/innobase/srv/srv0srv.cc b/storage/innobase/srv/srv0srv.cc
index 43c0c43edf2..5405ecb2754 100644
--- a/storage/innobase/srv/srv0srv.cc
+++ b/storage/innobase/srv/srv0srv.cc
@@ -65,6 +65,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "ibuf0ibuf.h"
 #ifndef UNIV_HOTBACKUP
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "log0recv.h"
 #include "mem0mem.h"
 #include "os0proc.h"
diff --git a/storage/innobase/trx/trx0i_s.cc b/storage/innobase/trx/trx0i_s.cc
index 8896ff3a39a..2f31baf09da 100644
--- a/storage/innobase/trx/trx0i_s.cc
+++ b/storage/innobase/trx/trx0i_s.cc
@@ -52,6 +52,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "hash0hash.h"
 #include "lock0iter.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "mem0mem.h"
 #include "mysql/plugin.h"
 #include "page0page.h"
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index 196a43ec379..17505230275 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -44,6 +44,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "fsp0sysspace.h"
 #include "ha_prototypes.h"
 #include "lock0lock.h"
+#include "lock0guards.h"
 #include "log0log.h"
 #include "os0proc.h"
 #include "que0que.h"
@@ -1311,7 +1312,6 @@ static void trx_start_low(
  @return true if the transaction number was added to the serialisation_list. */
 static void trx_serialise(
     trx_t *trx,                         /*!< in/out: transaction */
-    mtr_t *mtr,                         /*!< in/out: mini-transaction */
     trx_undo_ptr_t *redo_rseg_undo_ptr, /*!< in/out: Set trx
                                         serialisation number in
                                         referred undo rseg. */
@@ -1444,7 +1444,7 @@ static void trx_write_serialisation_history(
                                                    : nullptr;
 
     /* Will set trx->no and will add rseg to purge queue. */
-    trx_serialise(trx, mtr, redo_rseg_undo_ptr, temp_rseg_undo_ptr);
+    trx_serialise(trx, redo_rseg_undo_ptr, temp_rseg_undo_ptr);
 
     /* It is not necessary to obtain trx->undo_mutex here because
     only a single OS thread is allowed to do the transaction commit
-- 
2.28.0.windows.1


From fe365e850781bf6e6b1847745f4f11e60ace26d6 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Wed, 25 Nov 2020 11:29:22 +0800
Subject: [PATCH 11/19] Pointer parameter element of function copy_one_id can
 be pointer to const.

---
 storage/innobase/include/trx0sys.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index bf6b0f02ebf..6208a43756c 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -812,7 +812,7 @@ struct trx_sys_t {
     trx_id_t no;
   };
 
-  static bool copy_one_id(rw_trx_hash_element_t *element,
+  static bool copy_one_id(const rw_trx_hash_element_t *element,
                           snapshot_ids_arg *arg) {
     if (element->id < arg->id) {
       trx_id_t no = element->no.load(std::memory_order_relaxed);
-- 
2.28.0.windows.1


From 8be90cd92a35be329adc014822a1eaa1745e6e01 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Mon, 7 Dec 2020 15:19:54 +0800
Subject: [PATCH 12/19] Member function put_pins and n_prepared_trx could made
 const.

---
 storage/innobase/include/trx0sys.h | 4 ++--
 storage/innobase/trx/trx0sys.cc    | 4 ++--
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/storage/innobase/include/trx0sys.h b/storage/innobase/include/trx0sys.h
index 6208a43756c..ca9457ca7ac 100644
--- a/storage/innobase/include/trx0sys.h
+++ b/storage/innobase/include/trx0sys.h
@@ -449,7 +449,7 @@ class rw_trx_hash_t {
 
   Since pins are not allowed to be transferred to another thread,
   initialisation thread calls this for recovered transactions. */
-  void put_pins(trx_t *trx);
+  void put_pins(trx_t *trx) const;
 
   /** Finds trx object in lock-free hash with given id.
 
@@ -785,7 +785,7 @@ struct trx_sys_t {
     return count;
   }
 
-  ulint n_prepared_trx(); /*!< Return number of transactions currently
+  ulint n_prepared_trx() const; /*!< Return number of transactions currently
                           in the XA PREPARED state. */
   bool found_prepared_trx; /*!< True if XA PREPARED transactions are found. */
 
diff --git a/storage/innobase/trx/trx0sys.cc b/storage/innobase/trx/trx0sys.cc
index 7c4012ea346..195f5180500 100644
--- a/storage/innobase/trx/trx0sys.cc
+++ b/storage/innobase/trx/trx0sys.cc
@@ -637,7 +637,7 @@ called earlier if thread is expected not to use rw_trx_hash.
 
 Since pins are not allowed to be transferred to another thread,
 initialisation thread calls this for recovered transactions. */
-void rw_trx_hash_t::put_pins(trx_t *trx) {
+void rw_trx_hash_t::put_pins(trx_t *trx) const {
   if (trx->rw_trx_hash_pins != nullptr) {
     lf_hash_put_pins(trx->rw_trx_hash_pins);
     trx->rw_trx_hash_pins = nullptr;
@@ -919,7 +919,7 @@ static bool trx_sys_found_prepared_trx_callback(
 }
 
 /** @return true if found prepared transaction(s). */
-ulint trx_sys_t::n_prepared_trx() {
+ulint trx_sys_t::n_prepared_trx() const {
   trx_sys_found_prepared_trx_callback_arg arg = {0};
 
   trx_sys->rw_trx_hash.iterate(reinterpret_cast<lf_hash_walk_func *>(
-- 
2.28.0.windows.1


From af12022ff0fae499638fdcece6bb21a19dc0cdb5 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Mon, 7 Dec 2020 16:36:24 +0800
Subject: [PATCH 13/19] Avoid integer overflow.

---
 storage/innobase/trx/trx0sys.cc | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/storage/innobase/trx/trx0sys.cc b/storage/innobase/trx/trx0sys.cc
index 195f5180500..38ece81493e 100644
--- a/storage/innobase/trx/trx0sys.cc
+++ b/storage/innobase/trx/trx0sys.cc
@@ -858,7 +858,8 @@ ulint trx_sys_t::any_active_transactions() {
     }
 
     if (srv_force_recovery >= SRV_FORCE_NO_TRX_UNDO) {
-      if (trx->state == TRX_STATE_ACTIVE && trx->is_recovered) {
+      if (trx->state == TRX_STATE_ACTIVE && trx->is_recovered &&
+          total_trx > 0) {
         total_trx--;
       }
     }
-- 
2.28.0.windows.1


From d2b0f95020af8e65278805100145e0a5d28a0f83 Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Thu, 10 Dec 2020 19:21:11 +0800
Subject: [PATCH 14/19] Clean up recovered transactions of state
 TRX_STATE_COMMITTED_IN_MEMORY

---
 storage/innobase/include/trx0roll.h |  8 ++--
 storage/innobase/include/trx0trx.h  | 10 +++--
 storage/innobase/srv/srv0start.cc   |  2 +-
 storage/innobase/trx/trx0roll.cc    | 61 ++++++++++++++------------
 storage/innobase/trx/trx0trx.cc     | 67 ++++++++++++++++-------------
 5 files changed, 83 insertions(+), 65 deletions(-)

diff --git a/storage/innobase/include/trx0roll.h b/storage/innobase/include/trx0roll.h
index 26f7760864d..83c13a9dbf3 100644
--- a/storage/innobase/include/trx0roll.h
+++ b/storage/innobase/include/trx0roll.h
@@ -64,10 +64,10 @@ trx_undo_rec_t *trx_roll_pop_top_rec_of_trx(
 /** Rollback or clean up any incomplete transactions which were
  encountered in crash recovery.  If the transaction already was
  committed, then we clean up a possible insert undo log. If the
- transaction was not yet committed, then we roll it back.
- @param all true=roll back all recovered active transactions;
- false=roll back any incomplete dictionary transaction */
-void trx_rollback_recovered(bool all);
+ transaction was not yet committed, then we roll it back. */
+void trx_rollback_or_clean_recovered(
+    ibool all); /*!< in: FALSE=roll back dictionary transactions;
+                TRUE=roll back all non-PREPARED transactions */
 
 /** Rollback or clean up any incomplete transactions which were
 encountered in crash recovery.  If the transaction already was
diff --git a/storage/innobase/include/trx0trx.h b/storage/innobase/include/trx0trx.h
index c41befa550a..9f839268b0c 100644
--- a/storage/innobase/include/trx0trx.h
+++ b/storage/innobase/include/trx0trx.h
@@ -99,9 +99,9 @@ trx_t *trx_allocate_for_background(void);
 /** Resurrect table locks for resurrected transactions. */
 void trx_resurrect_locks();
 
-/** Release a trx_t instance back to the pool.
-@param[in,out]  trx the instance to release */
-void trx_free(trx_t *&trx);
+/** Free and initialize a transaction object instantiated during recovery.
+@param[in,out]	trx	transaction object to free and initialize */
+void trx_free_resurrected(trx_t *trx);
 
 /** Free a transaction that was allocated by background or user threads.
 @param[in,out]	trx	transaction object to free */
@@ -192,6 +192,10 @@ void trx_commit_low(
     trx_t *trx,  /*!< in/out: transaction */
     mtr_t *mtr); /*!< in/out: mini-transaction (will be committed),
                  or NULL if trx made no modifications */
+/** Cleans up a transaction at database startup. The cleanup is needed if
+ the transaction already got to the middle of a commit when the database
+ crashed, and we cannot roll it back. */
+void trx_cleanup_at_db_startup(trx_t *trx); /*!< in: transaction */
 /** Does the transaction commit for MySQL.
  @return DB_SUCCESS or error number */
 dberr_t trx_commit_for_mysql(trx_t *trx); /*!< in/out: transaction */
diff --git a/storage/innobase/srv/srv0start.cc b/storage/innobase/srv/srv0start.cc
index 67a00bfa55a..4ba679cb7d8 100644
--- a/storage/innobase/srv/srv0start.cc
+++ b/storage/innobase/srv/srv0start.cc
@@ -2895,7 +2895,7 @@ void srv_dict_recover_on_restart() {
   The data dictionary latch should guarantee that there is at
   most one data dictionary transaction active at a time. */
   if (srv_force_recovery < SRV_FORCE_NO_TRX_UNDO) {
-    trx_rollback_recovered(false);
+    trx_rollback_or_clean_recovered(false);
   }
 
   /* Do after all DD transactions recovery, to get consistent metadata */
diff --git a/storage/innobase/trx/trx0roll.cc b/storage/innobase/trx/trx0roll.cc
index 23eb719fc77..9aabb6b6835 100644
--- a/storage/innobase/trx/trx0roll.cc
+++ b/storage/innobase/trx/trx0roll.cc
@@ -620,13 +620,13 @@ static void trx_rollback_active(trx_t *trx) /*!< in/out: transaction */
   trx_roll_crash_recv_trx = nullptr;
 }
 
-static bool trx_rollback_recovered_callback(rw_trx_hash_element_t *element,
+static bool trx_rollback_or_clean_recovered_callback(rw_trx_hash_element_t *element,
                                             std::vector<trx_t *> *trx_list) {
   mutex_enter(&element->mutex);
   trx_t *trx = element->trx;
   if (trx != nullptr) {
     mutex_enter(&trx->mutex);
-    if (trx->is_recovered && trx_state_eq(trx, TRX_STATE_ACTIVE)) {
+    if (trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) || trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY))) {
       ut_ad(trx != trx_dummy_sess->trx);
       trx_list->push_back(trx);
     }
@@ -636,19 +636,14 @@ static bool trx_rollback_recovered_callback(rw_trx_hash_element_t *element,
   return false;
 }
 
-/** Rollback any incomplete transactions which were encountered in crash
- recovery.
-
- If the transaction already was committed, then we cleanup a possible insert
- undo log. If the transaction was not yet committed, then we roll it back.
-
- Note: For XA recovered transactions, we rely on MySQL to do rollback. They
- will be in TRX_STATE_PREPARED state. If the server is shutdown and they are
- still lingering in trx_sys_t::trx_list then the shutdown will hang.
-
- @param [in] all true=roll back all recovered active transactions;
-                 fasle=roll back any incomplete dictionary transaction */
-void trx_rollback_recovered(bool all) {
+/** Rollback or clean up any incomplete transactions which were
+ encountered in crash recovery.  If the transaction already was
+ committed, then we clean up a possible insert undo log. If the
+ transaction was not yet committed, then we roll it back. */
+void trx_rollback_or_clean_recovered(
+    ibool all) /*!< in: FALSE=roll back dictionary transactions;
+               TRUE=roll back all non-PREPARED transactions */
+{
   std::vector<trx_t *> trx_list;
 
   ut_a(srv_force_recovery < SRV_FORCE_NO_TRX_UNDO);
@@ -657,7 +652,7 @@ void trx_rollback_recovered(bool all) {
   other thread is allowed to modify or remove these transactions from
   rw_trx_hash. */
   trx_sys->rw_trx_hash.iterate_no_dups(
-      reinterpret_cast<lf_hash_walk_func *>(trx_rollback_recovered_callback),
+      reinterpret_cast<lf_hash_walk_func *>(trx_rollback_or_clean_recovered_callback),
       &trx_list);
 
   while (!trx_list.empty()) {
@@ -667,19 +662,31 @@ void trx_rollback_recovered(bool all) {
 #ifdef UNIV_DEBUG
     ut_ad(trx != nullptr);
     trx_mutex_enter(trx);
-    ut_ad(trx->is_recovered && trx_state_eq(trx, TRX_STATE_ACTIVE));
+    ut_ad(trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) || trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY)));
     trx_mutex_exit(trx);
 #endif /* UNIV_DEBUG */
 
-    if (all || trx_get_dict_operation(trx) != TRX_DICT_OP_NONE) {
-      trx_rollback_active(trx);
-      if (trx->error_state != DB_SUCCESS) {
-        trx->error_state = DB_SUCCESS;
-        trx_sys->deregister_rw(trx);
-        trx_free_at_shutdown(trx);
-      } else {
-        trx_free(trx);
-      }
+    switch (trx->state) {
+      case TRX_STATE_COMMITTED_IN_MEMORY:
+        ib::info(ER_IB_MSG_1188)
+            << "Cleaning up trx with id " << trx_get_id_for_print(trx);
+        trx_cleanup_at_db_startup(trx);
+        trx_free_resurrected(trx);
+        break;
+      case TRX_STATE_ACTIVE:
+        if (all || trx_get_dict_operation(trx) != TRX_DICT_OP_NONE) {
+          trx_rollback_active(trx);
+          if (trx->error_state != DB_SUCCESS) {
+            trx->error_state = DB_SUCCESS;
+            trx_sys->deregister_rw(trx);
+            trx_free_at_shutdown(trx);
+          } else {
+            trx_free_for_background(trx);
+          }
+        }
+        break;
+      default:
+        break;
     }
   }
 }
@@ -702,7 +709,7 @@ void trx_recovery_rollback_thread() {
   if (trx_sys->rw_trx_hash.size() > 0) {
     ib::info(ER_IB_MSG_1189) << "Starting in background the rollback"
                                 " of uncommitted transactions";
-    trx_rollback_recovered(true);
+    trx_rollback_or_clean_recovered(true);
     ib::info(ER_IB_MSG_1190) << "Rollback of non-prepared transactions"
                                 " completed";
   }
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index 17505230275..2dac75bfb37 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -489,41 +489,12 @@ static trx_t *trx_create_low() {
 /**
 Release a trx_t instance back to the pool.
 @param trx the instance to release. */
-void trx_free(trx_t *&trx) {
+static void trx_free(trx_t *&trx) {
   ut_ad(!trx->declared_to_be_inside_innodb);
   ut_ad(trx->n_mysql_tables_in_use == 0);
   ut_ad(trx->mysql_n_tables_locked == 0);
   ut_ad(!trx->internal);
 
-  if (trx->declared_to_be_inside_innodb) {
-    ib::error(ER_IB_MSG_1270)
-        << "Freeing a trx (" << trx << ", " << trx_get_id_for_print(trx)
-        << ") which is declared"
-           " to be processing inside InnoDB";
-
-    trx_print(stderr, trx, 600);
-    putc('\n', stderr);
-
-    /* This is an error but not a fatal error. We must keep the coutners like
-    srv_conc.n_active accurate. */
-    srv_conc_force_exit_innodb(trx);
-  }
-
-  if (trx->n_mysql_tables_in_use > 0 || trx->mysql_n_tables_locked > 0) {
-    ib::error(ER_IB_MSG_1270)
-        << "MySQL is freeing a thd though"
-           " trx->n_mysql_tables_in_use is "
-        << trx->n_mysql_tables_in_use << " and trx->mysql_n_tables_locked is "
-        << trx->mysql_n_tables_locked << ".";
-
-    trx_print(stderr, trx, 600);
-    ut_print_buf(stderr, trx, sizeof(trx_t));
-    putc('\n', stderr);
-  }
-
-  trx->dict_operation = TRX_DICT_OP_NONE;
-  assert_trx_is_inactive(trx);
-
   trx_sys->deregister_trx(trx);
   assert_trx_is_free(trx);
 
@@ -605,6 +576,16 @@ static void trx_validate_state_before_free(trx_t *trx) {
   assert_trx_is_inactive(trx);
 }
 
+/** Free and initialize a transaction object instantiated during recovery.
+@param[in,out]	trx	transaction object to free and initialize */
+void trx_free_resurrected(trx_t *trx) {
+  trx_validate_state_before_free(trx);
+
+  trx_init(trx);
+
+  trx_free(trx);
+}
+
 /** Free a transaction that was allocated by background or user threads.
 @param[in,out]	trx	transaction object to free */
 void trx_free_for_background(trx_t *trx) {
@@ -1987,6 +1968,32 @@ void trx_commit(trx_t *trx) /*!< in/out: transaction */
   trx_commit_low(trx, mtr);
 }
 
+/** Cleans up a transaction at database startup. The cleanup is needed if
+ the transaction already got to the middle of a commit when the database
+ crashed, and we cannot roll it back. */
+void trx_cleanup_at_db_startup(trx_t *trx) /*!< in: transaction */
+{
+  ut_ad(trx->is_recovered);
+
+  /* Cleanup any durable undo logs in non-temporary rollback segments.
+  At database start-up there are no active transactions recorded in
+  any rollback segments in the temporary tablespace because all those
+  changes are all lost on restart. */
+  if (trx->rsegs.m_redo.insert_undo != nullptr) {
+    trx_undo_insert_cleanup(&trx->rsegs.m_redo, false);
+  }
+
+  memset(&trx->rsegs, 0x0, sizeof(trx->rsegs));
+  trx->undo_no = 0;
+  trx->undo_rseg_space = 0;
+  trx->last_sql_stat_start.least_undo_no = 0;
+
+  trx_sys->deregister_rw(trx);
+
+  ut_ad(trx->is_recovered);
+  trx->state = TRX_STATE_NOT_STARTED;
+}
+
 /** Prepares a transaction for commit/rollback. */
 void trx_commit_or_rollback_prepare(trx_t *trx) /*!< in/out: transaction */
 {
-- 
2.28.0.windows.1


From 2bc5ece9a023ed3f91abcf14d4b7b8792ce8670d Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Sat, 12 Dec 2020 16:52:22 +0800
Subject: [PATCH 15/19] Adjust alignment and modify the line length below 80.

---
 storage/innobase/trx/trx0roll.cc | 11 +++++++----
 1 file changed, 7 insertions(+), 4 deletions(-)

diff --git a/storage/innobase/trx/trx0roll.cc b/storage/innobase/trx/trx0roll.cc
index 9aabb6b6835..3b375cfd215 100644
--- a/storage/innobase/trx/trx0roll.cc
+++ b/storage/innobase/trx/trx0roll.cc
@@ -620,13 +620,15 @@ static void trx_rollback_active(trx_t *trx) /*!< in/out: transaction */
   trx_roll_crash_recv_trx = nullptr;
 }
 
-static bool trx_rollback_or_clean_recovered_callback(rw_trx_hash_element_t *element,
-                                            std::vector<trx_t *> *trx_list) {
+static bool trx_rollback_or_clean_recovered_callback(
+    rw_trx_hash_element_t *element,
+    std::vector<trx_t *> *trx_list) {
   mutex_enter(&element->mutex);
   trx_t *trx = element->trx;
   if (trx != nullptr) {
     mutex_enter(&trx->mutex);
-    if (trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) || trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY))) {
+    if (trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) ||
+        trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY))) {
       ut_ad(trx != trx_dummy_sess->trx);
       trx_list->push_back(trx);
     }
@@ -662,7 +664,8 @@ void trx_rollback_or_clean_recovered(
 #ifdef UNIV_DEBUG
     ut_ad(trx != nullptr);
     trx_mutex_enter(trx);
-    ut_ad(trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) || trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY)));
+    ut_ad(trx->is_recovered && (trx_state_eq(trx, TRX_STATE_ACTIVE) ||
+          trx_state_eq(trx, TRX_STATE_COMMITTED_IN_MEMORY)));
     trx_mutex_exit(trx);
 #endif /* UNIV_DEBUG */
 
-- 
2.28.0.windows.1


From 812014129cbfdc00df6a735ca1766011cdb31578 Mon Sep 17 00:00:00 2001
From: zhang-lujie <zhanglujie666@sina.com>
Date: Tue, 15 Dec 2020 11:40:53 +0800
Subject: [PATCH 16/19] Add destructor function.

---
 storage/innobase/include/lock0guards.h | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
index 15ee99be846..299eb986445 100644
--- a/storage/innobase/include/lock0guards.h
+++ b/storage/innobase/include/lock0guards.h
@@ -114,6 +114,8 @@ class Shard_latch_guard {
 
   explicit Shard_latch_guard(const page_id_t &page_id)
       : m_global_shared_latch_guard{}, m_shard_naked_latch_guard{page_id} {}
+
+  ~Shard_latch_guard() {}
 };
 
 /**
-- 
2.28.0.windows.1


From acbd08cc16098d7f401e4a628b1b16ee2617158d Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Tue, 15 Dec 2020 16:20:03 +0800
Subject: [PATCH 17/19] FIX ASSERTION FAILURE:
 LOCK0LATCHES.CC:42:LOCK_SYS->REC_HASH->N_CELLS == LOCK_SYS->P

---
 .../suite/innodb/r/lock_sys_resize.result     | 31 ++++++++
 .../suite/innodb/t/lock_sys_resize-master.opt |  2 +
 .../suite/innodb/t/lock_sys_resize.test       | 76 +++++++++++++++++++
 storage/innobase/include/lock0guards.h        | 45 ++++++++---
 storage/innobase/include/lock0latches.h       |  3 +-
 storage/innobase/lock/lock0guards.cc          | 18 ++---
 storage/innobase/lock/lock0lock.cc            | 16 +++-
 7 files changed, 170 insertions(+), 21 deletions(-)
 create mode 100644 mysql-test/suite/innodb/r/lock_sys_resize.result
 create mode 100644 mysql-test/suite/innodb/t/lock_sys_resize-master.opt
 create mode 100644 mysql-test/suite/innodb/t/lock_sys_resize.test

diff --git a/mysql-test/suite/innodb/r/lock_sys_resize.result b/mysql-test/suite/innodb/r/lock_sys_resize.result
new file mode 100644
index 00000000000..33fe134225e
--- /dev/null
+++ b/mysql-test/suite/innodb/r/lock_sys_resize.result
@@ -0,0 +1,31 @@
+# Bug #31329634 ASSERTION FAILURE:
+#    LOCK0LATCHES.CC:42:LOCK_SYS->REC_HASH->N_CELLS == LOCK_SYS->P
+SELECT @@innodb_buffer_pool_size;
+@@innodb_buffer_pool_size
+17825792
+SELECT @@innodb_buffer_pool_chunk_size;
+@@innodb_buffer_pool_chunk_size
+1048576
+CREATE TABLE t1 (id INT PRIMARY KEY, val VARCHAR(1000)) ENGINE=INNODB;
+INSERT INTO t1 (id,val) VALUES (1,''),(2,''),(3,''),(4,''),(5,'');
+SET DEBUG_SYNC='lock_rec_restore_from_page_infimum_will_latch
+    SIGNAL con1_will_latch
+    WAIT_FOR con1_can_go';
+UPDATE t1 SET val='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa';
+SET DEBUG_SYNC='now WAIT_FOR con1_will_latch';
+SET GLOBAL DEBUG='+d,syncpoint_after_lock_sys_resize_rec_hash';
+SET GLOBAL innodb_buffer_pool_size=
+@@innodb_buffer_pool_size * 2 + @@innodb_buffer_pool_chunk_size;
+SET DEBUG_SYNC='now WAIT_FOR reached_after_lock_sys_resize_rec_hash';
+SET DEBUG_SYNC='now SIGNAL con1_can_go';
+SET GLOBAL DEBUG='-d,syncpoint_after_lock_sys_resize_rec_hash';
+SET DEBUG_SYNC='now SIGNAL continue_after_lock_sys_resize_rec_hash';
+DROP TABLE t1;
+SELECT @@innodb_buffer_pool_size;
+@@innodb_buffer_pool_size
+36700160
+SET GLOBAL innodb_buffer_pool_size=
+(@@innodb_buffer_pool_size - @@innodb_buffer_pool_chunk_size) div 2;
+SELECT @@innodb_buffer_pool_size;
+@@innodb_buffer_pool_size
+17825792
diff --git a/mysql-test/suite/innodb/t/lock_sys_resize-master.opt b/mysql-test/suite/innodb/t/lock_sys_resize-master.opt
new file mode 100644
index 00000000000..4f833433c9b
--- /dev/null
+++ b/mysql-test/suite/innodb/t/lock_sys_resize-master.opt
@@ -0,0 +1,2 @@
+--innodb_buffer-pool-size=17825792
+--innodb-buffer-pool-chunk-size=1048576
diff --git a/mysql-test/suite/innodb/t/lock_sys_resize.test b/mysql-test/suite/innodb/t/lock_sys_resize.test
new file mode 100644
index 00000000000..a7dfd3e11a9
--- /dev/null
+++ b/mysql-test/suite/innodb/t/lock_sys_resize.test
@@ -0,0 +1,76 @@
+--source include/have_debug.inc
+--source include/have_debug_sync.inc
+--source include/count_sessions.inc
+
+--echo # Bug #31329634 ASSERTION FAILURE:
+--echo #    LOCK0LATCHES.CC:42:LOCK_SYS->REC_HASH->N_CELLS == LOCK_SYS->P
+
+# The two values in master-opt were chosen so that the following
+# SET GLOBAL innodb_buffer_pool_size= ...
+# will succeed, and will resize lock_sys in parallel to the UPDATE.
+# (As opposed to say, block, as is the case when it shrinks instead of growing)
+# Also it must be larger than BUF_LRU_MIN_LEN pages, as otherwise BP shrink will
+# not be able to finish as it will try to keep BUF_LRU_MIN_LEN pages in BP.
+SELECT @@innodb_buffer_pool_size;
+SELECT @@innodb_buffer_pool_chunk_size;
+
+CREATE TABLE t1 (id INT PRIMARY KEY, val VARCHAR(1000)) ENGINE=INNODB;
+INSERT INTO t1 (id,val) VALUES (1,''),(2,''),(3,''),(4,''),(5,'');
+
+
+--connect (con1,localhost,root,,)
+  SET DEBUG_SYNC='lock_rec_restore_from_page_infimum_will_latch
+    SIGNAL con1_will_latch
+    WAIT_FOR con1_can_go';
+  # This will cause resize of records and require calls to
+  # lock_rec_restore_from_page_infimum() which exercise Shard_latches_guard
+  --send UPDATE t1 SET val='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
+
+--connect (con2,localhost,root,,)
+  SET DEBUG_SYNC='now WAIT_FOR con1_will_latch';
+  # resize happens in a background thread so we need to enable sync point
+  SET GLOBAL DEBUG='+d,syncpoint_after_lock_sys_resize_rec_hash';
+  SET GLOBAL innodb_buffer_pool_size=
+    @@innodb_buffer_pool_size * 2 + @@innodb_buffer_pool_chunk_size;
+
+--connection default
+  SET DEBUG_SYNC='now WAIT_FOR reached_after_lock_sys_resize_rec_hash';
+  SET DEBUG_SYNC='now SIGNAL con1_can_go';
+  # This is the moment where con1 could observe assertion failure
+  SET GLOBAL DEBUG='-d,syncpoint_after_lock_sys_resize_rec_hash';
+  SET DEBUG_SYNC='now SIGNAL continue_after_lock_sys_resize_rec_hash';
+
+--connection con1
+  --reap
+
+
+--connection default
+--disconnect con1
+--disconnect con2
+
+DROP TABLE t1;
+
+# Make sure we finish previous resizing before issuing another
+let $wait_timeout = 60;
+let $wait_condition =
+  SELECT SUBSTR(variable_value, 1, 9) = 'Completed'
+  FROM performance_schema.global_status
+  WHERE variable_name = 'innodb_buffer_pool_resize_status';
+--source include/wait_condition.inc
+
+SELECT @@innodb_buffer_pool_size;
+
+# Restore original smaller size
+SET GLOBAL innodb_buffer_pool_size=
+  (@@innodb_buffer_pool_size - @@innodb_buffer_pool_chunk_size) div 2;
+# Make sure we finish resizing and restore original state before ending
+let $wait_timeout = 60;
+let $wait_condition =
+  SELECT SUBSTR(variable_value, 1, 9) = 'Completed'
+  FROM performance_schema.global_status
+  WHERE variable_name = 'innodb_buffer_pool_resize_status';
+--source include/wait_condition.inc
+
+SELECT @@innodb_buffer_pool_size;
+
+--source include/wait_until_count_sessions.inc
diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
index 299eb986445..14082dfb108 100644
--- a/storage/innobase/include/lock0guards.h
+++ b/storage/innobase/include/lock0guards.h
@@ -119,23 +119,28 @@ class Shard_latch_guard {
 };
 
 /**
-A RAII wrapper class which s-latches the global lock_sys shard, and mutexes
-protecting specified shards for the duration of its scope.
+A RAII helper which latches the mutexes protecting specified shards for the
+duration of its scope.
 It makes sure to take the latches in correct order and handles the case where
 both pages are in the same shard correctly.
+You quite probably don't want to use this class, which only takes a shard's
+latch, without acquiring global_latch - which gives no protection from threads
+which latch only the global_latch exclusively to prevent any activity.
+You should use it in combination with Global_shared_latch_guard, so that you
+first obtain an s-latch on the global_latch, or simply use the
+Shard_latches_guard class which already combines the two for you.
 */
-class Shard_latches_guard {
-  explicit Shard_latches_guard(Lock_mutex &shard_mutex_a,
-                               Lock_mutex &shard_mutex_b);
+class Shard_naked_latches_guard {
+  explicit Shard_naked_latches_guard(Lock_mutex &shard_mutex_a,
+                                     Lock_mutex &shard_mutex_b);
 
  public:
-  explicit Shard_latches_guard(const buf_block_t &block_a,
-                               const buf_block_t &block_b);
+  explicit Shard_naked_latches_guard(const buf_block_t &block_a,
+                                     const buf_block_t &block_b);
 
-  ~Shard_latches_guard();
+  ~Shard_naked_latches_guard();
 
  private:
-  Global_shared_latch_guard m_global_shared_latch_guard;
   /** The "smallest" of the two shards' mutexes in the latching order */
   Lock_mutex &m_shard_mutex_1;
   /** The "largest" of the two shards' mutexes in the latching order */
@@ -143,6 +148,28 @@ class Shard_latches_guard {
   /** The ordering on shard mutexes used to avoid deadlocks */
   static constexpr std::less<Lock_mutex *> MUTEX_ORDER{};
 };
+/**
+A RAII wrapper class which s-latches the global lock_sys shard, and mutexes
+protecting specified shards for the duration of its scope.
+It makes sure to take the latches in correct order and handles the case where
+both pages are in the same shard correctly.
+The order of initialization is important: we have to take shared global latch
+BEFORE we attempt to use hash function to compute correct shard and latch it.
+*/
+class Shard_latches_guard {
+ public:
+  explicit Shard_latches_guard(const buf_block_t &block_a,
+                               const buf_block_t &block_b)
+      : m_global_shared_latch_guard{},
+        m_shard_naked_latches_guard{block_a, block_b} {}
+
+  ~Shard_latches_guard() {}
+
+ private:
+  Global_shared_latch_guard m_global_shared_latch_guard;
+  Shard_naked_latches_guard m_shard_naked_latches_guard;
+};
+
 }  // namespace locksys
 
 #endif /* lock0guards_h */
diff --git a/storage/innobase/include/lock0latches.h b/storage/innobase/include/lock0latches.h
index 5b3733e5fca..528526ee08d 100644
--- a/storage/innobase/include/lock0latches.h
+++ b/storage/innobase/include/lock0latches.h
@@ -244,8 +244,7 @@ class Latches {
   friend class Global_exclusive_try_latch;
   friend class Global_shared_latch_guard;
   friend class Shard_naked_latch_guard;
-  friend class Shard_latch_guard;
-  friend class Shard_latches_guard;
+  friend class Shard_naked_latches_guard;
 
   /** You should not use this functionality in new code.
   Instead use Global_exclusive_latch_guard.
diff --git a/storage/innobase/lock/lock0guards.cc b/storage/innobase/lock/lock0guards.cc
index 3aa41fd3cc9..6e83c805d4f 100644
--- a/storage/innobase/lock/lock0guards.cc
+++ b/storage/innobase/lock/lock0guards.cc
@@ -84,26 +84,26 @@ Global_shared_latch_guard::~Global_shared_latch_guard() {
   lock_sys->latches.global_latch.s_unlock();
 }
 
-/* Shard_latches_guard */
+/* Shard_naked_latches_guard */
 
-Shard_latches_guard::Shard_latches_guard(Lock_mutex &shard_mutex_a,
-                                         Lock_mutex &shard_mutex_b)
-    : m_global_shared_latch_guard{},
-      m_shard_mutex_1{*std::min(&shard_mutex_a, &shard_mutex_b, MUTEX_ORDER)},
+Shard_naked_latches_guard::Shard_naked_latches_guard(Lock_mutex &shard_mutex_a,
+                                                     Lock_mutex &shard_mutex_b)
+    : m_shard_mutex_1{*std::min(&shard_mutex_a, &shard_mutex_b, MUTEX_ORDER)},
       m_shard_mutex_2{*std::max(&shard_mutex_a, &shard_mutex_b, MUTEX_ORDER)} {
+  ut_ad(owns_shared_global_latch());
   if (&m_shard_mutex_1 != &m_shard_mutex_2) {
     mutex_enter(&m_shard_mutex_1);
   }
   mutex_enter(&m_shard_mutex_2);
 }
 
-Shard_latches_guard::Shard_latches_guard(const buf_block_t &block_a,
-                                         const buf_block_t &block_b)
-    : Shard_latches_guard{
+Shard_naked_latches_guard::Shard_naked_latches_guard(const buf_block_t &block_a,
+                                                     const buf_block_t &block_b)
+    : Shard_naked_latches_guard{
           lock_sys->latches.page_shards.get_mutex(block_a.get_page_id()),
           lock_sys->latches.page_shards.get_mutex(block_b.get_page_id())} {}
 
-Shard_latches_guard::~Shard_latches_guard() {
+Shard_naked_latches_guard::~Shard_naked_latches_guard() {
   mutex_exit(&m_shard_mutex_2);
   if (&m_shard_mutex_1 != &m_shard_mutex_2) {
     mutex_exit(&m_shard_mutex_1);
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index e12dda5a021..08974fb5794 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -43,6 +43,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 
 #include "btr0btr.h"
 #include "current_thd.h"
+#include "debug_sync.h" /* CONDITIONAL_SYNC_POINT */
 #include "dict0boot.h"
 #include "dict0mem.h"
 #include "ha_prototypes.h"
@@ -364,6 +365,18 @@ void lock_sys_resize(ulint n_cells) {
   HASH_MIGRATE(old_hash, lock_sys->rec_hash, lock_t, hash, lock_rec_lock_fold);
   hash_table_free(old_hash);
 
+  DBUG_EXECUTE_IF("syncpoint_after_lock_sys_resize_rec_hash", {
+    /* A workaround for buf_resize_thread() not using create_thd().
+    TBD: if buf_resize_thread() were to use create_thd() then should it be
+    instrumented (together or instead of os_thread_create instrumentation)? */
+    ut_ad(current_thd == nullptr);
+    THD *thd = create_thd(false, true, true, PSI_NOT_INSTRUMENTED);
+    ut_ad(current_thd == thd);
+    CONDITIONAL_SYNC_POINT("after_lock_sys_resize_rec_hash");
+    destroy_thd(thd);
+    ut_ad(current_thd == nullptr);
+  });
+
   old_hash = lock_sys->prdt_hash;
   lock_sys->prdt_hash = hash_create(n_cells);
   HASH_MIGRATE(old_hash, lock_sys->prdt_hash, lock_t, hash, lock_rec_lock_fold);
@@ -2648,7 +2661,7 @@ void lock_move_reorganize_page(
 
       ut_ad(lock_rec_find_set_bit(lock) == ULINT_UNDEFINED);
     }
-  } /* Shard_latches_guard */
+  } /* Shard_latch_guard */
 
   mem_heap_free(heap);
 
@@ -3242,6 +3255,7 @@ void lock_rec_restore_from_page_infimum(
                                 state; lock bits are reset on
                                 the infimum */
 {
+  DEBUG_SYNC_C("lock_rec_restore_from_page_infimum_will_latch");
   ulint heap_no = page_rec_get_heap_no(rec);
 
   locksys::Shard_latches_guard guard{*block, *donator};
-- 
2.28.0.windows.1


From 9db1a771e7441132997e3d5d61030dcdd99f430e Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Wed, 16 Dec 2020 11:18:44 +0800
Subject: [PATCH 18/19] =?UTF-8?q?Issue=20#24=20codecheck=E6=95=B4=E6=94=B9?=
 =?UTF-8?q?=EF=BC=8C=E6=BB=A1=E8=B6=B3=E4=BB=A5=E4=B8=8B=E8=A7=84=E5=88=99?=
 =?UTF-8?q?=201.=E6=9E=84=E9=80=A0=E5=87=BD=E6=95=B0=E5=88=9D=E5=A7=8B?=
 =?UTF-8?q?=E5=8C=96=E5=88=97=E8=A1=A8=E6=8C=89=E5=9B=9B=E7=A9=BA=E6=A0=BC?=
 =?UTF-8?q?=E7=BC=A9=E8=BF=9B=E5=B9=B6=E6=8E=92=E5=A4=9A=E8=A1=8C=202.?=
 =?UTF-8?q?=E5=90=88=E7=90=86=E5=AE=89=E6=8E=92=E7=A9=BA=E8=A1=8C=203.?=
 =?UTF-8?q?=E6=B3=A8=E9=87=8A=E4=B8=8D=E8=83=BD=E5=8C=85=E5=90=ABTBD?=
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

---
 storage/innobase/include/lock0guards.h | 5 ++---
 storage/innobase/lock/lock0lock.cc     | 4 +---
 2 files changed, 3 insertions(+), 6 deletions(-)

diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
index 14082dfb108..f3f2e6be6b7 100644
--- a/storage/innobase/include/lock0guards.h
+++ b/storage/innobase/include/lock0guards.h
@@ -160,8 +160,8 @@ class Shard_latches_guard {
  public:
   explicit Shard_latches_guard(const buf_block_t &block_a,
                                const buf_block_t &block_b)
-      : m_global_shared_latch_guard{},
-        m_shard_naked_latches_guard{block_a, block_b} {}
+    : m_global_shared_latch_guard{},
+      m_shard_naked_latches_guard{block_a, block_b} {}
 
   ~Shard_latches_guard() {}
 
@@ -169,7 +169,6 @@ class Shard_latches_guard {
   Global_shared_latch_guard m_global_shared_latch_guard;
   Shard_naked_latches_guard m_shard_naked_latches_guard;
 };
-
 }  // namespace locksys
 
 #endif /* lock0guards_h */
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index 08974fb5794..57874208f76 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -366,9 +366,7 @@ void lock_sys_resize(ulint n_cells) {
   hash_table_free(old_hash);
 
   DBUG_EXECUTE_IF("syncpoint_after_lock_sys_resize_rec_hash", {
-    /* A workaround for buf_resize_thread() not using create_thd().
-    TBD: if buf_resize_thread() were to use create_thd() then should it be
-    instrumented (together or instead of os_thread_create instrumentation)? */
+    /* A workaround for buf_resize_thread() not using create_thd(). */
     ut_ad(current_thd == nullptr);
     THD *thd = create_thd(false, true, true, PSI_NOT_INSTRUMENTED);
     ut_ad(current_thd == thd);
-- 
2.28.0.windows.1


From 1ab6a6bb7bf00c7da91607c4f59b83ec44a9a260 Mon Sep 17 00:00:00 2001
From: zwang28 <84491488@qq.com>
Date: Thu, 17 Dec 2020 11:31:54 +0800
Subject: [PATCH 19/19] Issue#37 Add copyright and disclaimer information

---
 storage/innobase/include/lock0aarch64_atomic.h | 15 +++++++++++++++
 storage/innobase/lock/lock0aarch64_atomic.cc   | 15 +++++++++++++++
 2 files changed, 30 insertions(+)

diff --git a/storage/innobase/include/lock0aarch64_atomic.h b/storage/innobase/include/lock0aarch64_atomic.h
index 772a169355d..a6bf245c6d5 100644
--- a/storage/innobase/include/lock0aarch64_atomic.h
+++ b/storage/innobase/include/lock0aarch64_atomic.h
@@ -1,3 +1,18 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Huawei Technologies Co., Ltd. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+*****************************************************************************/
+
 #ifndef lock0aarch64_atomic_h
 #define lock0aarch64_atomic_h
 
diff --git a/storage/innobase/lock/lock0aarch64_atomic.cc b/storage/innobase/lock/lock0aarch64_atomic.cc
index 4b70694b2d1..a546bc60661 100644
--- a/storage/innobase/lock/lock0aarch64_atomic.cc
+++ b/storage/innobase/lock/lock0aarch64_atomic.cc
@@ -1,3 +1,18 @@
+/*****************************************************************************
+
+Copyright (c) 2020, Huawei Technologies Co., Ltd. All Rights Reserved.
+
+This program is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License, version 2.0, as published by the
+Free Software Foundation.
+
+This program is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+FOR A PARTICULAR PURPOSE. See the GNU General Public License, version 2.0,
+for more details.
+
+*****************************************************************************/
+
 #include "lock0aarch64_atomic.h"
 
 lint word_add_fetch(volatile lint *word, ulint amount) {
-- 
2.28.0.windows.1

